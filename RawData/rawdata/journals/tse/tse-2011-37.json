[
    {
        "title": "Editorial: State of the Journal.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2011.17",
        "volume": "37"
    },
    {
        "title": "Bristlecone: Language Support for Robust Software Applications.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.27",
        "volume": "37",
        "abstract": "We present Bristlecone, a programming language for robust software systems. Bristlecone applications have two components: a high-level organization specification that describes how the application's conceptual operations interact and a low-level operational specification that describes the sequence of instructions that comprise an individual conceptual operation. Bristlecone uses the high-level organization specification to recover the software system from an error to a consistent state and to reason how to safely continue the software system's execution after the error. We have implemented a compiler and runtime for Bristlecone. We have evaluated this implementation on three benchmark applications: a Web crawler, a Web server, and a multiroom chat server. We developed both a Bristlecone version and a Java version of each benchmark application. We used injected failures to evaluate the robustness of each version of the application. We found that the Bristlecone versions of the benchmark applications more successfully survived the injected failures. The Bristlecone compiler contains a static analysis that operates on the organization specification to generate a set of diagrams that graphically present the task interactions in the application. We have used the analysis to help understand the high-level structure of three Bristlecone applications: a game server, a Web server, and a chat server.",
        "keywords": [
            "Robustness",
            "Application software",
            "Software systems",
            "Switches",
            "Runtime",
            "Computer languages",
            "Costs",
            "Web server",
            "Java",
            "Software tools"
        ]
    },
    {
        "title": "Deriving a Slicing Algorithm via FermaT Transformations.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.13",
        "volume": "37",
        "abstract": "In this paper, we present a case study in deriving an algorithm from a formal specification via FermaT transformations. The general method (which is presented in a separate paper) is extended to a method for deriving an implementation of a program transformation from a specification of the program transformation. We use program slicing as an example transformation since this is of interest outside the program transformation community. We develop a formal specification for program slicing in the form of a WSL specification statement which is refined into a simple slicing algorithm by applying a sequence of general purpose program transformations and refinements. Finally, we show how the same methods can be used to derive an algorithm for semantic slicing. The main novel contributions of this paper are: 1) developing a formal specification for slicing, 2) expressing the definition of slicing in terms of a WSL specification statement, and 3) by applying correctness preserving transformations to the specification, we can derive a simple slicing algorithm.",
        "keywords": [
            "Formal specifications",
            "Logic",
            "Software algorithms",
            "Reverse engineering",
            "Assembly",
            "High level languages"
        ]
    },
    {
        "title": "Developing a Single Model and Test Prioritization Strategies for Event-Driven Software.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.12",
        "volume": "37",
        "abstract": "Event-Driven Software (EDS) can change state based on incoming events; common examples are GUI and Web applications. These EDSs pose a challenge to testing because there are a large number of possible event sequences that users can invoke through a user interface. While valuable contributions have been made for testing these two subclasses of EDS, such efforts have been disjoint. This work provides the first single model that is generic enough to study GUI and Web applications together. In this paper, we use the model to define generic prioritization criteria that are applicable to both GUI and Web applications. Our ultimate goal is to evolve the model and use it to develop a unified theory of how all EDS should be tested. An empirical study reveals that the GUI and Web-based applications, when recast using the new model, show similar behavior. For example, a criterion that gives priority to all pairs of event interactions did well for GUI and Web applications; another criterion that gives priority to the smallest number of parameter value settings did poorly for both. These results reinforce our belief that these two subclasses of applications should be modeled and studied together.",
        "keywords": [
            "Software testing",
            "Graphical user interfaces",
            "Application software",
            "Computer science",
            "User interfaces",
            "Protocols",
            "Embedded software",
            "Information systems",
            "Educational institutions",
            "Abstracts"
        ]
    },
    {
        "title": "From UML to Petri Nets: The PCM-Based Methodology.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.10",
        "volume": "37",
        "abstract": "In this paper, we present an evaluation methodology to validate the performance of a UML model, representing a software architecture. The proposed approach is based on open and well-known standards: UML for software modeling and the OMG Profile for Schedulability, Performance, and Time Specification for the performance annotations into UML models. Such specifications are collected in an intermediate model, called the Performance Context Model (PCM). The intermediate model is translated into a performance model which is subsequently evaluated. The paper is focused on the mapping from the PCM to the performance domain. More specifically, we adopt Petri nets as the performance domain, specifying a mapping process based on a compositional approach we have entirely implemented in the ArgoPerformance tool. All of the rules to derive a Petri net from a PCM and the performance measures assessable from the former are carefully detailed. To validate the proposed technique, we provide an in-depth analysis of a web application for music streaming.",
        "keywords": [
            "Unified modeling language",
            "Petri nets",
            "Phase change materials",
            "Software architecture",
            "Software performance",
            "Context modeling",
            "Application software",
            "Design engineering",
            "Performance analysis",
            "Stochastic processes"
        ]
    },
    {
        "title": "Genetic Algorithms for Randomized Unit Testing.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.46",
        "volume": "37",
        "abstract": "Randomized testing is an effective method for testing software units. The thoroughness of randomized unit testing varies widely according to the settings of certain parameters, such as the relative frequencies with which methods are called. In this paper, we describe Nighthawk, a system which uses a genetic algorithm (GA) to find parameters for randomized unit testing that optimize test coverage. Designing GAs is somewhat of a black art. We therefore use a feature subset selection (FSS) tool to assess the size and content of the representations within the GA. Using that tool, we can reduce the size of the representation substantially while still achieving most of the coverage found using the full representation. Our reduced GA achieves almost the same results as the full system, but in only 10 percent of the time. These results suggest that FSS could significantly optimize metaheuristic search-based software engineering tools.",
        "keywords": [
            "Testing",
            "Biological cells",
            "Gallium",
            "Receivers",
            "Software",
            "Java",
            "Optimization"
        ]
    },
    {
        "title": "Plat_Forms: A Web Development Platform Comparison by an Exploratory Experiment Searching for Emergent Platform Properties.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.22",
        "volume": "37",
        "abstract": "Background: For developing Web-based applications, there exist several competing and widely used technological platforms (consisting of a programming language, framework(s), components, and tools), each with an accompanying development culture and style. Research question: Do Web development projects exhibit emergent process or product properties that are characteristic and consistent within a platform, but show relevant substantial differences across platforms or do team-to-team individual differences outweigh such differences, if any? Such a property could be positive (i.e., a platform advantage), negative, or neutral, and it might be unobvious which is which. Method: In a nonrandomized, controlled experiment, framed as a public contest called “Plat_Forms,” top-class teams of three professional programmers competed to implement the same requirements for a Web-based application within 30 hours. Three different platforms (Java EE, PHP, or Perl) were used by three teams each. We compare the resulting nine products and process records along many dimensions, both external (usability, functionality, reliability, security, etc.) and internal (size, structure, modifiability, etc.). Results: The various results obtained cover a wide spectrum: First, there are results that many people would have called “obvious” or “well known,” say, that Perl solutions tend to be more compact than Java solutions. Second, there are results that contradict conventional wisdom, say, that our PHP solutions appear in some (but not all) respects to be actually at least as secure as the others. Finally, one result makes a statement we have not seen discussed previously: Along several dimensions, the amount of within-platform variation between the teams tends to be smaller for PHP than for the other platforms. Conclusion: The results suggest that substantial characteristic platform differences do indeed exist in some dimensions, but possibly not in others.",
        "keywords": [
            "Java",
            "Computer languages",
            "Usability",
            "Security",
            "Libraries",
            "Programming profession",
            "Product design",
            "Buildings",
            "Cascading style sheets",
            "Ecosystems"
        ]
    },
    {
        "title": "Semi-Proving: An Integrated Method for Program Proving, Testing, and Debugging.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.23",
        "volume": "37",
        "abstract": "We present an integrated method for program proving, testing, and debugging. Using the concept of metamorphic relations, we select necessary properties for target programs. For programs where global symbolic evaluation can be conducted and the constraint expressions involved can be solved, we can either prove that these necessary conditions for program correctness are satisfied or identify all inputs that violate the conditions. For other programs, our method can be converted into a symbolic-testing approach. Our method extrapolates from the correctness of a program for tested inputs to the correctness of the program for related untested inputs. The method supports automatic debugging through the identification of constraint expressions that reveal failures.",
        "keywords": [
            "Software testing",
            "Automatic testing",
            "Computer science",
            "Built-in self-test",
            "Software debugging",
            "Costs",
            "Automation",
            "Australia Council",
            "Communications technology",
            "Software engineering"
        ]
    },
    {
        "title": "Verifying the Evolution of Probability Distributions Governed by a DTMC.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.80",
        "volume": "37",
        "abstract": "We propose a new probabilistic temporal logic, iLTL, which captures properties of systems whose state can be represented by probability mass functions (pmfs). Using iLTL, we can specify reachability to a state (i.e., a pmf), as well as properties representing the aggregate (expected) behavior of a system. We then consider a class of systems whose transitions are governed by a Markov Chain-in this case, the set of states a system may be in is specified by the transitions of pmfs from all potential initial states to the final state. We then provide a model checking algorithm to check iLTL properties of such systems. Unlike existing model checking techniques, which either compute the portions of the computational paths that satisfy a specification or evaluate properties along a single path of pmf transitions, our model checking technique enables us to do a complete analysis on the expected behaviors of large-scale systems. Desirable system parameters may also be found as a counterexample of a negated goal. Finally, we illustrate the usefulness of iLTL model checking by means of two examples: assessing software reliability and ensuring the results of administering a drug.",
        "keywords": [
            "Markov processes",
            "Limiting",
            "Eigenvalues and eigenfunctions",
            "Computational modeling",
            "Transient analysis",
            "Probability distribution",
            "Steady-state"
        ]
    },
    {
        "title": "Editorial: What Makes a Publication Archival?",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2011.34",
        "volume": "37"
    },
    {
        "title": "A Comparative Study of Software Model Checkers as Unit Testing Tools: An Industrial Case Study.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.68",
        "volume": "37",
        "abstract": "Conventional testing methods often fail to detect hidden flaws in complex embedded software such as device drivers or file systems. This deficiency incurs significant development and support/maintenance cost for the manufacturers. Model checking techniques have been proposed to compensate for the weaknesses of conventional testing methods through exhaustive analyses. Whereas conventional model checkers require manual effort to create an abstract target model, modern software model checkers remove this overhead by directly analyzing a target C program, and can be utilized as unit testing tools. However, since software model checkers are not fully mature yet, they have limitations according to the underlying technologies and tool implementations, potentially critical issues when applied in industrial projects. This paper reports our experience in applying Blast and CBMC to testing the components of a storage platform software for flash memory. Through this project, we analyzed the strong and weak points of two different software model checking technologies in the viewpoint of real-world industrial application-counterexample-guided abstraction refinement with predicate abstraction and SAT-based bounded analysis.",
        "keywords": [
            "Software tools",
            "Software testing",
            "Computer industry",
            "Manufacturing industries",
            "System testing",
            "Embedded software",
            "File systems",
            "Costs",
            "Flash memory",
            "Refining"
        ]
    },
    {
        "title": "Assessing, Comparing, and Combining State Machine-Based Testing and Structural Testing: A Series of Experiments.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.32",
        "volume": "37",
        "abstract": "A large number of research works have addressed the importance of models in software engineering. However, the adoption of model-based techniques in software organizations is limited since these models are perceived to be expensive and not necessarily cost-effective. Focusing on model-based testing, this paper reports on a series of controlled experiments. It investigates the impact of state machine testing on fault detection in class clusters and its cost when compared with structural testing. Based on previous work showing this is a good compromise in terms of cost and effectiveness, this paper focuses on a specific state-based technique: the round-trip paths coverage criterion. Round-trip paths testing is compared to structural testing, and it is investigated whether they are complementary. Results show that even when a state machine models the behavior of the cluster under test as accurately as possible, no significant difference between the fault detection effectiveness of the two test strategies is observed, while the two test strategies are significantly more effective when combined by augmenting state machine testing with structural testing. A qualitative analysis also investigates the reasons why test techniques do not detect certain faults and how the cost of state machine testing can be brought down.",
        "keywords": [
            "Object oriented modeling",
            "Costs",
            "Fault detection",
            "Unified modeling language",
            "System testing",
            "Software testing",
            "Automatic testing",
            "Software engineering",
            "Software design",
            "Logic testing"
        ]
    },
    {
        "title": "Automatically Detecting and Tracking Inconsistencies in Software Design Models.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.38",
        "volume": "37",
        "abstract": "Software models typically contain many inconsistencies and consistency checkers help engineers find them. Even if engineers are willing to tolerate inconsistencies, they are better off knowing about their existence to avoid follow-on errors and unnecessary rework. However, current approaches do not detect or track inconsistencies fast enough. This paper presents an automated approach for detecting and tracking inconsistencies in real time (while the model changes). Engineers only need to define consistency rules-in any language-and our approach automatically identifies how model changes affect these consistency rules. It does this by observing the behavior of consistency rules to understand how they affect the model. The approach is quick, correct, scalable, fully automated, and easy to use as it does not require any special skills from the engineers using it. We evaluated the approach on 34 models with model sizes of up to 162,237 model elements and 24 types of consistency rules. Our empirical evaluation shows that our approach requires only 1.4 ms to reevaluate the consistency of the model after a change (on average); its performance is not noticeably affected by the model size and common consistency rules but only by the number of consistency rules, at the expense of a quite acceptable, linearly increasing memory consumption.",
        "keywords": [
            "Software design",
            "Feedback",
            "Design engineering",
            "Maintenance engineering",
            "Best practices",
            "Software engineering",
            "Programming profession"
        ]
    },
    {
        "title": "Improving Source Code Lexicon via Traceability and Information Retrieval.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.89",
        "volume": "37",
        "abstract": "The paper presents an approach helping developers to maintain source code identifiers and comments consistent with high-level artifacts. Specifically, the approach computes and shows the textual similarity between source code and related high-level artifacts. Our conjecture is that developers are induced to improve the source code lexicon, i.e., terms used in identifiers or comments, if the software development environment provides information about the textual similarity between the source code under development and the related high-level artifacts. The proposed approach also recommends candidate identifiers built from high-level artifacts related to the source code under development and has been implemented as an Eclipse plug-in, called COde Comprehension Nurturant Using Traceability (COCONUT). The paper also reports on two controlled experiments performed with master's and bachelor's students. The goal of the experiments is to evaluate the quality of identifiers and comments (in terms of their consistency with high-level artifacts) in the source code produced when using or not using COCONUT. The achieved results confirm our conjecture that providing the developers with similarity between code and high-level artifacts helps to improve the quality of source code lexicon. This indicates the potential usefulness of COCONUT as a feature for software development environments.",
        "keywords": [
            "Documentation",
            "Programming",
            "Large scale integration",
            "Semantics",
            "Software quality",
            "Couplings"
        ]
    },
    {
        "title": "Loupe: Verifying Publish-Subscribe Architectures with a Magnifying Lens.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.39",
        "volume": "37",
        "abstract": "The Publish-Subscribe (P/S) communication paradigm fosters high decoupling among distributed components. This facilitates the design of dynamic applications, but also impacts negatively on their verification, making it difficult to reason on the overall federation of components. In addition, existing P/S infrastructures offer radically different features to the applications, e.g., in terms of message reliability. This further complicates the verification as its outcome depends on the specific guarantees provided by the underlying P/S system. Although model checking has been proposed as a tool for the verification of P/S architectures, existing solutions overlook many characteristics of the underlying communication infrastructure to avoid state explosion problems. To overcome these limitations, the Loupe domain-specific model checker adopts a different approach. The P/S infrastructure is not modeled on top of a general-purpose model checker. Instead, it is embedded within the checking engine, and the traditional P/S operations become part of the modeling language. In this paper, we describe Loupe's design and the dedicated state abstractions that enable accurate verification without incurring state explosion problems. We also illustrate our use of state-of-the-art software verification tools to assess some key functionality in Loupe's current implementation. A complete case study shows how Loupe eases the verification of P/S architectures. Finally, we quantitatively compare Loupe's performance against alternative approaches. The results indicate that Loupe is effective and efficient in enabling accurate verification of P/S architectures.",
        "keywords": [
            "Publish-subscribe",
            "Lenses",
            "Application software",
            "Explosions",
            "Computer architecture",
            "Context",
            "Engines",
            "Software tools",
            "Software systems",
            "Business communication"
        ]
    },
    {
        "title": "Self-Supervising BPEL Processes.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.37",
        "volume": "37",
        "abstract": "Service compositions suffer changes in their partner services. Even if the composition does not change, its behavior may evolve over time and become incorrect. Such changes cannot be fully foreseen through prerelease validation, but impose a shift in the quality assessment activities. Provided functionality and quality of service must be continuously probed while the application executes, and the application itself must be able to take corrective actions to preserve its dependability and robustness. We propose the idea of self-supervising BPEL processes, that is, special-purpose compositions that assess their behavior and react through user-defined rules. Supervision consists of monitoring and recovery. The former checks the system's execution to see whether everything is proceeding as planned, while the latter attempts to fix any anomalies. The paper introduces two languages for defining monitoring and recovery and explains how to use them to enrich BPEL processes with self-supervision capabilities. Supervision is treated as a cross-cutting concern that is only blended at runtime, allowing different stakeholders to adopt different strategies with no impact on the actual business logic. The paper also presents a supervision-aware runtime framework for executing the enriched processes, and briefly discusses the results of in-lab experiments and of a first evaluation with industrial partners.",
        "keywords": [
            "Runtime",
            "Monitoring",
            "Robustness",
            "Software engineering",
            "Application software",
            "Quality assessment",
            "Quality of service",
            "Logic",
            "Software performance",
            "Software tools"
        ]
    },
    {
        "title": "Software Module Clustering as a Multi-Objective Search Problem.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.26",
        "volume": "37",
        "abstract": "Software module clustering is the problem of automatically organizing software units into modules to improve program structure. There has been a great deal of recent interest in search-based formulations of this problem in which module boundaries are identified by automated search, guided by a fitness function that captures the twin objectives of high cohesion and low coupling in a single-objective fitness function. This paper introduces two novel multi-objective formulations of the software module clustering problem, in which several different objectives (including cohesion and coupling) are represented separately. In order to evaluate the effectiveness of the multi-objective approach, a set of experiments was performed on 17 real-world module clustering problems. The results of this empirical study provide strong evidence to support the claim that the multi-objective approach produces significantly better solutions than the existing single-objective approach.",
        "keywords": [
            "Search problems",
            "Computer science",
            "Performance evaluation",
            "Software engineering",
            "Clustering algorithms",
            "Computational intelligence",
            "Testing",
            "Educational institutions",
            "Computer applications",
            "Application software"
        ]
    },
    {
        "title": "Systematic Review and Aggregation of Empirical Studies on Elicitation Techniques.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.33",
        "volume": "37",
        "abstract": "We have located the results of empirical studies on elicitation techniques and aggregated these results to gather empirically grounded evidence. Our chosen surveying methodology was systematic review, whereas we used an adaptation of comparative analysis for aggregation because meta-analysis techniques could not be applied. The review identified 564 publications from the SCOPUS, IEEEXPLORE, and ACM DL databases, as well as Google. We selected and extracted data from 26 of those publications. The selected publications contain 30 empirical studies. These studies were designed to test 43 elicitation techniques and 50 different response variables. We got 100 separate results from the experiments. The aggregation generated 17 pieces of knowledge about the interviewing, laddering, sorting, and protocol analysis elicitation techniques. We provide a set of guidelines based on the gathered pieces of knowledge.",
        "keywords": [
            "Protocols",
            "Information analysis",
            "Sensitivity analysis",
            "Databases",
            "Data mining",
            "Sorting",
            "Software measurement"
        ]
    },
    {
        "title": "Guest Editors' Introduction: Special Section on the Socio-Technical Environment of Software Development Projects.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2011.53",
        "volume": "37"
    },
    {
        "title": "Does Socio-Technical Congruence Have an Effect on Software Build Success? A Study of Coordination in a Software Project.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2011.29",
        "volume": "37",
        "abstract": "Socio-technical congruence is an approach that measures coordination by examining the alignment between the technical dependencies and the social coordination in the project. We conduct a case study of coordination in the IBM Rational Team Concert project, which consists of 151 developers over seven geographically distributed sites, and expect that high congruence leads to a high probability of successful builds. We examine this relationship by applying two congruence measurements: an unweighted congruence measure from previous literature, and a weighted measure that overcomes limitations of the existing measure. We discover that there is a relationship between socio-technical congruence and build success probability, but only for certain build types, and observe that in some situations, higher congruence actually leads to lower build success rates. We also observe that a large proportion of zero-congruence builds are successful, and that socio-technical gaps in successful builds are larger than gaps in failed builds. Analysis of the social and technical aspects in IBM Rational Team Concert allows us to discuss the effects of congruence on build success. Our findings provide implications with respect to the limits of applicability of socio-technical congruence and suggest further improvements of socio-technical congruence to study coordination.",
        "keywords": [
            "Software",
            "Weight measurement",
            "Programming",
            "Software measurement",
            "Collaboration",
            "Software engineering",
            "Context"
        ]
    },
    {
        "title": "The Awareness Network, To Whom Should I Display My Actions? And, Whose Actions Should I Monitor?",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2011.19",
        "volume": "37",
        "abstract": "The concept of awareness plays a pivotal role in research in Computer-Supported Cooperative Work. Recently, software engineering researchers interested in the collaborative nature of software development have explored the implications of this concept in the design of software development tools. A critical aspect of awareness is the associated coordinative work practices of displaying and monitoring actions. This aspect concerns how colleagues monitor one another's actions to understand how these actions impact their own work and how they display their actions in such a way that others can easily monitor them while doing their own work. In this paper, we focus on an additional aspect of awareness: the identification of the social actors who should be monitored and the actors to whom their actions should be displayed. We address this aspect by presenting software developers' work practices based on ethnographic data from three different software development teams. In addition, we illustrate how these work practices are influenced by different factors, including the organizational setting, the age of the project, and the software architecture. We discuss how our results are relevant for both CSCW and software engineering researchers.",
        "keywords": [
            "Software",
            "Programming",
            "Monitoring",
            "Interviews",
            "Collaboration",
            "Servers"
        ]
    },
    {
        "title": "A Controlled Experiment for Program Comprehension through Trace Visualization.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.47",
        "volume": "37",
        "abstract": "Software maintenance activities require a sufficient level of understanding of the software at hand that unfortunately is not always readily available. Execution trace visualization is a common approach in gaining this understanding, and among our own efforts in this context is Extravis, a tool for the visualization of large traces. While many such tools have been evaluated through case studies, there have been no quantitative evaluations to the present day. This paper reports on the first controlled experiment to quantitatively measure the added value of trace visualization for program comprehension. We designed eight typical tasks aimed at gaining an understanding of a representative subject system, and measured how a control group (using the Eclipse IDE) and an experimental group (using both Eclipse and Extravis) performed these tasks in terms of time spent and solution correctness. The results are statistically significant in both regards, showing a 22 percent decrease in time requirements and a 43 percent increase in correctness for the group using trace visualization.",
        "keywords": [
            "Visualization",
            "Computer Society",
            "Time measurement",
            "Programming",
            "Documentation",
            "Scalability",
            "Software maintenance",
            "Gain measurement",
            "Control systems",
            "Performance evaluation"
        ]
    },
    {
        "title": "A General Software Defect-Proneness Prediction Framework.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.90",
        "volume": "37",
        "abstract": "BACKGROUND - Predicting defect-prone software components is an economically important activity and so has received a good deal of attention. However, making sense of the many, and sometimes seemingly inconsistent, results is difficult. OBJECTIVE - We propose and evaluate a general framework for software defect prediction that supports 1) unbiased and 2) comprehensive comparison between competing prediction systems. METHOD - The framework is comprised of 1) scheme evaluation and 2) defect prediction components. The scheme evaluation analyzes the prediction performance of competing learning schemes for given historical data sets. The defect predictor builds models according to the evaluated learning scheme and predicts software defects with new data according to the constructed model. In order to demonstrate the performance of the proposed framework, we use both simulation and publicly available software defect data sets. RESULTS - The results show that we should choose different learning schemes for different data sets (i.e., no scheme dominates), that small details in conducting how evaluations are conducted can completely reverse findings, and last, that our proposed framework is more effective and less prone to bias than previous approaches. CONCLUSIONS - Failure to properly or fully evaluate a learning scheme can be misleading; however, these problems may be overcome by our proposed framework.",
        "keywords": [
            "Software",
            "Training data",
            "Predictive models",
            "Buildings",
            "Data models",
            "Prediction algorithms",
            "Training"
        ]
    },
    {
        "title": "An Attack Surface Metric.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.60",
        "volume": "37",
        "abstract": "Measurement of software security is a long-standing challenge to the research community. At the same time, practical security metrics and measurements are essential for secure software development. Hence, the need for metrics is more pressing now due to a growing demand for secure software. In this paper, we propose using a software system's attack surface measurement as an indicator of the system's security. We formalize the notion of a system's attack surface and introduce an attack surface metric to measure the attack surface in a systematic manner. Our measurement method is agnostic to a software system's implementation language and is applicable to systems of all sizes; we demonstrate our method by measuring the attack surfaces of small desktop applications and large enterprise systems implemented in C and Java. We conducted three exploratory empirical studies to validate our method. Software developers can mitigate their software's security risk by measuring and reducing their software's attack surfaces. Our attack surface reduction approach complements the software industry's traditional code quality improvement approach for security risk mitigation and is useful in multiple phases of the software development lifecycle. Our collaboration with SAP demonstrates the use of our metric in the software development process.",
        "keywords": [
            "Software measurement",
            "Security",
            "Programming",
            "Software systems",
            "Size measurement",
            "Time measurement",
            "Pressing",
            "Application software",
            "Java",
            "Software quality"
        ]
    },
    {
        "title": "Dynamic QoS Management and Optimization in Service-Based Systems.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.92",
        "volume": "37",
        "abstract": "Service-based systems that are dynamically composed at runtime to provide complex, adaptive functionality are currently one of the main development paradigms in software engineering. However, the Quality of Service (QoS) delivered by these systems remains an important concern, and needs to be managed in an equally adaptive and predictable way. To address this need, we introduce a novel, tool-supported framework for the development of adaptive service-based systems called QoSMOS (QoS Management and Optimization of Service-based systems). QoSMOS can be used to develop service-based systems that achieve their QoS requirements through dynamically adapting to changes in the system state, environment, and workload. QoSMOS service-based systems translate high-level QoS requirements specified by their administrators into probabilistic temporal logic formulae, which are then formally and automatically analyzed to identify and enforce optimal system configurations. The QoSMOS self-adaptation mechanism can handle reliability and performance-related QoS requirements, and can be integrated into newly developed solutions or legacy systems. The effectiveness and scalability of the approach are validated using simulations and a set of experiments based on an implementation of an adaptive service-based system for remote medical assistance.",
        "keywords": [
            "Quality of service",
            "Markov processes",
            "Probabilistic logic",
            "Unified modeling language",
            "Analytical models",
            "Optimization",
            "Scattering"
        ]
    },
    {
        "title": "Efficient Consistency Measurement Based on Behavioral Profiles of Process Models.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.96",
        "volume": "37",
        "abstract": "Engineering of process-driven business applications can be supported by process modeling efforts in order to bridge the gap between business requirements and system specifications. However, diverging purposes of business process modeling initiatives have led to significant problems in aligning related models at different abstract levels and different perspectives. Checking the consistency of such corresponding models is a major challenge for process modeling theory and practice. In this paper, we take the inappropriateness of existing strict notions of behavioral equivalence as a starting point. Our contribution is a concept called behavioral profile that captures the essential behavioral constraints of a process model. We show that these profiles can be computed efficiently, i.e., in cubic time for sound free-choice Petri nets w.r.t. their number of places and transitions. We use behavioral profiles for the definition of a formal notion of consistency which is less sensitive to model projections than common criteria of behavioral equivalence and allows for quantifying deviation in a metric way. The derivation of behavioral profiles and the calculation of a degree of consistency have been implemented to demonstrate the applicability of our approach. We also report the findings from checking consistency between partially overlapping models of the SAP reference model.",
        "keywords": [
            "Unified modeling language",
            "Business",
            "Analytical models",
            "Semantics",
            "Computational modeling",
            "Petri nets",
            "Software"
        ]
    },
    {
        "title": "Which Crashes Should I Fix First?: Predicting Top Crashes at an Early Stage to Prioritize Debugging Efforts.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2011.20",
        "volume": "37",
        "abstract": "Many popular software systems automatically report failures back to the vendors, allowing developers to focus on the most pressing problems. However, it takes a certain period of time to assess which failures occur most frequently. In an empirical investigation of the Firefox and Thunderbird crash report databases, we found that only 10 to 20 crashes account for the large majority of crash reports; predicting these “top crashes” thus could dramatically increase software quality. By training a machine learner on the features of top crashes of past releases, we can effectively predict the top crashes well before a new release. This allows for quick resolution of the most important crashes, leading to improved user experience and better allocation of maintenance efforts.",
        "keywords": [
            "Fires",
            "Feature extraction",
            "Software",
            "Testing",
            "Computer bugs",
            "Training"
        ]
    },
    {
        "title": "A Controlled Experiment for Evaluating the Impact of Coupling on the Maintainability of Service-Oriented Software.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.61",
        "volume": "37",
        "abstract": "One of the goals of Service-Oriented Computing (SOC) is to improve software maintainability as businesses become more agile, and thus underlying processes and rules change more frequently. This paper presents a controlled experiment examining the relationship between coupling in service-oriented designs, as measured using a recently proposed suite of SOC-specific coupling metrics and software maintainability in terms of the specific subcharacteristics of analyzability, changeability, and stability. The results indicate a statistically significant causal relationship between the investigated coupling metrics and the maintainability of service-oriented software. As such, the investigated metrics can facilitate coupling related design decisions with the aim of producing more maintainable service-oriented software products.",
        "keywords": [
            "Software maintenance",
            "Programming",
            "Software measurement",
            "Logic",
            "Software design",
            "Stability analysis",
            "Product design",
            "Application software",
            "Costs",
            "Software metrics"
        ]
    },
    {
        "title": "A Flowchart Language for Quantum Programming.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.94",
        "volume": "37",
        "abstract": "Several high-level quantum programming languages have been proposed in the previous research. In this paper, we define a low-level flowchart language for quantum programming, which can be used in implementation of high-level quantum languages and in design of quantum compilers. The formal semantics of the flowchart language is given, and the notion of correctness for programs written in this language is introduced. A structured quantum programming theorem is presented, which provides a technique of translating quantum flowchart programs into programs written in a high-level language, namely, a quantum extension of the while-language.",
        "keywords": [
            "Quantum computing",
            "Semantics",
            "Quantum mechanics",
            "Programming",
            "Computer languages",
            "Probabilistic logic",
            "Computers"
        ]
    },
    {
        "title": "Dynamic Analysis for Diagnosing Integration Faults.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.93",
        "volume": "37",
        "abstract": "Many software components are provided with incomplete specifications and little access to the source code. Reusing such gray-box components can result in integration faults that can be difficult to diagnose and locate. In this paper, we present Behavior Capture and Test (BCT), a technique that uses dynamic analysis to automatically identify the causes of failures and locate the related faults. BCT augments dynamic analysis techniques with model-based monitoring. In this way, BCT identifies a structured set of interactions and data values that are likely related to failures (failure causes), and indicates the components and the operations that are likely responsible for failures (fault locations). BCT advances scientific knowledge in several ways. It combines classic dynamic analysis with incremental finite state generation techniques to produce dynamic models that capture complementary aspects of component interactions. It uses an effective technique to filter false positives to reduce the effort of the analysis of the produced data. It defines a strategy to extract information about likely causes of failures by automatically ranking and relating the detected anomalies so that developers can focus their attention on the faults. The effectiveness of BCT depends on the quality of the dynamic models extracted from the program. BCT is particularly effective when the test cases sample the execution space well. In this paper, we present a set of case studies that illustrate the adequacy of BCT to analyze both regression testing failures and rare field failures. The results show that BCT automatically filters out most of the false alarms and provides useful information to understand the causes of failures in 69 percent of the case studies.",
        "keywords": [
            "Automata",
            "Monitoring",
            "Analytical models",
            "Engines",
            "Software",
            "Testing",
            "Inference algorithms"
        ]
    },
    {
        "title": "Empirical Studies of Pair Programming for CS/SE Teaching in Higher Education: A Systematic Literature Review.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.59",
        "volume": "37",
        "abstract": "The objective of this paper is to present the current evidence relative to the effectiveness of pair programming (PP) as a pedagogical tool in higher education CS/SE courses. We performed a systematic literature review (SLR) of empirical studies that investigated factors affecting the effectiveness of PP for CS/SE students and studies that measured the effectiveness of PP for CS/SE students. Seventy-four papers were used in our synthesis of evidence, and 14 compatibility factors that can potentially affect PP's effectiveness as a pedagogical tool were identified. Results showed that students' skill level was the factor that affected PP's effectiveness the most. The most common measure used to gauge PP's effectiveness was time spent on programming. In addition, students' satisfaction when using PP was overall higher than when working solo. Our meta-analyses showed that PP was effective in improving students' grades on assignments. Finally, in the studies that used quality as a measure of effectiveness, the number of test cases succeeded, academic performance, and expert opinion were the quality measures mostly applied. The results of this SLR show two clear gaps in this research field: 1) a lack of studies focusing on pair compatibility factors aimed at making PP an effective pedagogical tool and 2) a lack of studies investigating PP for software design/modeling tasks in conjunction with programming tasks.",
        "keywords": [
            "Programming profession",
            "Education",
            "Educational programs",
            "Computer science",
            "Performance evaluation",
            "Time measurement",
            "Testing",
            "Software design",
            "Collaborative work",
            "Algorithm design and analysis"
        ]
    },
    {
        "title": "FlowTalk: Language Support for Long-Latency Operations in Embedded Devices.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.66",
        "volume": "37",
        "abstract": "Wireless sensor networks necessitate a programming model different from those used to develop desktop applications. Typically, resources in terms of power and memory are constrained. C is the most common programming language used to develop applications on very small embedded sensor devices. We claim that C does not provide efficient mechanisms to address the implicit asynchronous nature of sensor sampling. C applications for these devices suffer from a disruption in their control flow. In this paper, we present FlowTalk, a new object-oriented programming language aimed at making software development for wireless embedded sensor devices easier. FlowTalk is an object-oriented programming language in which dynamicity (e.g., object creation) has been traded for a reduction in memory consumption. The event model that traditionally comes from using sensors is adapted in FlowTalk with controlled disruption, a light-weight continuation mechanism. The essence of our model is to turn asynchronous long-latency operations into synchronous and blocking method calls. FlowTalk is built for TinyOS and can be used to develop applications that can fit in 4 KB of memory for a large number of wireless sensor devices.",
        "keywords": [
            "Sampling methods",
            "Object oriented modeling",
            "Wireless sensor networks",
            "Application software",
            "Computer languages",
            "Object oriented programming",
            "Java",
            "Biosensors",
            "Automotive engineering",
            "Embedded software"
        ]
    },
    {
        "title": "Frameworks Generate Domain-Specific Languages: A Case Study in the Multimedia Domain.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.48",
        "volume": "37",
        "abstract": "We present an approach to software framework development that includes the generation of domain-specific languages (DSLs) and pattern languages as goals for the process. Our model is made of three workflows-framework, metamodel, and patterns-and three phases-inception, construction, and formalization. The main conclusion is that when developing a framework, we can produce with minimal overhead-almost as a side effect-a metamodel with an associated DSL and a pattern language. Both outputs will not only help the framework evolve in the right direction, but will also be valuable in themselves. In order to illustrate these ideas, we present a case study in the multimedia domain. For several years, we have been developing a multimedia framework. The process has produced a full-fledged domain-specific metamodel for the multimedia domain, with an associated DSL and a pattern language.",
        "keywords": [
            "Domain specific languages",
            "DSL",
            "Unified modeling language",
            "Vocabulary",
            "Concrete",
            "Software engineering",
            "Computer aided software engineering",
            "Natural languages",
            "Metamodeling",
            "Best practices"
        ]
    },
    {
        "title": "GUI Interaction Testing: Incorporating Event Context.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.50",
        "volume": "37",
        "abstract": "Graphical user interfaces (GUIs), due to their event-driven nature, present an enormous and potentially unbounded way for users to interact with software. During testing, it is important to “adequately cover” this interaction space. In this paper, we develop a new family of coverage criteria for GUI testing grounded in combinatorial interaction testing. The key motivation of using combinatorial techniques is that they enable us to incorporate “context” into the criteria in terms of event combinations, sequence length, and by including all possible positions for each event. Our new criteria range in both efficiency (measured by the size of the test suite) and effectiveness (the ability of the test suites to detect faults). In a case study on eight applications, we automatically generate test cases and systematically explore the impact of context, as captured by our new criteria. Our study shows that by increasing the event combinations tested and by controlling the relative positions of events defined by the new criteria, we can detect a large number of faults that were undetectable by earlier techniques.",
        "keywords": [
            "Graphical user interfaces",
            "System testing",
            "Software testing",
            "Automatic testing",
            "Fault detection",
            "Context modeling",
            "Computer science",
            "Software performance",
            "Logic testing",
            "User interfaces"
        ]
    },
    {
        "title": "Zebu: A Language-Based Approach for Network Protocol Message Processing.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.64",
        "volume": "37",
        "abstract": "A network application communicates with other applications according to a set of rules known as a protocol. This communication is managed by the part of the application known as the protocol-handling layer, which enables the manipulation of protocol messages. The protocol-handling layer is a critical component of a network application since it represents the interface between the application and the outside world. It must thus satisfy two constraints: It must be efficient to be able to treat a large number of messages and it must be robust to face various attacks targeting the application itself or the underlying platform. Despite these constraints, the development process of this layer still remains rudimentary and requires a high level of expertise. It includes translating the protocol specification written in a high-level formalism such as ABNF toward low-level code such as C. The gap between these abstraction levels can entail many errors. This paper proposes a new language-based approach to developing protocol-handling layers, to improve their robustness without compromising their performance. Our approach is based on the use of a domain-specific language, Zebu, to specify the protocol-handling layer of network applications that use textual HTTP-like application protocols. The Zebu syntax is very close to that of ABNF, facilitating the adoption of Zebu by domain experts. By annotating the original ABNF specification of a protocol, the Zebu user can dedicate the protocol-handling layer to the needs of a given application. The Zebu compiler first checks the annotated specification for inconsistencies, and then generates a protocol-handling layer according to the annotations. This protocol-handling layer is made up of a set of data structures that represent a message, a parser that fills in these data structures, and various stub functions to access these data structures or drive the parsing of a message.",
        "keywords": [
            "Electronic mail",
            "Data structures",
            "Robustness",
            "Domain specific languages",
            "Access protocols",
            "Streaming media",
            "IP networks",
            "Network servers",
            "Web server",
            "Computer bugs"
        ]
    },
    {
        "title": "A Classification Framework for Software Component Models.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.83",
        "volume": "37",
        "abstract": "In the last decade, a large number of different software component models have been developed, with different aims and using different principles and technologies. This has resulted in a number of models which have many similarities, but also principal differences, and in many cases unclear concepts. Component-based development has not succeeded in providing standard principles, as has, for example, object-oriented development. In order to increase the understanding of the concepts and to differentiate component models more easily, this paper identifies, discusses, and characterizes fundamental principles of component models and provides a Component Model Classification Framework based on these principles. Further, the paper classifies a large number of component models using this framework.",
        "keywords": [
            "Data models",
            "Bismuth",
            "Packaging"
        ]
    },
    {
        "title": "A Comparison of Tabular Expression-Based Testing Strategies.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2011.78",
        "volume": "37",
        "abstract": "Tabular expressions have been proposed as a notation to document mathematically precise but readable software specifications. One of the many roles of such documentation is to guide testers. This paper 1) explores the application of four testing strategies (the partition strategy, decision table-based testing, the basic meaningful impact strategy, and fault-based testing) to tabular expression-based specifications, and 2) compares the strategies on a mathematical basis through formal and precise definitions of the subsumption relationship. We also compare these strategies through experimental studies. These results will help researchers improve current methods and will enable testers to select appropriate testing strategies for tabular expression-based specifications.",
        "keywords": [
            "Testing",
            "Redundancy",
            "Documentation",
            "Software quality",
            "Software engineering",
            "Electronic mail"
        ]
    },
    {
        "title": "A Risk Management Methodology for Project Risk Dependencies.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.108",
        "volume": "37",
        "abstract": "Project risks are not always independent, yet current risk management practices do not clearly manage dependencies between risks. If dependencies can be explicitly identified and analyzed, project managers will be able to develop better risk management strategies and make more effective risk planning decisions. This paper proposes a management methodology to address risk dependency issues. Through the study of three IT projects, we confirm that risk dependencies do exist in projects and can be identified and systematically managed. We also observed that, as project teams needed to deal with risk dependency issues, communications between projects were improved, and there were synergetic effects in managing risks and risk dependencies among projects.",
        "keywords": [
            "Risk management",
            "Delta modulation",
            "Analytical models",
            "Monitoring",
            "Measurement",
            "Lead",
            "Fault trees"
        ]
    },
    {
        "title": "An Analysis and Survey of the Development of Mutation Testing.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.62",
        "volume": "37",
        "abstract": "Mutation Testing is a fault-based software testing technique that has been widely studied for over three decades. The literature on Mutation Testing has contributed a set of approaches, tools, developments, and empirical results. This paper provides a comprehensive analysis and survey of Mutation Testing. The paper also presents the results of several development trend analyses. These analyses provide evidence that Mutation Testing techniques and tools are reaching a state of maturity and applicability, while the topic of Mutation Testing itself is the subject of increasing interest.",
        "keywords": [
            "Genetic mutations",
            "Software testing",
            "Fault detection",
            "History",
            "Books",
            "Programming profession",
            "Computer languages",
            "Java",
            "Educational institutions",
            "Automata"
        ]
    },
    {
        "title": "Dynamic Software Updating Using a Relaxed Consistency Model.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.79",
        "volume": "37",
        "abstract": "Software is inevitably subject to changes. There are patches and upgrades that close vulnerabilities, fix bugs, and evolve software with new features. Unfortunately, most traditional dynamic software updating approaches suffer some level of limitations; few of them can update multithreaded applications when involving data structure changes, while some of them lose binary compatibility or incur nonnegligible performance overhead. This paper presents POLUS, a software maintenance tool capable of iteratively evolving running unmodified multithreaded software into newer versions, yet with very low performance overhead. The main idea in POLUS is a relaxed consistency model that permits the concurrent activity of the old and new code. POLUS borrows the idea of cache-coherence protocol in computer architecture and uses a ”bidirectional write-through” synchronization protocol to ensure system consistency. To demonstrate the applicability of POLUS, we report our experience in using POLUS to dynamically update three prevalent server applications: vsftpd, sshd, and Apache HTTP server. Performance measurements show that POLUS incurs negligible runtime overhead on the three applications-a less than 1 percent performance degradation (but 5 percent for one case). The time to apply an update is also minimal.",
        "keywords": [
            "Software",
            "Synchronization",
            "Protocols",
            "Bidirectional control",
            "Registers",
            "Runtime"
        ]
    },
    {
        "title": "The Impact of Irrelevant and Misleading Information on Software Development Effort Estimates: A Randomized Controlled Field Experiment.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.78",
        "volume": "37",
        "abstract": "Studies in laboratory settings report that software development effort estimates can be strongly affected by effort-irrelevant and misleading information. To increase our knowledge about the importance of these effects in field settings, we paid 46 outsourcing companies from various countries to estimate the required effort of the same five software development projects. The companies were allocated randomly to either the original requirement specification or a manipulated version of the original requirement specification. The manipulations were as follows: 1) reduced length of requirement specification with no change of content, 2) information about the low effort spent on the development of the old system to be replaced, 3) information about the client's unrealistic expectations about low cost, and 4) a restriction of a short development period with start up a few months ahead. We found that the effect sizes in the field settings were much smaller than those found for similar manipulations in laboratory settings. Our findings suggest that we should be careful about generalizing to field settings the effect sizes found in laboratory settings. While laboratory settings can be useful to demonstrate the existence of an effect and better understand it, field studies may be needed to study the size and importance of these effects.",
        "keywords": [
            "Estimation",
            "Software",
            "Companies",
            "Laboratories",
            "Programming",
            "Materials",
            "Project management"
        ]
    },
    {
        "title": "Toward a Formalism for Conservative Claims about the Dependability of Software-Based Systems.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.67",
        "volume": "37",
        "abstract": "In recent work, we have argued for a formal treatment of confidence about the claims made in dependability cases for software-based systems. The key idea underlying this work is \"the inevitability of uncertainty\": It is rarely possible to assert that a claim about safety or reliability is true with certainty. Much of this uncertainty is epistemic in nature, so it seems inevitable that expert judgment will continue to play an important role in dependability cases. Here, we consider a simple case where an expert makes a claim about the probability of failure on demand (pfd) of a subsystem of a wider system and is able to express his confidence about that claim probabilistically. An important, but difficult, problem then is how such subsystem (claim, confidence) pairs can be propagated through a dependability case for a wider system, of which the subsystems are components. An informal way forward is to justify, at high confidence, a strong claim, and then, conservatively, only claim something much weaker: \"I'm 99 percent confident that the pfd is less than 10\n<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">-5</sup>\n, so it's reasonable to be 100 percent confident that it is less than 10\n<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">-3</sup>\n.\" These conservative pfds of subsystems can then be propagated simply through the dependability case of the wider system. In this paper, we provide formal support for such reasoning.",
        "keywords": [
            "Uncertainty",
            "Software reliability",
            "Phase frequency detector",
            "Battery powered vehicles",
            "Software systems",
            "Software safety",
            "Programming",
            "Power engineering computing",
            "Reliability engineering",
            "Power engineering and energy"
        ]
    },
    {
        "title": "WAM - The Weighted Average Method for Predicting the Performance of Systems with Bursts of Customer Sessions.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2011.65",
        "volume": "37",
        "abstract": "Predictive performance models are important tools that support system sizing, capacity planning, and systems management exercises. We introduce the Weighted Average Method (WAM) to improve the accuracy of analytic predictive performance models for systems with bursts of concurrent customers. WAM considers the customer population distribution at a system to reflect the impact of bursts. The WAM approach is robust with respect to distribution functions, including heavy-tail-like distributions, for workload parameters. We demonstrate the effectiveness of WAM using a case study involving a multitier TPC-W benchmark system. To demonstrate the utility of WAM with multiple performance modeling approaches, we developed both Queuing Network Models and Layered Queuing Models for the system. Results indicate that WAM improves prediction accuracy for bursty workloads for QNMs and LQMs by 10 and 12 percent, respectively, with respect to a Markov Chain approach reported in the literature.",
        "keywords": [
            "Markov processes",
            "Analytical models",
            "Predictive models",
            "Accuracy",
            "Queueing analysis",
            "Time factors",
            "Software"
        ]
    },
    {
        "title": "A Dynamic Slicing Technique for UML Architectural Models.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.112",
        "volume": "37",
        "abstract": "This paper proposes a technique for dynamic slicing of UML architectural models. The presence of related information in diverse model parts (or fragments) makes dynamic slicing of Unified Modeling Language (UML) models a complex problem. We first extract all relevant information from a UML model specifying a software architecture into an intermediate representation, which we call a Model Dependency Graph (MDG). For a given slicing criterion, our slicing algorithm traverses the constructed MDG to identify the relevant model parts that are directly or indirectly affected during the execution of a specified scenario. One novelty of our approach is computation of dynamic slice based on the structural and behavioral (interactions only) UML models as against independently processing separate UML models, and determining the implicit interdependencies among different model elements distributed across model views. We also briefly discuss a prototype tool named Archlice, which we have developed to implement our algorithm.",
        "keywords": [
            "Unified modeling language",
            "Computational modeling",
            "Heuristic algorithms",
            "Computer architecture",
            "Analytical models",
            "Software architecture",
            "Software algorithms"
        ]
    },
    {
        "title": "Evaluating Complexity, Code Churn, and Developer Activity Metrics as Indicators of Software Vulnerabilities.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.81",
        "volume": "37",
        "abstract": "Security inspection and testing require experts in security who think like an attacker. Security experts need to know code locations on which to focus their testing and inspection efforts. Since vulnerabilities are rare occurrences, locating vulnerable code locations can be a challenging task. We investigated whether software metrics obtained from source code and development history are discriminative and predictive of vulnerable code locations. If so, security experts can use this prediction to prioritize security inspection and testing efforts. The metrics we investigated fall into three categories: complexity, code churn, and developer activity metrics. We performed two empirical case studies on large, widely used open-source projects: the Mozilla Firefox web browser and the Red Hat Enterprise Linux kernel. The results indicate that 24 of the 28 metrics collected are discriminative of vulnerabilities for both projects. The models using all three types of metrics together predicted over 80 percent of the known vulnerable files with less than 25 percent false positives for both projects. Compared to a random selection of files for inspection and testing, these models would have reduced the number of files and the number of lines of code to inspect or test by over 71 and 28 percent, respectively, for both projects.",
        "keywords": [
            "Fault diagnosis",
            "Software security",
            "Complexity theory",
            "Predictive models",
            "Charge coupled devices"
        ]
    },
    {
        "title": "Measuring the Discriminative Power of Object-Oriented Class Cohesion Metrics.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.97",
        "volume": "37",
        "abstract": "Several object-oriented cohesion metrics have been proposed in the literature. These metrics aim to measure the relationship between class members, namely, methods and attributes. Different metrics use different models to represent the connectivity pattern of cohesive interactions (CPCI) between class members. Most of these metrics are normalized to allow for easy comparison of the cohesion of different classes. However, in some cases, these metrics obtain the same cohesion values for different classes that have the same number of methods and attributes but different CPCIs. This leads to incorrectly considering the classes to be the same in terms of cohesion, even though their CPCIs clearly indicate that the degrees of cohesion are different. We refer to this as a lack of discrimination anomaly (LDA) problem. In this paper, we list and discuss cases in which the LDA problem exists, as expressed through the use of 16 cohesion metrics. In addition, we empirically study the frequent occurrence of the LDA problem when the considered metrics are applied to classes in five open source Java systems. Finally, we propose a metric and a simulation-based methodology to measure the discriminative power of cohesion metrics. The discrimination metric measures the probability that a cohesion metric will produce distinct cohesion values for classes with the same number of attributes and methods but different CPCIs. A highly discriminating cohesion metric is more desirable because it exhibits a lower chance of incorrectly considering classes to be cohesively equal when they have different CPCIs.",
        "keywords": [
            "Power measurement",
            "Object oriented modeling",
            "Software measurement",
            "Phase measurement"
        ]
    },
    {
        "title": "Preventing Temporal Violations in Scientific Workflows: Where and How.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.99",
        "volume": "37",
        "abstract": "Due to the dynamic nature of the underlying high-performance infrastructures for scientific workflows such as grid and cloud computing, failures of timely completion of important scientific activities, namely, temporal violations, often take place. Unlike conventional exception handling on functional failures, nonfunctional QoS failures such as temporal violations cannot be passively recovered. They need to be proactively prevented through dynamically monitoring and adjusting the temporal consistency states of scientific workflows at runtime. However, current research on workflow temporal verification mainly focuses on runtime monitoring, while the adjusting strategy for temporal consistency states, namely, temporal adjustment, has so far not been thoroughly investigated. For this issue, two fundamental problems of temporal adjustment, namely, where and how, are systematically analyzed and addressed in this paper. Specifically, a novel minimum probability time redundancy-based necessary and sufficient adjustment point selection strategy is proposed to address the problem of where and an innovative genetic-algorithm-based effective and efficient local rescheduling strategy is proposed to tackle the problem of how. The results of large-scale simulation experiments with generic workflows and specific real-world applications demonstrate that our temporal adjustment strategy can remarkably prevent the violations of both local and global temporal constraints in scientific workflows.",
        "keywords": [
            "Decision support systems",
            "Software reliability",
            "Quality of service",
            "Workflow management software"
        ]
    },
    {
        "title": "Putting Preemptive Time Petri Nets to Work in a V-Model SW Life Cycle.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2011.4",
        "volume": "37",
        "abstract": "Preemptive Time Petri Nets (pTPNs) support modeling and analysis of concurrent timed SW components running under fixed priority preemptive scheduling. The model is supported by a well-established theory based on symbolic state space analysis through Difference Bounds Matrix (DBM) zones, with specific contributions on compositional modularization, trace analysis, and efficient overapproximation and cleanup in the management of suspension deriving from preemptive behavior. In this paper, we devise and implement a framework that brings the theory to application. To this end, we cast the theory into an organic tailoring of design, coding, and testing activities within a V-Model SW life cycle in respect of the principles of regulatory standards applied to the construction of safety-critical SW components. To implement the toolchain subtended by the overall approach into a Model Driven Development (MDD) framework, we complement the theory of state space analysis with methods and techniques supporting semiformal specification and automated compilation into pTPN models and real-time code, measurement-based Execution Time estimation, test case selection and execution, coverage evaluation.",
        "keywords": [
            "Real time systems",
            "Analytical models",
            "Unified modeling language",
            "Petri nets",
            "Mathematical model",
            "Computer architecture"
        ]
    },
    {
        "title": "Swarm Verification Techniques.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.110",
        "volume": "37",
        "abstract": "The range of verification problems that can be solved with logic model checking tools has increased significantly in the last few decades. This increase in capability is based on algorithmic advances and new theoretical insights, but it has also benefitted from the steady increase in processing speeds and main memory sizes on standard computers. The steady increase in processing speeds, though, ended when chip-makers started redirecting their efforts to the development of multicore systems. For the near-term future, we can anticipate the appearance of systems with large numbers of CPU cores, but without matching increases in clock-speeds. We will describe a model checking strategy that can allow us to leverage this trend and that allows us to tackle significantly larger problem sizes than before.",
        "keywords": [
            "Memory management",
            "Formal verification",
            "Search problems",
            "Computational modeling",
            "Multicore processing",
            "Data models",
            "Parallel processing"
        ]
    },
    {
        "title": "Tuning Temporal Features within the Stochastic π-Calculus.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.95",
        "volume": "37",
        "abstract": "The stochastic \\pi-calculus is a formalism that has been used for modeling complex dynamical systems where the stochasticity and the delay of transitions are important features, such as in the case of biochemical reactions. Commonly, durations of transitions within stochastic \\pi-calculus models follow an exponential law. The underlying dynamics of such models are expressed in terms of continuous-time Markov chains, which can then be efficiently simulated and model-checked. However, the exponential law comes with a huge variance, making it difficult to model systems with accurate temporal constraints. In this paper, a technique for tuning temporal features within the stochastic \\pi-calculus is presented. This method relies on the introduction of a stochasticity absorption factor by replacing the exponential distribution with the Erlang distribution, which is a sum of exponential random variables. This paper presents a construction of the stochasticity absorption factor in the classical stochastic \\pi-calculus with exponential rates. Tools for manipulating the stochasticity absorption factor and its link with timed intervals for firing transitions are also presented. Finally, the model-checking of such designed models is tackled by supporting the stochasticity absorption factor in a translation from the stochastic \\pi-calculus to the probabilistic model checker PRISM.",
        "keywords": [
            "Stochastic processes",
            "Exponential distribution",
            "Random variables",
            "Analytical models"
        ]
    },
    {
        "title": "On the Distribution of Bugs in the Eclipse System.",
        "venue_name": "tse",
        "year": 2011,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2011.54",
        "volume": "37",
        "abstract": "The distribution of bugs in software systems has been shown to satisfy the Pareto principle, and typically shows a power-law tail when analyzed as a rank-frequency plot. In a recent paper, Zhang showed that the Weibull cumulative distribution is a very good fit for the Alberg diagram of bugs built with experimental data. In this paper, we further discuss the subject from a statistical perspective, using as case studies five versions of Eclipse, to show how log-normal, Double-Pareto, and Yule-Simon distributions may fit the bug distribution at least as well as the Weibull distribution. In particular, we show how some of these alternative distributions provide both a superior fit to empirical data and a theoretical motivation to be used for modeling the bug generation process. While our results have been obtained on Eclipse, we believe that these models, in particular the Yule-Simon one, can generalize to other software systems.",
        "keywords": [
            "Computer bugs",
            "Software systems",
            "Data models",
            "Computational modeling",
            "Weibull distribution",
            "Object oriented modeling"
        ]
    }
]