[
    {
        "title": "The Impact of Surface Features on Choice of (in)Secure Answers by Stackoverflow Readers.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2981317",
        "volume": "48",
        "abstract": "Existing research has shown that developers will use StackOverflow to answer programming questions: but what draws them to one particular answer over any other? The choice of answer they select can mean the difference between a secure application and insecure one, as the quality of supposedly secure answers can vary. Prior work has studied people posting on Stack Overflow—a two-way communication between the original poster and the Stack Overflow community. Instead, we study the situation of one-way communication, where people only read a Stack Overflow thread without being actively involved in it, sometimes long after a thread has closed. We report on a mixed-method study including a controlled between-groups experiment and qualitative analysis of participants’ rationale (N=1188), investigating whether explanation detail, answer scoring, accepted answer marks, as well as the security of the code snippet itself affect the answers participants accept. Our findings indicate that explanation detail affects what answers participants reading a thread select (p<0.01), while answer score and acceptance do not (p>0.05)—the \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">inverse</i>\n of what research has shown for those asking and answering questions. The qualitative analysis of participants’ rationale further explains how several cognitive biases underpin these findings. Correspondence bias, in particular, plays an important role in instilling readers with a false sense of confidence in an answer through the \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">way it looks</i>\n, regardless of whether it works, is secure, or if the community agrees with it. As a result, we argue that StackOverflow's use as a knowledge base by people not actively involved in threads—when there is only one-way-communication—may inadvertently contribute to the spread of insecure code, as the community's voting mechanisms hold little power to deter them from answers.",
        "keywords": [
            "Security",
            "Semantics",
            "Software",
            "Documentation",
            "Knowledge based systems",
            "Message systems",
            "Licenses"
        ]
    },
    {
        "title": "Dominoes: An Interactive Exploratory Data Analysis Tool for Software Relationships.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2988241",
        "volume": "48",
        "abstract": "Project comprehension questions, such as “which modified artifacts can affect my work?” and “how can I identify the developers who should be assigned to a given task?” are difficult to answer, require an analysis of the project and its data, are context specific, and cannot always be pre-defined. Current research approaches are restricted to post hoc analyses over software repositories. Very few interactive exploratory tools exist since the large amount of data that need to be analyzed prohibits its exploration at interactive rates. Moreover, such analyses typically require the user to create complex scripts or queries to extract the desired information from data. Here we present Dominoes, a tool for interactive data exploration aimed at end users (i.e., project managers or developers). Dominoes allows users to interact with different types and units of data to investigate project relationships and view intermediate results as charts, tables, and graphs. Additionally, it allows users to save the derived data as well as their exploration paths for later use. In a scenario-based evaluation study, participants achieved a success rate of 86 percent in their explorations, with a mean time of 7.25 minutes for answering a set of (project) exploration questions.",
        "keywords": [
            "Tiles",
            "Data mining",
            "Graphics processing units",
            "Tools",
            "Feature extraction"
        ]
    },
    {
        "title": "Property Satisfiability Analysis for Product Lines of Modelling Languages.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2989506",
        "volume": "48",
        "abstract": "Software engineering uses models throughout most phases of the development process. Models are defined using modelling languages. To make these languages applicable to a wider set of scenarios and customizable to specific needs, researchers have proposed using product lines to specify modelling language variants. However, there is currently a lack of efficient techniques for ensuring correctness with respect to properties of the models accepted by a set of language variants. This may prevent detecting problematic combinations of language variants that produce undesired effects at the model level. To attack this problem, we first present a classification of instantiability properties for language product lines. Then, we propose a novel approach to lifting the satisfiability checking of model properties of individual language variants, to the product line level. Finally, we report on an implementation of our proposal in the \n<sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Merlin</small>\n tool, and demonstrate the efficiency gains of our lifted analysis method compared to an enumerative analysis of each individual language variant.",
        "keywords": [
            "Unified modeling language",
            "Analytical models",
            "Petri nets",
            "Syntactics",
            "Tools",
            "Programmable logic arrays",
            "Software engineering"
        ]
    },
    {
        "title": "A3: Assisting Android API Migrations Using Code Examples.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2988396",
        "volume": "48",
        "abstract": "The fast-paced evolution of Android APIs has posed a challenging task for Android app developers. To leverage Androids frequently released APIs, developers must often spend considerable effort on API migrations. Prior research and Android official documentation typically provide enough information to guide developers in identifying the API calls that must be migrated and the corresponding API calls in an updated version of Android (<i>what</i> to migrate). However, API migration remains a challenging task since developers lack the knowledge of <i>how</i> to migrate the API calls. There exist code examples, such as Google Samples, that illustrate the usage of APIs. We posit that by analyzing the changes of API usage in code examples, we can learn API migration patterns to assist developers with API Migrations. In this paper, we propose an approach that learns API migration patterns from code examples, applies these patterns to the source code of Android apps for API migration, and presents the results to users as potential migration solutions. To evaluate our approach, we migrate API calls in open source Android apps by learning API migration patterns from code examples. We find that our approach can successfully learn API migration patterns and provide API migration assistance in 71 out of 80 cases. Our approach can either migrate API calls with little to no extra modifications needed or provide guidance to assist with the migrations. Through a user study, we find that adopting our approach can reduce the time spent on migrating APIs, on average, by 29 percent. Moreover, our interviews with app developers highlight the benefits of our approach when seeking API migrations. Our approach demonstrates the value of leveraging the knowledge contained in software repositories to facilitate API migrations.",
        "keywords": [
            "Task analysis",
            "Documentation",
            "Google",
            "Software maintenance",
            "Interviews",
            "Indexes"
        ]
    },
    {
        "title": "Effects of Mindfulness on Conceptual Modeling Performance: A Series of Experiments.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2991699",
        "volume": "48",
        "abstract": "<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Context</i>\n. Mindfulness is a meditation technique whose main goal is keeping the mind calm and educating attention by focusing only on one thing at a time, usually breathing. The reported benefits of its continued practice can be of interest for Software Engineering students and practitioners, especially in tasks like conceptual modeling, in which concentration and clearness of mind are crucial. \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Goal</i>\n. In order to evaluate whether Software Engineering students enhance their conceptual modeling performance after several weeks of mindfulness practice, a series of three controlled experiments were carried out at the University of Seville during three consecutive academic years (2013–2016) involving 130 students. \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Method</i>\n. In all the experiments, the subjects were divided into two groups. While the experimental group practiced mindfulness, the control group was trained in public speaking as a placebo treatment. All the subjects developed two conceptual models based on a transcript of an interview, one before and another one after the treatment. The results were compared in terms of conceptual modeling quality (measured as effectiveness, i.e., the percentage of model elements correctly identified) and productivity (measured as efficiency, i.e., the number of model elements correctly identified per unit of time). \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Results</i>\n. The statistically significant results of the series of experiments revealed that the subjects who practiced mindfulness developed slightly better conceptual models (their quality was 8.16 percent higher) and they did it faster (they were 46.67 percent more productive) than the control group, even if they did not have a previous interest in meditation. \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Conclusions</i>\n. The practice of mindfulness improves the performance of Software Engineering students in conceptual modeling, especially their productivity. Nevertheless, more experimentation is needed in order to confirm the outcomes in other Software Engineering tasks and populations.",
        "keywords": [
            "Software engineering",
            "Education",
            "Task analysis",
            "Productivity",
            "Stress",
            "Focusing",
            "Public speaking"
        ]
    },
    {
        "title": "A Multi-Armed Bandit Approach for Test Case Prioritization in Continuous Integration Environments.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2992428",
        "volume": "48",
        "abstract": "Continuous Integration (CI) environments have been increasingly adopted in the industry to allow frequent integration of software changes, making software evolution faster and cost-effective. In such environments, Test Case Prioritization (TCP) techniques play an important role to reduce regression testing costs, establishing a test case execution order that usually maximizes early fault detection. Existing works on TCP in CI environments (TCPCI) present some limitations. Few pieces of work consider CI particularities, such as the test case volatility, that is, they do not consider the dynamic environment of the software life-cycle in which new test cases can be added or removed (discontinued), characteristic related to the Exploration versus Exploitation (EvE) dilemma. To solve such a dilemma an approach needs to balance: i) the diversity of test suite; and ii) the quantity of new test cases and test cases that are error-prone or that comprise high fault-detection capabilities. To deal with this, most approaches use, besides the failure-history, other measures that rely on code instrumentation or require additional information, such as testing coverage. However, to maintain the information updated can be difficult and time-consuming, not scalable due to the test budget of CI environments. In this context, and to properly deal with the TCPCI problem, this work presents an approach based on Multi-Armed Bandit (MAB) called \n<monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">COLEMAN</monospace>\n (\n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><b>C</b>ombinatorial V<b>O</b>lati<b>LE</b> <b>M</b>ulti-Armed B<b>AN</b>dit</i>\n). The TCPCI problem falls into the category of volatile and combinatorial MAB, because multiple arms (test cases) need to be selected, and they are added or removed over the cycles. We conducted an evaluation considering three time budgets and eleven systems. The results show the applicability of our approach and that \n<monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">COLEMAN</monospace>\n outperforms the most similar approach from literature in terms of early fault detection and performance.",
        "keywords": [
            "Testing",
            "Fault detection",
            "Software",
            "Instruments",
            "Google",
            "Industries",
            "Companies"
        ]
    },
    {
        "title": "A Theory of Value for Value-Based Feature Selection in Software Engineering.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2989666",
        "volume": "48",
        "abstract": "Value-Based Software Engineering stresses the role of value in software related decisions. In the context of feature selection, software features judged to provide higher value take priority in the development process. This paper focuses on what value means when selecting software features. Using grounded theory, we conducted and analyzed semi-structured interviews with 21 key stakeholders (decision-makers) from three software/software-intensive companies, within a context where value-based decision-making was already established. Our analysis led to the building of a theory of value for value-based feature selection that identifies the nature of value propositions considered by key stakeholders when selecting software features (i.e., decision-making criteria for deciding upon software features, as suggested by Boehm (2003)). We found that some value propositions were common to all three company cases (core value propositions), whereas others were dependent upon the context in which a company operates, and the characteristics of the product under development (specific value propositions). Moreover, value propositions vary according to the stakeholder group and the type of feature being assessed. Our study provides significant insight into value in the context of feature selection, and generates new concepts around value-based feature selection such as new value propositions.",
        "keywords": [
            "Software",
            "Feature extraction",
            "Stakeholders",
            "Companies",
            "Decision making",
            "Software engineering",
            "Planning"
        ]
    },
    {
        "title": "A Methodology for Analyzing Uptake of Software Technologies Among Developers.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2993758",
        "volume": "48",
        "abstract": "Motivation: The question of what combination of attributes drives the adoption of a particular software technology is critical to developers. It determines both those technologies that receive wide support from the community and those which may be abandoned, thus rendering developers’ investments worthless. Aim and Context: We model software technology adoption by developers and provide insights on specific technology attributes that are associated with better visibility among alternative technologies. Thus, our findings have practical value for developers seeking to increase the adoption rate of their products. Approach: We leverage social contagion theory and statistical modeling to identify, define, and test empirically measures that are likely to affect software adoption. More specifically, we leverage a large collection of open source version control repositories (containing over 4 billion unique versions) to construct a software dependency chain for a specific set of R language source-code files. We formulate logistic regression models, where developers’ software library choices are modeled, to investigate the combination of technological attributes that drive adoption among competing data frame (a core concept for a data science languages) implementations in the R language: \n<monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">tidy</monospace>\n and \n<monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">data.table</monospace>\n. To describe each technology, we quantify key project attributes that might affect adoption (e.g., response times to raised issues, overall deployments, number of open defects, knowledge base) and also characteristics of developers making the selection (performance needs, scale, and their social network). Results: We find that a quick response to raised issues, a larger number of overall deployments, and a larger number of high-score StackExchange questions are associated with higher adoption. Decision makers tend to adopt the technology that is closer to them in the technical dependency network and in author collaborations networks while meeting their performance needs. To gauge the generalizability of the proposed methodology, we investigate the spread of two popular web JavaScript frameworks \n<monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Angular</monospace>\n and \n<monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">React</monospace>\n, and discuss the results. Future work: We hope that our methodology encompassing social contagion that captures both rational and irrational preferences and the elucidation of key measures from large collections of version control data provides a general path toward increasing visibility, driving better informed decisions, and producing more sustainable and widely adopted software.",
        "keywords": [
            "Software",
            "Supply chains",
            "Technological innovation",
            "Software measurement",
            "Data models",
            "Time factors",
            "Libraries"
        ]
    },
    {
        "title": "A Study of Bug Management Using the Stack Exchange Question and Answering Platform.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2994006",
        "volume": "48",
        "abstract": "Traditional bug management systems, like Bugzilla, are widely used in open source and commercial projects. Stack Exchange uses its online question and answer (Q&A) platform to collect and manage bugs, which brings several new unique features that are not offered in traditional bug management systems. Users can edit bug reports, use different communication channels, and vote on bug reports, answers, and their associated comments. Understanding how these features manage bug reports can provide insights to the designers of traditional bug management systems, like whether a feature should be introduced? and how would users leverage such a feature? We performed a large-scale analysis of 19,151 bug reports of the bug management system of Stack Exchange and studied the in-place editing, the answering and commenting, and the voting features. We find that: 1) The three features are used actively. 2) 57 percent of the edits improved the quality of bug reports. 3) Commenting provides a channel for discussing bug-related information, while answering offers a channel for explaining the causes of a bug and bug-fix information. 4) Downvotes are made due to the disagreement of the reported “bug” being a real bug and the low quality of bug reports. Based on our findings, we provide suggestions for traditional bug management systems.",
        "keywords": [
            "Computer bugs",
            "Tagging",
            "History",
            "Communication channels",
            "Software",
            "Message systems",
            "Indexes"
        ]
    },
    {
        "title": "Automated Expansion of Abbreviations Based on Semantic Relation and Transfer Expansion.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2995736",
        "volume": "48",
        "abstract": "Although the negative impact of abbreviations in source code is well-recognized, abbreviations are common for various reasons. To this end, a number of approaches have been proposed to expand abbreviations in identifiers. However, such approaches are either inaccurate or confined to specific identifiers. To this end, in this paper, we propose a generic and accurate approach to expand identifier abbreviations by leveraging both semantic relation and transfer expansion. One of the key insights of the approach is that abbreviations in the name of software entity \n<inline-formula><tex-math notation=\"LaTeX\">$e$</tex-math></inline-formula>\n have a great chance to find their full terms in names of software entities that are semantically related to \n<inline-formula><tex-math notation=\"LaTeX\">$e$</tex-math></inline-formula>\n. Consequently, the proposed approach builds a knowledge graph to represent such entities and their relationships with \n<inline-formula><tex-math notation=\"LaTeX\">$e$</tex-math></inline-formula>\n and searches the graph for full terms. Another key insight is that literally identical abbreviations within the same application are likely (but not necessary) to have identical expansions, and thus the semantics-based expansion in one place may be transferred to other places. To investigate when abbreviation expansion could be transferred safely, we conduct a case study on three open-source applications. The results suggest that a significant part (75 percent) of expansions could be transferred among lexically identical abbreviations within the same application. However, the risk of transfer varies according to various factors, e.g., length of abbreviations, the physical distance between abbreviations, and semantic relations between abbreviations. Based on these findings, we design nine heuristics for transfer expansion and propose a learning-based approach to prioritize both transfer heuristics and semantic-based expansion heuristics. Evaluation results on nine open-source applications suggest that the proposed approach significantly improves the state of the art, improving recall from 29 to 89 percent and precision from 39 to 92 percent.",
        "keywords": [
            "Semantics",
            "Dictionaries",
            "Manuals",
            "Internet",
            "Open source software",
            "Encyclopedias"
        ]
    },
    {
        "title": "Quantitative Verification for Monitoring Event-Streaming Systems.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2996033",
        "volume": "48",
        "abstract": "High-performance data streaming technologies are increasingly adopted in IT companies to support the integration of heterogeneous and possibly distributed applications. Compared with the traditional message queuing middleware, a streaming platform enables the implementation of event-streaming systems (ESS) which include not only complex queues but also pipelines that transform and react to the streams of data. By analysing the centralised data streams, one can evaluate the Quality-of-Service for other systems and components that produce or consume those streams. We consider the exploitation of <i>probabilistic model checking</i> as a performance monitoring technique for ESS systems. Probabilistic model checking is a mature, powerful verification technique with successful application in performance analysis. However, an ESS system may contain quantitative parameters that are determined by event streams observed in a certain period of time. In this paper, we present a novel theoretical framework called QV4M (meaning &#x201C;quantitative verification for monitoring&#x201D;) for monitoring ESS systems, which is based on two recent methods of probabilistic model checking. QV4M assumes the parameters in a probabilistic system model as random variables and infers the statistical significance for the probabilistic model checking output. We also present an empirical evaluation of computational time and data cost for QV4M.",
        "keywords": [
            "Task analysis",
            "Probabilistic logic",
            "Model checking",
            "Measurement",
            "Monitoring",
            "Pipelines",
            "Computational modeling"
        ]
    },
    {
        "title": "PackerGrind: An Adaptive Unpacking System for Android Apps.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2996433",
        "volume": "48",
        "abstract": "App developers are increasingly using packing services (or packers) to protect their code against being reverse engineered or modified. However, such packing techniques are also leveraged by the malicious developers to prevent the malware from being analyzed and detected by the static malware analysis and detection systems. Though there are already studies on unpacking packed Android apps, they usually leverage the manual reverse engineered packing behaviors to unpack apps packed by the specific packers and cannot be appified to the evolved and new packers. In this paper, we propose a novel unpacking approach with the capacity of adaptively unpacking the evolved and newly encountered packers. Also, we develop a new system, named <monospace>PackerGrind</monospace>, based on this adaptive approach for unpacking Android packers. The evaluation with real packed apps demonstrates that <monospace>PackerGrind</monospace> can successfully reveal packers protection mechanisms, effectively handle their evolution and recover Dex files with low overhead.",
        "keywords": [
            "Androids",
            "Humanoid robots",
            "Runtime",
            "Subspace constraints",
            "Open area test sites",
            "Tools",
            "Monitoring"
        ]
    },
    {
        "title": "Inferring Bug Signatures to Detect Real Bugs.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2996975",
        "volume": "48",
        "abstract": "Static tools like Findbugs allow their users to manually define bug patterns, so they can detect more types of bugs, but due to the complexity and variety of programs, it is difficult to manually enumerate all bug patterns, especially for those related to API usages or project-specific rules. Therefore, existing bug-detection tools (e.g., Findbugs) based on manual bug patterns are insufficient in detecting many bugs. Meanwhile, with the rapid development of software, many past bug fixes accumulate in software version histories. These bug fixes contain valuable samples of illegal coding practices. The gap between existing bug samples and well-defined bug patterns motivates our research. In the literature, researchers have explored techniques on learning bug signatures from existing bugs, and a bug signature is defined as a set of program elements explaining the cause/effect of the bug. However, due to various limitations, existing approaches cannot analyze past bug fixes in large scale, and to the best of our knowledge, no previously unknown bugs were ever reported by their work. The major challenge to automatically analyze past bug fixes is that, bug-inducing inputs are typically not recorded, and many bug fixes are partial programs that have compilation errors. As a result, for most bugs in the version history, it is infeasible to reproduce them for dynamic analysis or to feed buggy/fixed code directly into static analysis tools which mostly depend on compilable complete programs. In this paper, we propose an approach, called <small>DePa</small>, that extracts bug signatures based on accurate partial-code analysis of bug fixes. With its support, we conduct the first large scale evaluation on 6,048 past bug fixes collected from four popular Apache projects. In particular, we use <small>DePa</small> to infer bug signatures from these fixes, and to check the latest versions of the four projects with the inferred bug signatures. Our results show that <small>DePa</small> detected 27 unique previously unknown bugs in total, including at least one bug from each project. These bugs are not detected by their developers nor other researchers. Among them, three of our reported bugs are already confirmed and repaired by their developers. Furthermore, our results show that the state-of-the-art tools detected only two of our found bugs, and our filtering techniques improve our precision from 25.5 to 51.5 percent.",
        "keywords": [
            "Computer bugs",
            "Tools",
            "Benchmark testing",
            "Software",
            "Manuals",
            "History",
            "Sun"
        ]
    },
    {
        "title": "Automatic Generation of Acceptance Test Cases From Use Case Specifications: An NLP-Based Approach.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2998503",
        "volume": "48",
        "abstract": "Acceptance testing is a validation activity performed to ensure the conformance of software systems with respect to their functional requirements. In safety critical systems, it plays a crucial role since it is enforced by software standards, which mandate that each requirement be validated by such testing in a clearly traceable manner. Test engineers need to identify all the representative test execution scenarios from requirements, determine the runtime conditions that trigger these scenarios, and finally provide the input data that satisfy these conditions. Given that requirements specifications are typically large and often provided in natural language (e.g., use case specifications), the generation of acceptance test cases tends to be expensive and error-prone. In this paper, we present Use Case Modeling for System-level, Acceptance Tests Generation (UMTG), an approach that supports the generation of executable, system-level, acceptance test cases from requirements specifications in natural language, with the goal of reducing the manual effort required to generate test cases and ensuring requirements coverage. More specifically, UMTG automates the generation of acceptance test cases based on use case specifications and a domain model for the system under test, which are commonly produced in many development environments. Unlike existing approaches, it does not impose strong restrictions on the expressiveness of use case specifications. We rely on recent advances in natural language processing to automatically identify test scenarios and to generate formal constraints that capture conditions triggering the execution of the scenarios, thus enabling the generation of test data. In two industrial case studies, UMTG automatically and correctly translated 95 percent of the use case specification steps into formal constraints required for test data generation; furthermore, it generated test cases that exercise not only all the test scenarios manually implemented by experts, but also some critical scenarios not previously considered.",
        "keywords": [
            "Unified modeling language",
            "Natural language processing",
            "Embedded systems",
            "Test pattern generators",
            "Semantics"
        ]
    },
    {
        "title": "TkT: Automatic Inference of Timed and Extended Pushdown Automata.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2998527",
        "volume": "48",
        "abstract": "To mitigate the cost of manually producing and maintaining models capturing software specifications, \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">specification mining</i>\n techniques can be exploited to automatically derive up-to-date models that faithfully represent the behavior of software systems. So far, specification mining solutions focused on extracting information about the functional behavior of the system, especially in the form of models that represent the ordering of the operations. Well-known examples are finite state models capturing the usage protocol of software interfaces and temporal rules specifying relations among system events. Although the functional behavior of a software system is a primary aspect of concern, there are several other non-functional characteristics that must be typically addressed jointly with the functional behavior of a software system. Efficiency is one of the most relevant characteristics. Indeed, an application that delivers the right functionalities with an inefficient implementation may fail to satisfy the expectations of its users. Interestingly, the \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">timing behavior</i>\n is strongly dependent on the functional behavior of a software system. For instance, the timing of an operation depends on the functional complexity and size of the computation that is performed. Consequently, models that combine the functional and timing behaviors, as well as their dependencies, are extremely important to precisely reason on the behavior of software systems. In this paper, we address the challenge of generating models that capture both the functional and timing behavior of a software system from execution traces. The result is the \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Timed k-Tail (TkT) specification mining technique</i>\n, which can mine finite state models that capture such an interplay: the functional behavior is represented by the possible order of the events accepted by the transitions, while the timing behavior is represented through clocks and clock constraints of different nature associated with transitions. Our empirical evaluation with several libraries and applications shows that \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">TkT</i>\n can generate accurate models, capable of supporting the identification of timing anomalies due to overloaded environment and performance faults. Furthermore, our study shows that \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">TkT</i>\n outperforms state-of-the-art techniques in terms of scalability and accuracy of the mined models.",
        "keywords": [
            "Automata",
            "Clocks",
            "Software systems",
            "Timing",
            "Data mining",
            "Computational modeling"
        ]
    },
    {
        "title": "Quality of Automated Program Repair on Real-World Defects.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2998785",
        "volume": "48",
        "abstract": "Automated program repair is a promising approach to reducing the costs of manual debugging and increasing software quality. However, recent studies have shown that automated program repair techniques can be prone to producing patches of low quality, overfitting to the set of tests provided to the repair technique, and failing to generalize to the intended specification. This paper rigorously explores this phenomenon on real-world Java programs, analyzing the effectiveness of four well-known repair techniques, GenProg, Par, SimFix, and TrpAutoRepair, on defects made by the projects’ developers during their regular development process. We find that: (1) When applied to real-world Java code, automated program repair techniques produce patches for between 10.6 and 19.0 percent of the defects, which is less frequent than when applied to C code. (2) The produced patches often overfit to the provided test suite, with only between 13.8 and 46.1 percent of the patches passing an independent set of tests. (3) Test suite size has an extremely small but significant effect on the quality of the patches, with larger test suites producing higher-quality patches, though, surprisingly, higher-coverage test suites correlate with lower-quality patches. (4) The number of tests that a buggy program fails has a small but statistically significant positive effect on the quality of the produced patches. (5) Test suite provenance, whether the test suite is written by a human or automatically generated, has a significant effect on the quality of the patches, with developer-written tests typically producing higher-quality patches. And (6) the patches exhibit insufficient diversity to improve quality through some method of combining multiple patches. We develop JaRFly, an open-source framework for implementing techniques for automatic search-based improvement of Java programs. Our study uses JaRFly to faithfully reimplement GenProg and TrpAutoRepair to work on Java code, and makes the first public release of an implementation of Par. Unlike prior work, our study carefully controls for confounding factors and produces a methodology, as well as a dataset of automatically-generated test suites, for objectively evaluating the quality of Java repair techniques on real-world defects.",
        "keywords": [
            "Maintenance engineering",
            "Java",
            "Contracts",
            "Manuals",
            "Diversity reception",
            "Inspection",
            "Software quality"
        ]
    },
    {
        "title": "CloudRaid: Detecting Distributed Concurrency Bugs via Log Mining and Enhancement.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2999364",
        "volume": "48",
        "abstract": "Cloud systems suffer from distributed concurrency bugs, which often lead to data loss and service outage. This paper presents \n<sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">CloudRaid</small>\n, a new automatical tool for finding distributed concurrency bugs efficiently and effectively. Distributed concurrency bugs are notoriously difficult to find as they are triggered by untimely interaction among nodes, i.e., unexpected message orderings. To detect concurrency bugs in cloud systems efficiently and effectively, \n<sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">CloudRaid</small>\n analyzes and tests automatically only the message orderings that are likely to expose errors. Specifically, \n<sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">CloudRaid</small>\n mines the logs from previous executions to uncover the message orderings that are feasible but inadequately tested. In addition, we also propose a log enhancing technique to introduce new logs automatically in the system being tested. These extra logs added improve further the effectiveness of \n<sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">CloudRaid</small>\n without introducing any noticeable performance overhead. Our log-based approach makes it well-suited for live systems. We have applied \n<sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">CloudRaid</small>\n to analyze six representative distributed systems: Hadoop2/Yarn, HBase, HDFS, Cassandra, Zookeeper, and Flink. \n<sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">CloudRaid</small>\n has succeeded in testing 60 different versions of these six systems (10 versions per system) in 35 hours, uncovering 31 concurrency bugs, including nine new bugs that have never been reported before. For these nine new bugs detected, which have all been confirmed by their original developers, three are critical and have already been fixed.",
        "keywords": [
            "Computer bugs",
            "Concurrent computing",
            "Cloud computing",
            "Task analysis",
            "Runtime",
            "Message systems",
            "Tools"
        ]
    },
    {
        "title": "Explaining Static Analysis With Rule Graphs.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2999534",
        "volume": "48",
        "abstract": "As static data-flow analysis becomes able to report increasingly complex bugs, using an evergrowing set of complex internal rules encoded into flow functions, the analysis tools themselves grow more and more complex. In result, for users to be able to effectively use those tools on specific codebases, they require special configurations—a task which in industry is typically performed by individual developers or dedicated teams. To efficiently use and configure static analysis tools, developers need to build a certain understanding of the analysis’ rules, i.e., how the underlying analyses interpret the analyzed code and their reasoning for reporting certain warnings. In this article, we explore how to assist developers in understanding the analysis’ warnings, and finding weaknesses in the analysis’ rules. To this end, we introduce the concept of \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">rule graphs</i>\n that expose to the developer selected information about the internal rules of data-flow analyses. We have implemented rule graphs on top of a taint analysis, and show how the graphs can support the abovementioned tasks. Our user study and empirical evaluation show that using rule graphs helps developers understand analysis warnings more accurately than using simple warning traces, and that rule graphs can help developers identify causes for false positives in analysis rules.",
        "keywords": [
            "Static analysis",
            "Tools",
            "SQL injection",
            "Task analysis",
            "Computer bugs",
            "Cognition",
            "Usability"
        ]
    },
    {
        "title": "Automatic Test Case and Test Oracle Generation Based on Functional Scenarios in Formal Specifications for Conformance Testing.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2999884",
        "volume": "48",
        "abstract": "Testing a program to confirm whether it consistently implements its requirements specification is a necessary but time-consuming activity in software development. Automatic testing based on specifications can significantly alleviate the workload and cost, but faces a challenge of how to ensure that both the user’s concerns in the specification and possible execution paths in the program are all covered. In this paper, we describe a new method, called “Vibration-Method” or simply “V-Method”, for automatic generation of test cases and test oracle from model-based formal specifications, aiming to address this challenge. The proposed method is suitable for testing information systems in which rich data types are used. Supporting the principle of “divide and conquer”, the method provides a specific technique for generating test cases based on functional scenarios defined in the specification, test case generation criteria, automatic test case generation algorithms, and a well-defined mechanism for deriving test oracle. We elaborate on the method by discussing how initial test cases can be automatically generated, how additional necessary test cases are produced using the “vibration” technique, and how a test oracle can be automatically derived for a group of test cases. We also describe a controlled experiment to evaluate the effectiveness of the method and discuss the important issues in relation to the performance and applicability of the method.",
        "keywords": [
            "Software",
            "Vibrations",
            "Input variables",
            "Conformance testing",
            "Automatic testing",
            "Information systems"
        ]
    },
    {
        "title": "Utilizing Automatic Query Reformulations as Genetic Operations to Improve Feature Location in Software Models.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3000520",
        "volume": "48",
        "abstract": "In the combination of Model-Driven Engineering (MDE) and Search-Based Software Engineering (SBSE), genetic operations are one of the key ingredients. Our work proposes a novel adaptation of automatic query reformulations as genetic operations that leverage the latent semantics of software models (the cornerstone artefact of MDE). We analyze the impact of these reformulation operations in a real-world industrial case study of feature location in models. As baselines, we use: 1) the widespread single-point crossover plus random mutation; and 2) mask crossover plus random mutation, which is the best performer for feature location in models. We also perform a statistical analysis to provide quantitative evidence of the impact of the results and to show that this impact is significant. Our reformulation operations improve the results of the best baseline by 37.73 percent in recall and 14.08 percent in precision. These results are relevant for the task of feature location in models (one of the main activities performed during software maintenance and evolution). Furthermore, given that the only requirement to apply our approach is term availability in models, our work opens a new research direction to improve more tasks in MDE such as bug location or requirements traceability.",
        "keywords": [
            "Genetics",
            "Adaptation models",
            "Task analysis",
            "Software",
            "Semantics",
            "Evolutionary computation",
            "Analytical models"
        ]
    },
    {
        "title": "Machine Learning Testing: Survey, Landscapes and Horizons.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2019.2962027",
        "volume": "48",
        "abstract": "This paper provides a comprehensive survey of techniques for testing machine learning systems; Machine Learning Testing (ML testing) research. It covers 144 papers on testing properties (e.g., correctness, robustness, and fairness), testing components (e.g., the data, learning program, and framework), testing workflow (e.g., test generation and test evaluation), and application scenarios (e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research focus, concluding with research challenges and promising research directions in ML testing.",
        "keywords": [
            "Machine learning",
            "Software testing",
            "Software engineering",
            "Training data",
            "Data models",
            "Robustness"
        ]
    },
    {
        "title": "How Developers Choose Names.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2976920",
        "volume": "48",
        "abstract": "The names of variables and functions serve as implicit documentation and are instrumental for program comprehension. But choosing good meaningful names is hard. We perform a sequence of experiments in which a total of 334 subjects are required to choose names in given programming scenarios. The first experiment shows that the probability that two developers would select the same name is low: in the 47 instances in our experiments the median probability was only 6.9 percent. At the same time, given that a specific name is chosen, it is usually understood by the majority of developers. Analysis of the names given in the experiment suggests a model where naming is a (not necessarily cognizant or serial) three-step process: (1) selecting the concepts to include in the name, (2) choosing the words to represent each concept, and (3) constructing a name using these words. A followup experiment, using the same experimental setup, then checked whether using this model explicitly can improve the quality of names. The results were that names selected by subjects using the model were judged by two independent judges to be superior to names chosen in the original experiment by a ratio of two-to-one. Using the model appears to encourage the use of more concepts and longer names.",
        "keywords": [
            "Programming profession",
            "Documentation",
            "Natural languages",
            "Unified modeling language"
        ]
    },
    {
        "title": "Integrating Provenance Capture and UML With UML2PROV: Principles and Experience.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2977016",
        "volume": "48",
        "abstract": "In response to the increasing calls for algorithmic accountability, UML2PROV is a novel approach to address the existing gap between application design, where models are described by UML diagrams, and provenance design, where generated provenance is meant to describe an application's flows of data, processes and responsibility, enabling greater accountability of this application. The originality of UML2PROV is that designers are allowed to follow their preferred software engineering methodology to create the UML Diagrams for their application, while UML2PROV takes the UML diagrams as a starting point to automatically generate: (1) the design of the provenance to be generated (expressed as \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">PROV templates</i>\n); and (2) the software library for collecting runtime values of interest (encoded as variable-value associations known as \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">bindings</i>\n), which can be deployed in the application without developer intervention. At runtime, the PROV templates combined with the bindings are used to generate high-quality provenance suitable for subsequent consumption. UML2PROV is rigorously defined by an extensive set of 17 patterns mapping UML diagrams to provenance templates, and is accompanied by a reference implementation based on Model Driven Development techniques. A systematic evaluation of UML2PROV uses quantitative data and qualitative arguments to show the benefits and trade-offs of applying UML2PROV for software engineers seeking to make applications provenance-aware. In particular, as the UML design drives both the design and capture of provenance, we discuss how the levels of detail in UML designs affect aspects such as provenance design generation, application instrumentation, provenance capability maintenance, storage and run-time overhead, and quality of the generated provenance. Some key lessons are learned such as: starting from a non-tailored UML design leads to the capture of more provenance than required to satisfy provenance requirements and therefore, increases the overhead unnecessarily; alternatively, if the UML design is tailored to focus on addressing provenance requirements, only relevant provenance gets to be collected, resulting in lower overheads.",
        "keywords": [
            "Unified modeling language",
            "Software",
            "Proposals",
            "Runtime",
            "Data models",
            "Tools"
        ]
    },
    {
        "title": "Code Reviews With Divergent Review Scores: An Empirical Study of the OpenStack and Qt Communities.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2977907",
        "volume": "48",
        "abstract": "Code review is a broadly adopted software quality practice where developers critique each others’ patches. In addition to providing constructive feedback, reviewers may provide a score to indicate whether the patch should be integrated. Since reviewer opinions may differ, patches can receive both positive and negative scores. If reviews with divergent scores are not carefully resolved, they may contribute to a tense reviewing culture and may slow down integration. In this article, we study patches with divergent review scores in the \n<sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">OpenStack</small>\n and \n<sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Qt</small>\n communities. Quantitative analysis indicates that patches with divergent review scores: (1) account for 15–37 percent of patches that receive multiple review scores; (2) are integrated more often than they are abandoned; and (3) receive negative scores after positive ones in 70 percent of cases. Furthermore, a qualitative analysis indicates that patches with strongly divergent scores that: (4) are abandoned more often suffer from external issues (e.g., integration planning, content duplication) than patches with weakly divergent scores and patches without divergent scores; and (5) are integrated often address reviewer concerns indirectly (i.e., without changing patches). Our results suggest that review tooling should integrate with release schedules and detect concurrent development of similar patches to optimize review discussions with divergent scores. Moreover, patch authors should note that even the most divisive patches are often integrated through discussion, integration timing, and careful revision.",
        "keywords": [
            "Testing",
            "Timing",
            "Software quality",
            "Statistical analysis",
            "Planning",
            "Organizations"
        ]
    },
    {
        "title": "Just-In-Time Defect Identification and Localization: A Two-Phase Framework.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2978819",
        "volume": "48",
        "abstract": "Defect localization aims to locate buggy program elements (e.g., buggy files, methods or lines of code) based on defect symptoms, e.g., bug reports or program spectrum. However, when we receive the defect symptoms, the defect has been exposed and negative impacts have been introduced. Thus, one challenging task is: whether we can locate buggy program prior to the appearance of the defect symptom (e.g., when buggy program elements are being committed to a version control system). We refer to this type of defect localization as \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">“Just-In-Time (JIT) Defect localization”</i>\n. Although many prior studies have proposed various JIT defect identification methods to identify whether a new change is buggy, these prior methods do not locate the suspicious positions. Thus, JIT defect localization is the next step of JIT defect identification (i.e., after a buggy change is identified, suspicious source code lines are located). To address this problem, we propose a two-phase framework, i.e., JIT defect identification and JIT defect localization. Given a new change, JIT defect identification will identify it as buggy change or clean change first. If a new change is identified as buggy, JIT defect localization will rank the source code lines introduced by the new change according to their suspiciousness scores. The source code lines ranked at the top of the list are estimated as the defect location. For JIT defect identification phase, we use 14 change-level features to build a classifier by following existing approach. For JIT defect localization phase, we propose a JIT defect localization approach that leverages software naturalness with the N-gram model. To evaluate the proposed framework, we conduct an empirical study on 14 open source projects with a total of 177,250 changes. The results show that software naturalness is effective for our JIT defect localization. Our model achieves a reasonable performance, and outperforms the two baselines (i.e., random guess and a static bug finder (i.e., PMD)) by a substantial margin in terms of four ranking measures.",
        "keywords": [
            "Software",
            "Computer bugs",
            "Task analysis",
            "History",
            "Fans",
            "Computer science"
        ]
    },
    {
        "title": "Reinforcement-Learning-Guided Source Code Summarization Using Hierarchical Attention.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2979701",
        "volume": "48",
        "abstract": "Code summarization (aka comment generation) provides a high-level natural language description of the function performed by code, which can benefit the software maintenance, code categorization and retrieval. To the best of our knowledge, the state-of-the-art approaches follow an encoder-decoder framework which encodes source code into a hidden space and later decodes it into a natural language space. Such approaches suffer from the following drawbacks: (a) they are mainly input by representing code as a sequence of tokens while ignoring code hierarchy; (b) most of the encoders only input simple features (e.g., tokens) while ignoring the features that can help capture the correlations between comments and code; (c) the decoders are typically trained to predict subsequent words by maximizing the likelihood of subsequent ground truth words, while in real world, they are excepted to generate the entire word sequence from scratch. As a result, such drawbacks lead to inferior and inconsistent comment generation accuracy. To address the above limitations, this paper presents a new code summarization approach using hierarchical attention network by incorporating multiple code features, including type-augmented abstract syntax trees and program control flows. Such features, along with plain code sequences, are injected into a deep reinforcement learning (DRL) framework (e.g., actor-critic network) for comment generation. Our approach assigns weights (pays &#x201C;attention&#x201D;) to tokens and statements when constructing the code representation to reflect the hierarchical code structure under different contexts regarding code features (e.g., control flows and abstract syntax trees). Our reinforcement learning mechanism further strengthens the prediction results through the actor network and the critic network, where the actor network provides the confidence of predicting subsequent words based on the current state, and the critic network computes the reward values of all the possible extensions of the current state to provide global guidance for explorations. Eventually, we employ an advantage reward to train both networks and conduct a set of experiments on a real-world dataset. The experimental results demonstrate that our approach outperforms the baselines by around 22 to 45 percent in BLEU-1 and outperforms the state-of-the-art approaches by around 5 to 60 percent in terms of S-BLEU and C-BLEU.",
        "keywords": [
            "Software",
            "Recurrent neural networks",
            "Training",
            "Machine learning",
            "Decoding",
            "Syntactics",
            "Maintenance engineering"
        ]
    },
    {
        "title": "Incidents are Meant for Learning, Not Repeating: Sharing Knowledge About Security Incidents in Cyber-Physical Systems.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2981310",
        "volume": "48",
        "abstract": "Cyber-physical systems (CPSs) are part of many critical infrastructures such as industrial automation and transportation systems. Thus, security incidents targeting CPSs can have disruptive consequences to assets and people. As incidents tend to re-occur, sharing knowledge about these incidents can help organizations be more prepared to prevent, mitigate or investigate future incidents. This paper proposes a novel approach to enable representation and sharing of knowledge about CPS incidents across different organizations. To support sharing, we represent incident knowledge (\n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">incident patterns</i>\n) capturing incident characteristics that can manifest again, such as incident activities or vulnerabilities exploited by offenders. Incident patterns are a more abstract representation of specific incident instances and, thus, are general enough to be applicable to various systems - different from the one in which the incident originally occurred. They can also avoid disclosing potentially sensitive information about an organization's assets and resources. We provide an automated technique to \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">extract</i>\n an incident pattern from a specific incident instance. To understand how an incident pattern can manifest again in other cyber-physical systems, we also provide an automated technique to \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">instantiate</i>\n incident patterns to specific systems. We demonstrate the feasibility of our approach in the application domain of smart buildings. We evaluate correctness, scalability, and performance using two substantive scenarios inspired by real-world systems and incidents.",
        "keywords": [
            "Security",
            "Smart buildings",
            "Cyber-physical systems",
            "Organizations",
            "HVAC",
            "Servers"
        ]
    },
    {
        "title": "Contextual Documentation Referencing on Stack Overflow.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2981898",
        "volume": "48",
        "abstract": "Software engineering is knowledge-intensive and requires software developers to continually search for knowledge, often on community question answering platforms such as Stack Overflow. Such information sharing platforms do not exist in isolation, and part of the evidence that they exist in a broader software documentation ecosystem is the common presence of hyperlinks to other documentation resources found in forum posts. With the goal of helping to improve the information diffusion between Stack Overflow and other documentation resources, we conducted a study to answer the question of how and why documentation is referenced in Stack Overflow threads. We sampled and classified 759 links from two different domains, regular expressions and Android development, to qualitatively and quantitatively analyze the links’ context and purpose, including attribution, awareness, and recommendations. We found that links on Stack Overflow serve a wide range of distinct purposes, ranging from citation links attributing content copied into Stack Overflow, over links clarifying concepts using Wikipedia pages, to recommendations of software components and resources for background reading. This purpose spectrum has major corollaries, including our observation that links to documentation resources are a reflection of the information needs typical to a technology domain. We contribute a framework and method to analyze the context and purpose of Stack Overflow links, a public dataset of annotated links, and a description of five major observations about linking practices on Stack Overflow. Those observations include the above-mentioned purpose spectrum, its interplay with documentation resources and applications domains, and the fact that links on Stack Overflow often lack context in form of accompanying quotes or summaries. We further point to potential tool support to enhance the information diffusion between Stack Overflow and other documentation resources.",
        "keywords": [
            "Documentation",
            "Context",
            "Internet",
            "Software",
            "Encyclopedias",
            "Electronic publishing"
        ]
    },
    {
        "title": "Diversified Third-Party Library Prediction for Mobile App Development.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2982154",
        "volume": "48",
        "abstract": "The rapid growth of mobile apps has significantly promoted the use of third-party libraries in mobile app development. However, mobile app developers are now facing the challenge of finding useful third-party libraries for improving their apps, e.g., to enhance user interfaces, to add social features, etc. An effective approach is to leverage collaborative filtering (CF) to predict useful third-party libraries for developers. We employed Matrix Factorization (MF) approaches - the classic CF-based prediction approaches - to make the predictions based on a total of 31,432 Android apps from Google Play. However, our investigation shows that there is a significant lack of diversity in the prediction results - a small fraction of popular third-party libraries dominate the prediction results while most other libraries are ill-served. The low diversity in the prediction results limits the usefulness of the prediction because it lacks novelty and serendipity which are much appreciated by mobile app developers. In order to increase the diversity in the prediction results, we designed an innovative MF-based approach, namely LibSeek, specifically for predicting useful third-party libraries for mobile apps. It employs an adaptive weighting mechanism to neutralize the bias caused by the popularity of third-party libraries. In addition, it introduces neighborhood information, i.e., information about similar apps and similar third-party libraries, to personalize the predictions for individual apps. The experimental results show that LibSeek can significantly diversify the prediction results, and in the meantime, increase the prediction accuracy.",
        "keywords": [
            "Libraries",
            "Mobile applications",
            "Predictive models",
            "Google",
            "Gold",
            "User interfaces",
            "Collaboration"
        ]
    },
    {
        "title": "An Empirical Study of Model-Agnostic Techniques for Defect Prediction Models.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2982385",
        "volume": "48",
        "abstract": "Software analytics have empowered software organisations to support a wide range of improved decision-making and policy-making. However, such predictions made by software analytics to date have not been explained and justified. Specifically, current defect prediction models still fail to explain why models make such a prediction and fail to uphold the privacy laws in terms of the requirement to explain any decision made by an algorithm. In this paper, we empirically evaluate three model-agnostic techniques, i.e., two state-of-the-art Local Interpretability Model-agnostic Explanations technique (LIME) and BreakDown techniques, and our improvement of LIME with Hyper Parameter Optimisation (LIME-HPO). Through a case study of 32 highly-curated defect datasets that span across 9 open-source software systems, we conclude that (1) model-agnostic techniques are needed to explain individual predictions of defect models; (2) instance explanations generated by model-agnostic techniques are mostly overlapping (but not exactly the same) with the global explanation of defect models and reliable when they are re-generated; (3) model-agnostic techniques take less than a minute to generate instance explanations; and (4) more than half of the practitioners perceive that the contrastive explanations are necessary and useful to understand the predictions of defect models. Since the implementation of the studied model-agnostic techniques is available in both Python and R, we recommend model-agnostic techniques be used in the future.",
        "keywords": [
            "Predictive models",
            "Software",
            "Analytical models",
            "Software algorithms",
            "Prediction algorithms",
            "Electric breakdown",
            "Software engineering"
        ]
    },
    {
        "title": "Enabling Mutant Generation for Open- and Closed-Source Android Apps.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2982638",
        "volume": "48",
        "abstract": "Mutation testing has been widely used to assess the fault-detection effectiveness of a test suite, as well as to guide test case generation or prioritization. Empirical studies have shown that, while mutants are generally representative of real faults, an effective application of mutation testing requires “traditional” operators designed for programming languages to be augmented with operators specific to an application domain and/or technology. The case for Android apps is not an exception. Therefore, in this paper we describe the process we followed to create (i) a taxonomy of mutation operations and, (ii) two tools, \n<monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">MDroid+</monospace>\n and \n<monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">MutAPK</monospace>\n for mutant generation of Android apps. To this end, we systematically devise a taxonomy of 262 types of Android faults grouped in 14 categories by manually analyzing 2,023 software artifacts from different sources (\n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">e.g.,</i>\n bug reports, commits). Then, we identified a set of 38 mutation operators, and implemented them in two tools, the first enabling mutant generation at the source code level, and the second designed to perform mutations at APK level. The rationale for having a dual-approach is based on the fact that source code is not always available when conducting mutation testing. Thus, mutation testing for APKs enables new scenarios in which researchers/practitioners only have access to APK files. The taxonomy, proposed operators, and tools have been evaluated in terms of the number of non-compilable, trivial, equivalent, and \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">duplicate</i>\n mutants generated and their capacity to represent real faults in Android apps as compared to other well-known mutation tools.",
        "keywords": [
            "Testing",
            "Tools",
            "Taxonomy",
            "Mobile applications",
            "Computer bugs",
            "Java",
            "Software"
        ]
    },
    {
        "title": "Studying Ad Library Integration Strategies of Top Free-to-Download Apps.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2983399",
        "volume": "48",
        "abstract": "In-app advertisements have become a major revenue source for app developers in the mobile app ecosystem. Ad libraries play an integral part in this ecosystem as app developers integrate these libraries into their apps to display ads. In this paper, we study ad library integration strategies by analyzing 35,459 updates of 1,837 top free-to-download apps of the Google Play Store. We observe that ad libraries (e.g., Google AdMob) are not always used for serving ads – 22.5% of the apps that integrate Google AdMob do not display ads. They instead depend on Google AdMob for analytical purposes. Among the apps that display ads, we observe that 57.9% of them integrate multiple ad libraries. We observe that such integration of multiple ad libraries occurs commonly in apps with a large number of downloads and ones in app categories with a high proportion of ad-displaying apps. We manually analyze a sample of apps and derive a set of rules to automatically identify four common strategies for integrating multiple ad libraries. Our analysis of the apps across the identified strategies shows that app developers prefer to manage their own integrations instead of using off-the-shelf features of ad libraries for integrating multiple ad libraries. Our findings are valuable for ad library developers who wish to learn first hand about the challenges of integrating ad libraries.",
        "keywords": [
            "Libraries",
            "Google",
            "Advertising",
            "Mobile applications",
            "Tools",
            "Ecosystems",
            "Companies"
        ]
    },
    {
        "title": "Detecting Developers' Task Switches and Types.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2984086",
        "volume": "48",
        "abstract": "Developers work on a broad variety of tasks during their workdays and constantly switch between them. While these task switches can be beneficial, they can also incur a high cognitive burden on developers, since they have to continuously remember and rebuild the task context–the artifacts and applications relevant to the task. Researchers have therefore proposed to capture task context more explicitly and use it to provide better task support, such as task switch reduction or task resumption support. Yet, these approaches generally require the developer to \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">manually</i>\n identify task switches. Automatic approaches for predicting task switches have so far been limited in their accuracy, scope, evaluation, and the time discrepancy between predicted and actual task switches. In our work, we examine the use of \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">automatically</i>\n collected computer interaction data for detecting developers’ task switches as well as task types. In two field studies–a 4h observational study and a multi-day study with experience sampling–we collected data from a total of 25 professional developers. Our study results show that we are able to use temporal and semantic features from developers’ computer interaction data to detect task switches and types in the field with high accuracy of 84 percent and 61 percent respectively, and within a short time window of less than 1.6 minutes on average from the actual task switch. We discuss our findings and their practical value for a wide range of applications in real work settings.",
        "keywords": [
            "Task analysis",
            "Feature extraction",
            "Semantics",
            "Microsoft Windows",
            "Switches",
            "Machine learning"
        ]
    },
    {
        "title": "How Gender-Biased Tools Shape Newcomer Experiences in OSS Projects.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2984173",
        "volume": "48",
        "abstract": "Previous research has revealed that newcomer women are disproportionately affected by gender-biased barriers in open source software (OSS) projects. However, this research has focused mainly on social/cultural factors, neglecting the software tools and infrastructure. To shed light on how OSS tools and infrastructure might factor into OSS barriers to entry, we conducted two studies: (1) a field study with five teams of software professionals, who worked through five use cases to analyze the tools and infrastructure used in their OSS projects; and (2) a diary study with 22 newcomers (9 women and 13 men) to investigate whether the barriers matched the ones identified by the software professionals. The field study produced a bleak result: software professionals found gender biases in 73 percent of all the newcomer barriers they identified. Further, the diary study confirmed these results: Women newcomers encountered gender biases in 63 percent of barriers they faced. Fortunately, many kinds of barriers and biases revealed in these studies could potentially be ameliorated through changes to the OSS software environments and tools.",
        "keywords": [
            "Tools",
            "Problem-solving",
            "Open source software",
            "Documentation",
            "Cultural differences",
            "Productivity"
        ]
    },
    {
        "title": "Managing Episodic Volunteers in Free/Libre/Open Source Software Communities.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2985093",
        "volume": "48",
        "abstract": "We draw on the concept of episodic volunteering (EV) from the general volunteering literature to identify practices for managing EV in free/libre/open source software (FLOSS) communities. Infrequent but ongoing participation is widespread, but the practices that community managers are using to manage EV, and their concerns about EV, have not been previously documented. We conducted a policy Delphi study involving 24 FLOSS community managers from 22 different communities. Our panel identified 16 concerns related to managing EV in FLOSS, which we ranked by prevalence. We also describe 65 practices for managing EV in FLOSS. Almost three-quarters of these practices are used by at least three community managers. We report these practices using a systematic presentation that includes context, relationships between practices, and concerns that they address. These findings provide a coherent framework that can help FLOSS community managers to better manage episodic contributors.",
        "keywords": [
            "Software",
            "Computer bugs",
            "Organizations",
            "Systematics",
            "Lenses",
            "Sustainable development",
            "Object recognition"
        ]
    },
    {
        "title": "Better Data Labelling With EMBLEM (and how that Impacts Defect Prediction).",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2986415",
        "volume": "48",
        "abstract": "Standard automatic methods for recognizing problematic development commits can be greatly improved via the incremental application of human+artificial expertise. In this approach, called EMBLEM, an AI tool first explore the software development process to label commits that are most problematic. Humans then apply their expertise to check those labels (perhaps resulting in the AI updating the support vectors within their SVM learner). We recommend this human+AI partnership, for several reasons. When a new domain is encountered, EMBLEM can learn better ways to label which comments refer to real problems. Also, in studies with 9 open source software projects, labelling via EMBLEM's incremental application of human+AI is at least an order of magnitude cheaper than existing methods (\n<inline-formula><tex-math notation=\"LaTeX\">$\\approx$</tex-math></inline-formula>\n eight times). Further, EMBLEM is very effective. For the data sets explored here, EMBLEM better labelling methods significantly improved \n<inline-formula><tex-math notation=\"LaTeX\">$P_{opt}20$</tex-math></inline-formula>\n and G-scores performance in nearly all the projects studied here.",
        "keywords": [
            "Labeling",
            "Computer bugs",
            "Data models",
            "Software",
            "Support vector machines",
            "Standards",
            "Task analysis"
        ]
    },
    {
        "title": "Output Sampling for Output Diversity in Automatic Unit Test Generation.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2987377",
        "volume": "48",
        "abstract": "Diverse test sets are able to expose bugs that test sets generated with structural coverage techniques cannot discover. Input-diverse test set generators have been shown to be effective for this, but also have limitations: e.g., they need to be complemented with semantic information derived from the Software Under Test. We demonstrate how to drive the test set generation process with semantic information in the form of output diversity. We present the first totally automatic output sampling for output diversity unit test set generation tool, called OutGen. OutGen transforms a program into an SMT formula in bit-vector arithmetic. It then applies universal hashing in order to generate an output-based diverse set of inputs. The result offers significant diversity improvements when measured as a high output uniqueness count. It achieves this by ensuring that the test set&#x2019;s output probability distribution is uniform, i.e., highly diverse. The use of output sampling, as opposed to any of input sampling, CBMC, CAVM, behaviour diversity or random testing improves mutation score and bug detection by up to 4150 and 963 percent respectively on programs drawn from three different corpora: the R-project, SIR and CodeFlaws. OutGen test sets achieve an average mutation score of up to 92 percent, and 70 percent of the test sets detect the defect. Moreover, OutGen is the only automatic unit test generation tool that is able to detect bugs on the real number C functions from the R-project.",
        "keywords": [
            "Computer bugs",
            "Tools",
            "Semantics",
            "Generators",
            "Software engineering",
            "Test pattern generators"
        ]
    },
    {
        "title": "Restore: Retrospective Fault Localization Enhancing Automated Program Repair.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2987862",
        "volume": "48",
        "abstract": "Fault localization is a crucial step of automated program repair, because accurately identifying program locations that are most closely implicated with a fault greatly affects the effectiveness of the patching process. An ideal fault localization technique would provide precise information while requiring moderate computational resources—to best support an efficient search for correct fixes. In contrast, most automated program repair tools use standard fault localization techniques—which are not tightly integrated with the overall program repair process, and hence deliver only subpar efficiency. In this paper, we present \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">retrospective fault localization</i>\n: a novel fault localization technique geared to the requirements of automated program repair. A key idea of retrospective fault localization is to reuse the outcome of failed patch validation to support mutation-based dynamic analysis—providing accurate fault localization information without incurring onerous computational costs. We implemented retrospective fault localization in a tool called \n<sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Restore</small>\n—based on the \n<sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Jaid</small>\n Java program repair system. Experiments involving faults from the \n<sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Defects4J</small>\n standard benchmark indicate that retrospective fault localization can boost automated program repair: \n<sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Restore</small>\n efficiently explores a large fix space, delivering state-of-the-art effectiveness (41 \n<sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Defects4J</small>\n bugs correctly fixed, 8 of which no other automated repair tool for Java can fix) while simultaneously boosting performance (speedup over 3 compared to \n<sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Jaid</small>\n). Retrospective fault localization is applicable to any automated program repair techniques that rely on fault localization and dynamic validation of patches.",
        "keywords": [
            "Maintenance engineering",
            "Tools",
            "Java",
            "Computer bugs",
            "Software",
            "Standards",
            "Electronic mail"
        ]
    },
    {
        "title": "Defining Smart Contract Defects on Ethereum.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2989002",
        "volume": "48",
        "abstract": "<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Smart contracts</i>\n are programs running on a blockchain. They are immutable to change, and hence can not be patched for bugs once deployed. Thus it is critical to ensure they are bug-free and well-designed before deployment. A \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Contract defect</i>\n is an error, flaw or fault in a smart contract that causes it to produce an incorrect or unexpected result, or to behave in unintended ways. The detection of contract defects is a method to avoid potential bugs and improve the design of existing code. Since smart contracts contain numerous distinctive features, such as the \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">gas system. decentralized</i>\n, it is important to find smart contract specified defects. To fill this gap, we collected smart-contract-related posts from Ethereum StackExchange, as well as real-world smart contracts. We manually analyzed these posts and contracts; using them to define 20 kinds of \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">contract defects</i>\n. We categorized them into indicating potential security, availability, performance, maintainability and reusability problems. To validate if practitioners consider these contract as harmful, we created an online survey and received 138 responses from 32 different countries. Feedback showed these contract defects are harmful and removing them would improve the quality and robustness of smart contracts. We manually identified our defined contract defects in 587 real world smart contract and publicly released our dataset. Finally, we summarized 5 impacts caused by contract defects. These help developers better understand the symptoms of the defects and removal priority.",
        "keywords": [
            "Contracts",
            "Computer bugs",
            "Robustness",
            "Protocols",
            "Bitcoin"
        ]
    },
    {
        "title": "Automatic Detection, Validation, and Repair of Race Conditions in Interrupt-Driven Embedded Software.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.2989171",
        "volume": "48",
        "abstract": "Interrupt-driven programs are widely deployed in safety-critical embedded systems to perform hardware and resource dependent data operation tasks. The frequent use of interrupts in these systems can cause race conditions to occur due to interactions between application tasks and interrupt handlers (or two interrupt handlers). Numerous program analysis and testing techniques have been proposed to detect races in multithreaded programs. Little work, however, has addressed race condition problems related to hardware interrupts. In this paper, we present SDRacer, an automated framework that can detect, validate and repair race conditions in interrupt-driven embedded software. It uses a combination of static analysis and symbolic execution to generate input data for exercising the potential races. It then employs virtual platforms to dynamically validate these races by forcing the interrupts to occur at the potential racing points. Finally, it provides repair candidates to eliminate the detected races. We evaluate SDRacer on nine real-world embedded programs written in C language. The results show that SDRacer can precisely detect and successfully fix race conditions.",
        "keywords": [
            "Task analysis",
            "Maintenance engineering",
            "Hardware",
            "Embedded systems",
            "Concurrent computing",
            "Testing",
            "Embedded software"
        ]
    },
    {
        "title": "Pegasus: Performance Engineering for Software Applications Targeting HPC Systems.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3001257",
        "volume": "48",
        "abstract": "Developing and optimizing software applications for high performance and energy efficiency is a very challenging task, even when considering a single target machine. For instance, optimizing for multicore-based computing systems requires in-depth knowledge about programming languages, application programming interfaces (APIs), compilers, performance tuning tools, and computer architecture and organization. Many of the tasks of performance engineering methodologies require manual efforts and the use of different tools not always part of an integrated toolchain. This paper presents Pegasus, a performance engineering approach supported by a framework that consists of a source-to-source compiler, controlled and guided by strategies programmed in a Domain-Specific Language, and an autotuner. Pegasus is a holistic and versatile approach spanning various decision layers composing the software stack, and exploiting the system capabilities and workloads effectively through the use of runtime autotuning. The Pegasus approach helps developers by automating tasks regarding the efficient implementation of software applications in multicore computing systems. These tasks focus on application analysis, profiling, code transformations, and the integration of runtime autotuning. Pegasus allows developers to program their strategies or to automatically apply existing strategies to software applications in order to ensure the compliance of non-functional requirements, such as performance and energy efficiency. We show how to apply Pegasus and demonstrate its applicability and effectiveness in a complex case study, which includes tasks from a smart navigation system.",
        "keywords": [
            "Task analysis",
            "Software",
            "Tools",
            "Runtime",
            "Tuning",
            "Power demand",
            "Libraries"
        ]
    },
    {
        "title": "Managing Technical Debt in Database Normalization.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3001339",
        "volume": "48",
        "abstract": "Database normalization is one of the main principles for designing relational databases, which is the most popular database model, with the objective of improving data and system qualities, such as performance. Refactoring the database for normalization can be costly, if the benefits of the exercise are not justified. Developers often ignore the normalization process due to the time and expertise it requires, introducing technical debt into the system. Technical debt is a metaphor that describes trade-offs between short-term goals and applying optimal design and development practices. We consider database normalization debts are likely to be incurred for tables below the fourth normal form. To manage the debt, we propose a multi-attribute analysis framework that makes a novel use of the Portfolio Theory and the TOPSIS method (Technique for Order of Preference by Similarity to Ideal Solution) to rank the candidate tables for normalization to the fourth normal form. The ranking is based on the tables estimated impact on data quality, performance, maintainability, and cost. The techniques are evaluated using an industrial case study of a database-backed web application for human resource management. The results show that the debt-aware approach can provide an informed justification for the inclusion of critical tables to be normalized, while reducing the effort and cost of normalization.",
        "keywords": [
            "Data integrity",
            "Data models",
            "Relational databases",
            "Redundancy",
            "Portfolios"
        ]
    },
    {
        "title": "Reuse of Similarly Behaving Software Through Polymorphism-Inspired Variability Mechanisms.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3001512",
        "volume": "48",
        "abstract": "In many cases, software artifacts share similarity across projects and development teams. However, often this similarity is only partially reflected on the level of design and implementation, and therefore the possibilities for its detection are limited in current variability analysis, clone detection, and application search approaches. In this paper, we propose a method for identification and comparison of similarly behaving software. The method, supported by a prototype tool, analyzes the behavioral similarity of object-oriented code artifacts based on shallow (behavior interface) and deep (behavior transformation) descriptions of the exhibited operations. It further recommends on suitable mechanisms inspired by the notion of polymorphism in order to guide and support current and future reuse. The approach was evaluated on two data-sets, obtained following two different scenarios: clone-and-own and independent development by different teams.",
        "keywords": [
            "Cloning",
            "Software",
            "Measurement",
            "Software product lines",
            "Java",
            "Object oriented modeling",
            "Tools"
        ]
    },
    {
        "title": "Revisiting Supervised and Unsupervised Methods for Effort-Aware Cross-Project Defect Prediction.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3001739",
        "volume": "48",
        "abstract": "Cross-project defect prediction (CPDP), aiming to apply defect prediction models built on source projects to a target project, has been an active research topic. A variety of supervised CPDP methods and some simple unsupervised CPDP methods have been proposed. In a recent study, Zhou \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">et al.</i>\n found that simple unsupervised CPDP methods (i.e., ManualDown and ManualUp) have a prediction performance comparable or even superior to complex supervised CPDP methods. Therefore, they suggested that the ManualDown should be treated as the baseline when considering non-effort-aware performance measures (NPMs) and the ManualUp should be treated as the baseline when considering effort-aware performance measures (EPMs) in future CPDP studies. However, in that work, these unsupervised methods are only compared with existing supervised CPDP methods using a small subset of NPMs, and the prediction results of baselines are directly collected from the primary literatures. Besides, the comparison has not considered other recently proposed EPMs, which consider context switches and developer fatigue due to initial false alarms. These limitations may not give a holistic comparison between the supervised methods and unsupervised methods. In this paper, we aim to revisit Zhou \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">et al.</i>\n’s study. To the best of our knowledge, we are the first to make a comparison between the existing supervised CPDP methods and the unsupervised methods proposed by Zhou \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">et al.</i>\n in the same experimental setting when considering both NPMs and EPMs. We also propose an improved supervised CPDP method EASC and make a further comparison with the unsupervised methods. According to the results on 82 projects in terms of 11 performance measures, we find that when considering NPMs, EASC can achieve prediction performance comparable or even superior to unsupervised method ManualDown in most cases. Besides, when considering EPMs, EASC can statistically significantly outperform the unsupervised method ManualUp with a large improvement in terms of Cliff’s delta in most cases. Therefore, the supervised CPDP methods are more promising than the unsupervised method in practical application scenarios, since the limitation of testing resource and the impact on developers cannot be ignored in these scenarios.",
        "keywords": [
            "Manuals",
            "Predictive models",
            "Atmospheric measurements",
            "Particle measurements",
            "Data models",
            "Software",
            "Testing"
        ]
    },
    {
        "title": "Sentinel: A Hyper-Heuristic for the Generation of Mutant Reduction Strategies.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3002496",
        "volume": "48",
        "abstract": "Mutation testing is an effective approach to evaluate and strengthen software test suites, but its adoption is currently limited by the mutants’ execution computational cost. Several strategies have been proposed to reduce this cost (a.k.a. mutation cost reduction strategies), however none of them has proven to be effective for all scenarios since they often need an ad-hoc manual selection and configuration depending on the software under test (SUT). In this paper, we propose a novel multi-objective evolutionary hyper-heuristic approach, dubbed Sentinel, to automate the generation of optimal cost reduction strategies for every new SUT. We evaluate Sentinel by carrying out a thorough empirical study involving 40 releases of 10 open-source real-world software systems and both baseline and state-of-the-art strategies as a benchmark. We execute a total of 4,800 experiments, and evaluate their results with both quality indicators and statistical significance tests, following the most recent best practice in the literature. The results show that strategies generated by Sentinel outperform the baseline strategies in 95 percent of the cases always with large effect sizes. They also obtain statistically significantly better results than state-of-the-art strategies in 88 percent of the cases, with large effect sizes for 95 percent of them. Also, our study reveals that the mutation strategies generated by Sentinel for a given software version can be used without any loss in quality for subsequently developed versions in 95 percent of the cases. These results show that Sentinel is able to automatically generate mutation strategies that reduce mutation testing cost without affecting its testing effectiveness (i.e., mutation score), thus taking off from the tester’s shoulders the burden of manually selecting and configuring strategies for each SUT.",
        "keywords": [
            "Testing",
            "Maintenance engineering",
            "Computational efficiency",
            "Open source software",
            "Software engineering",
            "Search problems"
        ]
    },
    {
        "title": "Gender Differences in Personality Traits of Software Engineers.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3003413",
        "volume": "48",
        "abstract": "There is a growing body of gender studies in software engineering to understand diversity and inclusion issues, as diversity is recognized to be a key issue to healthy teams and communities. A second factor often linked to team performance is personality, which has received far more attention. Very few studies, however, have focused on the intersection of these two fields. Hence, we set out to study gender differences in personality traits of software engineers. Through a survey study we collected personality data, using the HEXACO model, of 483 software engineers. The data were analyzed using a Bayesian independent sample t-test and network analysis. The results suggest that women score significantly higher in Openness to Experience, Honesty-Humility, and Emotionality than men. Further, men show higher psychopathic traits than women. Based on these findings, we develop a number of propositions that can guide future research.",
        "keywords": [
            "Software",
            "Software engineering",
            "Instruments",
            "Bayes methods",
            "Sea measurements",
            "Data models",
            "Face"
        ]
    },
    {
        "title": "Why Do Software Developers Use Static Analysis Tools? A User-Centered Study of Developer Needs and Motivations.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3004525",
        "volume": "48",
        "abstract": "As increasingly complex software is developed every day, a growing number of companies use static analysis tools to reason about program properties ranging from simple coding style rules to more advanced software bugs, to multi-tier security vulnerabilities. While increasingly complex analyses are created, developer support must also be updated to ensure that the tools are used to their best potential. Past research in the usability of static analysis tools has primarily focused on usability issues encountered by software developers, and the causes of those issues in analysis tools. In this article, we adopt a more user-centered approach, and aim at understanding why software developers use analysis tools, which decisions they make when using those tools, what they look for when making those decisions, and the motivation behind their strategies. This approach allows us to derive new tool requirements that closely support software developers (e.g., systems for recommending warnings to fix that take developer knowledge into account), and also open novel avenues for further static-analysis research such as collaborative user interfaces for analysis warnings.",
        "keywords": [
            "Tools",
            "Static analysis",
            "Usability",
            "Industries",
            "Computer bugs",
            "Security"
        ]
    },
    {
        "title": "Graph Based Mining of Code Change Patterns From Version Control Commits.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3004892",
        "volume": "48",
        "abstract": "Detailed knowledge of frequently recurring code changes can be beneficial for a variety of software engineering activities. For example, it is a key step to understand the process of software evolution, but is also necessary when developing more sophisticated code completion features predicting likely changes. Previous attempts on automatically finding such code change patterns were mainly based on frequent itemset mining, which essentially finds sets of edits occurring in close proximity. However, these approaches do not analyze the interplay among code elements, e.g., two code objects being named similarly, and thereby neglect great potential in identifying a number of meaningful patterns. We present a novel method for the automated mining of code change patterns from Git repositories that captures these context relations between individual edits. Our approach relies on a transformation of source code into a graph representation, while keeping relevant relations present. We then apply graph mining techniques to extract frequent subgraphs, which can be used for further analysis of development projects. We suggest multiple usage scenarios for the resulting pattern type. Additionally, we propose a transformation into complex event processing (CEP) rules which allows for easier application, especially for event-based auto-completion recommenders or similar tools. For evaluation, we mined seven open-source code repositories. We present 25 frequent change patterns occurring across these projects. We found these patterns to be meaningful, easy to interpret and mostly persistent across project borders. On average, a pattern from our set appeared in 45 percent of the analyzed code changes.",
        "keywords": [
            "Data mining",
            "Pipelines",
            "Itemsets",
            "Tools",
            "Optimization",
            "Open source software"
        ]
    },
    {
        "title": "How Does Refactoring Impact Security When Improving Quality? A Security-Aware Refactoring Approach.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3005995",
        "volume": "48",
        "abstract": "While state of the art of software refactoring research uses various quality attributes to identify refactoring opportunities and evaluate refactoring recommendations, the impact of refactoring on the security of software systems when improving other quality objectives is under-explored. It is critical to understand how a system is resistant to security risks after refactoring to improve quality metrics. For instance, refactoring is widely used to improve the reusability of code, however such an improvement may increase the attack surface due to the created abstractions. Increasing the spread of security-critical classes in the design to improve modularity may result in reducing the resilience of software systems to attacks. In this paper, we investigated the possible impact of improving different quality attributes (e.g., reusability, extendibility, etc.), from the QMOOD model, effectiveness on a set of 8 security metrics defined in the literature related to the data access. We also studied the impact of different refactorings on these static security metrics. Then, we proposed a multi-objective refactoring recommendation approach to find a balance between quality attributes and security based on the correlation results to guide the search. We evaluated our tool on 30 open source projects. We also collected the practitioner perceptions on the refactorings recommended by our tool in terms of the possible impact on both security and other quality attributes. Our results confirm that developers need to make trade-offs between security and other qualities when refactoring software systems due to the negative correlations between them.",
        "keywords": [
            "Security",
            "Measurement",
            "Tools",
            "Correlation",
            "Software systems",
            "Computer bugs"
        ]
    },
    {
        "title": "Logram: Efficient Log Parsing Using $n$n-Gram Dictionaries.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3007554",
        "volume": "48",
        "abstract": "Software systems usually record important runtime information in their logs. Logs help practitioners understand system runtime behaviors and diagnose field failures. As logs are usually very large in size, automated log analysis is needed to assist practitioners in their software operation and maintenance efforts. Typically, the first step of automated log analysis is log parsing, i.e., converting unstructured raw logs into structured data. However, log parsing is challenging, because logs are produced by static templates in the source code (i.e., logging statements) yet the templates are usually inaccessible when parsing logs. Prior work proposed automated log parsing approaches that have achieved high accuracy. However, as the volume of logs grows rapidly in the era of cloud computing, efficiency becomes a major concern in log parsing. In this work, we propose an automated log parsing approach, \n<i>Logram</i>\n, which leverages \n<inline-formula><tex-math notation=\"LaTeX\">$n$</tex-math></inline-formula>\n-gram dictionaries to achieve efficient log parsing. We evaluated \n<i>Logram</i>\n on 16 public log datasets and compared \n<i>Logram</i>\n with five state-of-the-art log parsing approaches. We found that \n<i>Logram</i>\n achieves a higher parsing accuracy than the best existing approaches (i.e., at least 10 percent higher, on average) and also outperforms these approaches in efficiency (i.e., 1.8 to 5.1 times faster than the second-fastest approaches in terms of end-to-end parsing time). Furthermore, we deployed \n<i>Logram</i>\n on \n<i>Spark</i>\n and we found that \n<i>Logram</i>\n scales out efficiently with the number of \n<i>Spark</i>\n nodes (e.g., with near-linear scalability for some logs) without sacrificing parsing accuracy. In addition, we demonstrated that \n<i>Logram</i>\n can support effective online parsing of logs, achieving similar parsing results and efficiency to the offline mode.",
        "keywords": [
            "Dictionaries",
            "Runtime",
            "Data mining",
            "Cows",
            "Sparks",
            "Software systems",
            "Moon"
        ]
    },
    {
        "title": "ConEx: Efficient Exploration of Big-Data System Configurations for Better Performance.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3007560",
        "volume": "48",
        "abstract": "Configuration space complexity makes the big-data software systems hard to configure well. Consider Hadoop, with over nine hundred parameters, developers often just use the <i>default</i> configurations provided with Hadoop distributions. The opportunity costs in lost performance are significant. Popular learning-based approaches to auto-tune software does not scale well for big-data systems because of the high cost of collecting training data. We present a new method based on a combination of <i>Evolutionary Markov Chain Monte Carlo (EMCMC)</i> sampling and cost reduction techniques to find better-performing configurations for big data systems. For cost reduction, we developed and experimentally tested and validated two approaches: using scaled-up big data jobs as proxies for the objective function for larger jobs and using a dynamic job similarity measure to infer that results obtained for one kind of big data problem will work well for similar problems. Our experimental results suggest that our approach promises to improve the performance of big data systems significantly and that it outperforms competing approaches based on random sampling, basic genetic algorithms (GA), and predictive model learning. Our experimental results support the conclusion that our approach strongly demonstrates the potential to improve the performance of big data systems significantly and frugally.",
        "keywords": [
            "Big Data",
            "Software systems",
            "Machine learning",
            "Markov processes",
            "Monte Carlo methods",
            "Predictive models"
        ]
    },
    {
        "title": "GUI-Guided Test Script Repair for Mobile Apps.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3007664",
        "volume": "48",
        "abstract": "Graphical User Interface (GUI) testing is widely used to test mobile apps. As mobile apps are frequently updated and need repeated testing, to reduce the test cost, their test cases are often coded as scripts to enable automated execution using test harnesses/tools. When those mobile apps evolve, many of the test scripts, however, may become broken due to changes made to the app GUIs. While it is desirable that the broken scripts get repaired, doing it manually can be preventively expensive if the number of tests need repairing is large. We propose in this paper a novel approach named \n<sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Meter</small>\n to repairing broken GUI test scripts automatically when mobile apps evolve. \n<sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Meter</small>\n leverages computer vision techniques to infer GUI changes between two versions of a mobile app and uses the inferred changes to guide the repair of GUI test scripts. Since \n<sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Meter</small>\n only relies on screenshots to repair GUI tests, it is applicable to apps targeting open or closed source mobile platforms. In experiments conducted on 22 Android apps and 6 iOS apps, repairs produced by \n<sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Meter</small>\n helped preserve 63.7 and 38.8 percent of all the test actions broken by the GUI changes, respectively.",
        "keywords": [
            "Meters",
            "Graphical user interfaces",
            "Mobile applications",
            "Maintenance engineering",
            "Testing",
            "Tools",
            "Computer vision"
        ]
    },
    {
        "title": "RefactoringMiner 2.0.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3007722",
        "volume": "48",
        "abstract": "Refactoring detection is crucial for a variety of applications and tasks: (i) empirical studies about code evolution, (ii) tools for library API migration, (iii) code reviews and change comprehension. However, recent research has questioned the accuracy of the state-of-the-art refactoring mining tools, which poses threats to the reliability of the detected refactorings. Moreover, the majority of refactoring mining tools depend on code similarity thresholds. Finding universal threshold values that can work well for all projects, regardless of their architectural style, application domain, and development practices is extremely challenging. Therefore, in a previous work [N. Tsantalis, M. Mansouri, L. M. Eshkevari, D. Mazinanian, and D. Dig, Accurate and efficient refactoring detection in commit history, in 40th International Conference on Software Engineering, 2018, pp. 483–494], we introduced the first refactoring mining tool that does not require any code similarity thresholds to operate. In this work, we extend our tool to support low-level refactorings that take place within the body of methods. To evaluate our tool, we created one of the most accurate, complete, and representative refactoring oracles to date, including 7,226 true instances for 40 different refactoring types detected by one (minimum) up to six (maximum) different tools, and validated by one up to four refactoring experts. Our evaluation showed that our approach achieves the highest average precision (99.6 percent) and recall (94 percent) among all competitive tools, and on median is 2.6 times faster than the second faster competitive tool.",
        "keywords": [
            "Tools",
            "Open source software",
            "Software systems",
            "Task analysis",
            "Libraries",
            "Syntactics",
            "Maintenance engineering"
        ]
    },
    {
        "title": "Execution of Partial State Machine Models.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3008850",
        "volume": "48",
        "abstract": "The iterative and incremental nature of software development using models typically makes a model of a system incomplete (i.e., partial) until a more advanced and complete stage of development is reached. Existing model execution approaches (interpretation of models or code generation) do not support the execution of partial models. Supporting the execution of partial models at early stages of software development allows early detection of defects, which can be fixed more easily and at lower cost. This paper proposes a conceptual framework for the execution of partial models, which consists of three steps: \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">static analysis</i>\n, \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">automatic refinement</i>\n, and \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">input-driven execution</i>\n. First, a static analysis that respects the execution semantics of models is applied to detect problematic elements of models that cause problems for the execution. Second, using model transformation techniques, the models are refined automatically, mainly by adding decision points where missing information can be supplied. Third, refined models are executed, and when the execution reaches the decision points, it uses inputs obtained either interactively or by a script that captures how to deal with partial elements. We created an execution engine called \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">PMExec</i>\n for the execution of partial models of UML-RT (i.e., a modeling language for the development of soft real-time systems) that embodies our proposed framework. We evaluated \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">PMExec</i>\n based on several use-cases that show that the static analysis, refinement, and application of user input can be carried out with reasonable performance, and that the overhead of approach, which is mostly due to the refinement and the increase in model complexity it causes, is manageable. We also discuss the properties of the refinement formally, and show how the refinement preserves the original behaviors of the model.",
        "keywords": [
            "Tools",
            "Analytical models",
            "Unified modeling language",
            "Context modeling",
            "Static analysis",
            "Real-time systems",
            "Debugging"
        ]
    },
    {
        "title": "Formal Verification of Masking Countermeasures for Arithmetic Programs.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3008852",
        "volume": "48",
        "abstract": "Cryptographic algorithms are widely used to protect data privacy in many aspects of daily lives from smart card to cyber-physical systems. Unfortunately, programs implementing cryptographic algorithms may be vulnerable to practical power side-channel attacks, which may infer private data via statistical analysis of the correlation between power consumptions of an electronic device and private data. To thwart these attacks, several masking schemes have been proposed, giving rise to effective countermeasures for reducing the statistical correlation between private data and power consumptions. However, programs that rely on secure masking schemes are not secure a priori. Indeed, designing effective masking programs is a labor intensive and error-prone task. Although some techniques have been proposed for formally verifying masking countermeasures and for quantifying masking strength, they are currently limited to Boolean programs and suffer from low accuracy. In this work, we propose an approach for formally verifying masking countermeasures of arithmetic programs. Our approach is more accurate for arithmetic programs and more scalable for Boolean programs comparing to the existing approaches. It is essentially a synergistic integration of type inference and model-counting based methods, armed with domain specific heuristics. The type inference system allows a fast deduction of leakage-freeness of most intermediate computations, the model-counting based methods accounts for completeness, namely, to eliminate spurious flaws, and the heuristics facilitate both type inference and model-counting based reasoning, which improve scalability and efficiency in practice. In case that the program does contain leakage, we provide a method to quantify its masking strength. A distuiguished feature of our type sytem lies in its support of compositonal reasoning when verifying programs with procedure calls, so the need of inlining procedures can be significantly reduced. We have implemented our methods in a verification tool <small>QMVerif</small> which has been extensively evaluated on cryptographic benchmarks including full AES, DES and MAC-Keccak. The experimental results demonstrate the effectiveness and efficiency of our approach, especially for compositional reasoning. In particular, our tool is able to automatically prove leakage-freeness of arithmetic programs for which only manual proofs exist so far; it is also significantly faster than the state-of-the-art tools: EasyCrypt on common arithmetic programs, <small>QMSInfer</small>, SC Sniffer and maskVerif on Boolean programs.",
        "keywords": [
            "Cryptography",
            "Tools",
            "Computational modeling",
            "Software algorithms",
            "Power demand",
            "Cognition"
        ]
    },
    {
        "title": "Theoretical and Empirical Analyses of the Effectiveness of Metamorphic Relation Composition.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3009698",
        "volume": "48",
        "abstract": "Metamorphic Relations (MRs) play a key role in determining the fault detection capability of Metamorphic Testing (MT). As human judgement is required for MR identification, systematic MR generation has long been an important research area in MT. Additionally, due to the extra program executions required for follow-up test cases, some concerns have been raised about MT cost-effectiveness. Consequently, the reduction in testing costs associated with MT has become another important issue to be addressed. MR composition can address both of these problems. This technique can automatically generate new MRs by composing existing ones, thereby reducing the number of follow-up test cases. Despite this advantage, previous studies on MR composition have empirically shown that some composite MRs have lower fault detection capability than their corresponding component MRs. To investigate this issue, we performed theoretical and empirical analyses to identify what characteristics component MRs should possess so that their corresponding composite MR has at least the same fault detection capability as the component MRs do. We have also derived a convenient, but effective guideline so that the fault detection capability of MT will most likely not be reduced after composition.",
        "keywords": [
            "Testing",
            "Fault detection",
            "Software",
            "Guidelines",
            "Systematics",
            "Australia",
            "Companies"
        ]
    },
    {
        "title": "Quantifying, Characterizing, and Mitigating Flakily Covered Program Elements.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3010045",
        "volume": "48",
        "abstract": "Code coverage measures the degree to which source code elements (e.g., statements, branches) are invoked during testing. Despite growing evidence that coverage is a problematic measurement, it is often used to make decisions about where testing effort should be invested. For example, using coverage as a guide, tests should be written to invoke the non-covered program elements. At their core, coverage measurements assume that invocation of a program element during any test is equally valuable. Yet in reality, some tests are more robust than others. As a concrete instance of this, we posit in this paper that program elements that are only covered by flaky tests, i.e., tests with non-deterministic behaviour, are also worthy of investment of additional testing effort. In this paper, we set out to quantify, characterize, and mitigate “flakily covered” program elements (i.e., those elements that are only covered by flaky tests). To that end, we perform an empirical study of three large software systems from the OpenStack community. In terms of quantification, we find that systems are disproportionately impacted by flakily covered statements with 5 and 10 percent of the covered statements in Nova and Neutron being flakily covered, respectively, while \n<inline-formula><tex-math notation=\"LaTeX\">$&lt;1\\%$</tex-math></inline-formula>\n of Cinder statements are flakily covered. In terms of characterization, we find that incidences of flakily covered statements could not be well explained by solely using code characteristics, such as dispersion, ownership, and development activity. In terms of mitigation, we propose GreedyFlake – a test effort prioritization algorithm to maximize return on investment when tackling the problem of flakily covered program elements. We find that GreedyFlake outperforms baseline approaches by at least eight percentage points of Area Under the Cost Effectiveness Curve.",
        "keywords": [
            "Testing",
            "Neutrons",
            "Software",
            "Logic gates",
            "Data mining",
            "Robustness"
        ]
    },
    {
        "title": "Flexible Combinatorial Interaction Testing.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3010317",
        "volume": "48",
        "abstract": "We present Flexible Combinatorial Interaction Testing (F-CIT), which aims to improve the flexibility of combinatorial interaction testing (CIT) by eliminating the necessity of developing specialized constructors for CIT problems that cannot be efficiently and effectively addressed by the existing CIT constructors. F-CIT expresses the entities to be covered and the space of valid test cases, from which the samples are drawn to obtain full coverage, as constraints. Computing an F-CIT object (i.e., a set of test cases obtaining full coverage under a given coverage criterion) then turns into an interesting constraint solving problem, which we call \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">cov-CSP</i>\n. cov-CSP aims to divide the constraints, each representing an entity to be covered, into a minimum number of satisfiable clusters, such that a solution for a cluster represents a test case and the collection of all the test cases generated (one per cluster) constitutes an F-CIT object, covering each required entity at least once. To solve the cov-CSP problem, thus to compute F-CIT objects, we first present two constructors. One of these constructors attempts to cover as many entities as possible in a cluster before generating a test case, whereas the other constructor generates a test case first and then marks all the entities accommodated by this test case as covered. We then use these constructors to evaluate F-CIT in three studies, each of which addresses a different CIT problem. In the first study, we develop structure-based F-CIT objects to obtain decision coverage-adequate test suites. In the second study, we develop order-based F-CIT objects, which enhance a number of existing order-based coverage criteria by taking the reachability constraints imposed by graph-based models directly into account when computing interaction test suites. In the third study, we develop usage-based F-CIT objects to address the scenarios, in which standard covering arrays are not desirable due to their sizes, by choosing the entities to be covered based on their usage statistics collected from the field. We also carry out user studies to further evaluate F-CIT. The results of these studies suggest that F-CIT is more flexible than the existing CIT approaches.",
        "keywords": [
            "Testing",
            "Standards",
            "Software",
            "Computational modeling",
            "Computers",
            "Electronic mail",
            "Tools"
        ]
    },
    {
        "title": "CBUA: A Probabilistic, Predictive, and Practical Approach for Evaluating Test Suite Effectiveness.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3010361",
        "volume": "48",
        "abstract": "Knowing the effectiveness of a test suite is essential for many activities such as assessing the test adequacy of code and guiding the generation of new test cases. Mutation testing is a commonly used defect injection technique for evaluating the effectiveness of a test suite. However, it is usually computationally expensive, as a large number of mutants (buggy versions) are needed to be generated from a production code under test and executed against the test suite. In order to reduce the expensive testing cost, recent studies proposed to use supervised models to predict the effectiveness of a test suite without executing the test suite against the mutants. Nonetheless, the training of such a supervised model requires labeled data, which still depends on the costly mutant execution. Furthermore, existing models are based on traditional supervised learning techniques, which assume that the training and testing data come from the same distribution. But, in practice, software systems are subject to considerable concept drifts, i.e., the same distribution assumption usually does not hold. This can lead to inaccurate predictions of a learned supervised model on the target code as time progresses. To tackle these problems, in this paper, we propose a Coverage-Based Unsupervised Approach (CBUA) for evaluating the effectiveness of a test suite. Given a production code under test, the corresponding mutants, and a test suite, CBUA first collects the coverage information of the mutated statements in the target production code under the execution of the test suite. Then, CBUA employs coverage to estimate the probability of each mutant being alive. As such, a mutation score is computed to evaluate the test suite effectiveness and the predicted labels (i.e., killed or alive) are obtained. The whole process only requires a one-time execution of the test suite against the target production code, without involving any mutant execution and any training data. CBUA can ensure the score monotonicity property (i.e., adding test cases to a test suite does not decrease its mutation score), which may be violated by a supervised approach. The experimental results show that CBUA is very competitive with the state-of-the-art supervised approaches in prediction accuracy. In particular, CBUA is shown to be more effective in finding mutants that are covered but not killed by a test suite, which is helpful in identifying the weaknesses in the current test suite and generating new test cases accordingly. Since CBUA is an easy-to-implement approach with a low cost, we suggest that it should be used as a baseline approach for comparison when any novel prediction approach is proposed in future studies.",
        "keywords": [
            "Testing",
            "Predictive models",
            "Production",
            "Data models",
            "Computational modeling",
            "Training",
            "Training data"
        ]
    },
    {
        "title": "Efficient Execution of ATL Model Transformations Using Static Analysis and Parallelism.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3011388",
        "volume": "48",
        "abstract": "Although model transformations are considered to be the heart and soul of Model Driven Engineering (MDE), there are still several challenges that need to be addressed to unleash their full potential in industrial settings. Among other shortcomings, their performance and scalability remain unsatisfactory for dealing with large models, making their wide adoption difficult in practice. This paper presents A2L, a compiler for the parallel execution of ATL model transformations, which produces efficient code that can use existing multicore computer architectures, and applies effective optimizations at the transformation level using static analysis. We have evaluated its performance in both sequential and multi-threaded modes obtaining significant speedups with respect to current ATL implementations. In particular, we obtain speedups between 2.32x and 38.28x for the A2L sequential version, and between 2.40x and 245.83x when A2L is executed in parallel, with expected average speedups of 8.59x and 22.42x, respectively.",
        "keywords": [
            "Unified modeling language",
            "Analytical models",
            "Engines",
            "Java",
            "Scalability",
            "Static analysis",
            "Biological system modeling"
        ]
    },
    {
        "title": "Why My App Crashes? Understanding and Benchmarking Framework-Specific Exceptions of Android Apps.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3013438",
        "volume": "48",
        "abstract": "Mobile apps have become ubiquitous. Ensuring their correctness and reliability is important. However, many apps still suffer from occasional to frequent crashes, weakening their competitive edge. Large-scale, deep analyses of the characteristics of real-world app crashes can provide useful insights to both developers and researchers. However, such studies are difficult and yet to be carried out — this work fills this gap. We collected 16,245 and 8,760 unique exceptions from 2,486 open-source and 3,230 commercial Android apps, respectively, and observed that the exceptions thrown from Android framework (termed \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">“framework-specific exceptions”</i>\n) account for the majority. With one-year effort, we (1) extensively investigated these framework-specific exceptions, and (2) further conducted an online survey of 135 professional app developers about how they analyze, test, reproduce and fix these exceptions. Specifically, we aim to understand the framework-specific exceptions from several perspectives: (i) their characteristics (e.g., manifestation locations, fault taxonomy), (ii) the developers’ testing practices, (iii) existing bug detection techniques’ effectiveness, (iv) their reproducibility and (v) bug fixes. To enable follow-up research (e.g., bug understanding, detection, localization and repairing), we further systematically constructed, \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">DroidDefects</i>\n, the first comprehensive and largest benchmark of Android app exception bugs. This benchmark contains 33 \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">reproducible</i>\n exceptions (with test cases, stack traces, faulty and fixed app versions, bug types, etc.), and 3,696 \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">ground-truth</i>\n exceptions (real faults manifested by automated testing tools), which cover the apps with different complexities and diverse exception types. Based on our findings, we also built two prototype tools: Stoat+, an optimized dynamic testing tool, which quickly uncovered three previously-unknown, fixed crashes in Gmail and Google+; ExLocator, an exception localization tool, which can locate the root causes of specific exception types. Our dataset, benchmark and tools are publicly available on \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><uri>https://github.com/tingsu/droiddefects</uri></i>\n.",
        "keywords": [
            "Computer bugs",
            "Tools",
            "Androids",
            "Humanoid robots",
            "Benchmark testing"
        ]
    },
    {
        "title": "Inputs From Hell.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3013716",
        "volume": "48",
        "abstract": "<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Grammars</i>\n can serve as \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">producers</i>\n for structured test inputs that are syntactically correct by construction. A probabilistic grammar assigns probabilities to individual productions, thus controlling the distribution of input elements. Using the grammars as input parsers, we show how to \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">learn input distributions from input samples,</i>\n allowing to create inputs that are \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">similar</i>\n to the sample; by \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">inverting</i>\n the probabilities, we can create inputs that are \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">dissimilar</i>\n to the sample. This allows for three \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">test generation strategies</i>\n: 1) “Common inputs”–by learning from common inputs, we can create inputs that are \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">similar</i>\n to the sample; this is useful for regression testing. 2) “Uncommon inputs”–learning from common inputs and inverting probabilities yields inputs that are \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">strongly dissimilar</i>\n to the sample; this is useful for completing a test suite with “inputs from hell” that test uncommon features, yet are syntactically valid. 3) “Failure-inducing inputs”–learning from inputs that caused failures in the past gives us inputs that share similar features and thus also have a \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">high chance of triggering bugs</i>\n; this is useful for testing the completeness of fixes. Our evaluation on three common input formats (JSON, JavaScript, CSS) shows the effectiveness of these approaches. Results show that “common inputs” reproduced 96 percent of the methods induced by the samples. In contrast, for almost all subjects (95 percent), the “uncommon inputs” covered significantly different methods from the samples. Learning from failure-inducing samples reproduced all exceptions (100 percent) triggered by the failure-inducing samples and discovered new exceptions not found in any of the samples learned from.",
        "keywords": [
            "Grammar",
            "Production",
            "Probabilistic logic",
            "Computer bugs",
            "Software",
            "Test pattern generators"
        ]
    },
    {
        "title": "On How Bit-Vector Logic Can Help Verify LTL-Based Specifications.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3014394",
        "volume": "48",
        "abstract": "This paper studies how bit-vector logic (bv logic) can help improve the efficiency of verifying specifications expressed in Linear Temporal Logic (LTL). First, it exploits the notion of Bounded Satisfiability Checking to propose an improved encoding of LTL formulae into formulae of bv logic, which can be formally verified by means of Satisfiability Modulo Theories (SMT) solvers. To assess the gain in efficiency, we compare the proposed encoding, implemented in our tool \n<inline-formula><tex-math notation=\"LaTeX\">$\\mathbb {Z}$</tex-math></inline-formula>\not, against three well-known encodings available in the literature: the classic bounded encoding and the optimized, incremental one, as implemented in both NuSMV and nuXmv, and the encoding optimized for metric temporal logic, which was the “standard” implementation provided by \n<inline-formula><tex-math notation=\"LaTeX\">$\\mathbb {Z}$</tex-math></inline-formula>\not. We also compared the newly proposed solution against five additional efficient algorithms proposed by nuXmv, which is the state-of-the-art tool for verifying LTL specifications. The experiments show that the new encoding provides significant benefits with respect to existing tools. Since the first set of experiments only used Z3 as SMT solver, we also wanted to assess whether the benefits were induced by the specific solver or were more general. This is why we also embedded different SMT solvers in \n<inline-formula><tex-math notation=\"LaTeX\">$\\mathbb {Z}$</tex-math></inline-formula>\not. Besides Z3, we also carried out experiments with CVC4, Mathsat, Yices2, and Boolector, and compared the results against the first and second best solutions provided by either NuSMV or nuXmv. Obtained results witness that the benefits of the bv logic encoding are independent of the specific solver. Bv logic-based solutions are better than traditional ones with only a few exceptions. It is also true that there is no particular SMT solver that outperformed the others. Boolector is often the best as for memory usage, while Yices2 and Z3 are often the fastest ones.",
        "keywords": [
            "Encoding",
            "Tools",
            "Standards",
            "Unified modeling language",
            "Optimization",
            "Semantics",
            "Measurement"
        ]
    },
    {
        "title": "Orderly Generation of Test Data via Sorting Mutant Branches Based on Their Dominance Degrees for Weak Mutation Testing.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3014960",
        "volume": "48",
        "abstract": "Compared with traditional structural test criteria, test data generated based on mutation testing are proved more effective at detecting faults. However, not all test data have the same potence in detecting software faults. If test data are prioritized while generating for mutation testing, the defect detectability of the test suite can be further strengthened. In view of this, we propose a method of test data generation for weak mutation testing via sorting mutant branches based on their dominance degrees. First, the problem of weak mutation testing is transformed into that of covering mutant branches for a transformed program. Then, the dominance relation of mutant branches in the transformed program is analyzed to obtain the non-dominated mutant branches and their dominance degrees. Following that, we prioritize all non-dominated mutant branches in descending order by virtue of their dominance degrees. Finally, the test data are generated in an orderly manner by selecting the mutant branches sequentially. The experimental results on 15 programs show that compared with other methods, the proposed test data generation method can not only improve the error detectability of the test suite, but also has higher efficiency.",
        "keywords": [
            "Software",
            "Software testing",
            "Fault detection",
            "Control engineering",
            "Data mining",
            "Sorting"
        ]
    },
    {
        "title": "Chatbot4QR: Interactive Query Refinement for Technical Question Retrieval.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3016006",
        "volume": "48",
        "abstract": "Technical Q&#x0026;A sites (e.g., Stack Overflow (SO)) are important resources for developers to search for knowledge about technical problems. Search engines provided in Q&#x0026;A sites and information retrieval approaches (e.g., word embedding-based) have limited capabilities to retrieve relevant questions when queries are imprecisely specified, such as missing important technical details (e.g., the user&#x2019;s preferred programming languages). Although many automatic query expansion approaches have been proposed to improve the quality of queries by expanding queries with relevant terms, the information missed in a query is not identified. Moreover, without user involvement, the existing query expansion approaches may introduce unexpected terms and lead to undesired results. In this paper, we propose an interactive query refinement approach for question retrieval, named <i>Chatbot4QR</i>, which can assist users in recognizing and clarifying technical details missed in queries and thus retrieve more relevant questions for users. Chatbot4QR automatically detects missing technical details in a query and generates several clarification questions (CQs) to interact with the user to capture their overlooked technical details. To ensure the accuracy of CQs, we design a heuristic-based approach for CQ generation after building two kinds of technical knowledge bases: a manually categorized result of 1,841 technical tags in SO and the multiple version-frequency information of the tags. We develop a Chatbot4QR prototype that uses 1.88 million SO questions as the repository for question retrieval. To evaluate Chatbot4QR, we conduct six user studies with 25 participants on 50 experimental queries. The results are as follows. (1) On average 60.8 percent of the CQs generated for a query are useful for helping the participants recognize missing technical details. (2) Chatbot4QR can rapidly respond to the participants after receiving a query within approximately 1.3 seconds. (3) The refined queries contribute to retrieving more relevant SO questions than nine baseline approaches. For more than 70 percent of the participants who have preferred techniques on the query tasks, Chatbot4QR significantly outperforms the state-of-the-art word embedding-based retrieval approach with an improvement of at least 54.6 percent in terms of two measurements: Pre<inline-formula><tex-math notation=\"LaTeX\">$@$</tex-math><alternatives><mml:math><mml:mo>@</mml:mo></mml:math><inline-graphic xlink:href=\"xia-ieq1-3016006.gif\"/></alternatives></inline-formula>k and NDCG<inline-formula><tex-math notation=\"LaTeX\">$@$</tex-math><alternatives><mml:math><mml:mo>@</mml:mo></mml:math><inline-graphic xlink:href=\"xia-ieq2-3016006.gif\"/></alternatives></inline-formula>k. (4) For 48-88 percent of the assigned query tasks, the participants obtain more desired results after interacting with Chatbot4QR than directly searching from Web search engines (e.g., the SO search engine and Google) using the original queries.",
        "keywords": [
            "Search engines",
            "Web search",
            "Java",
            "Task analysis",
            "Engines",
            "Databases"
        ]
    },
    {
        "title": "Conditional Quantitative Program Analysis.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3016778",
        "volume": "48",
        "abstract": "Standards for certifying safety-critical systems have evolved to permit the inclusion of evidence generated by program analysis and verification techniques. The past decade has witnessed the development of several program analyses that are capable of computing guarantees on bounds for the probability of failure. This paper develops a novel program analysis framework, CQA, that combines evidence from different underlying analyses to compute bounds on failure probability. It reports on an evaluation of different CQA-enabled analyses and implementations of state-of-the-art quantitative analyses to evaluate their relative strengths and weaknesses. To conduct this evaluation, we filter an existing verification benchmark to reflect certification evidence generation challenges. Our evaluation across the resulting set of 136 C programs, totaling more than 385k SLOC, each with a probability of failure below \n<inline-formula><tex-math notation=\"LaTeX\">$10^{-4}$</tex-math></inline-formula>\n, demonstrates how CQA extends the state-of-the-art. The CQA infrastructure, including tools, subjects, and generated data is publicly available at \n<uri>bitbucket.org/mgerrard/cqa</uri>\n.",
        "keywords": [
            "Safety",
            "Standards",
            "Static analysis",
            "Software",
            "Reliability",
            "Benchmark testing"
        ]
    },
    {
        "title": "DPWord2Vec: Better Representation of Design Patterns in Semantics.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3017336",
        "volume": "48",
        "abstract": "With the plain text descriptions of design patterns, developers could better learn and understand the definitions and usage scenarios of design patterns. To facilitate the automatic usage of these descriptions, e.g., recommending design patterns by free-text queries, design patterns and natural languages should be adequately associated. Existing studies usually use texts in design pattern books as the representations of design patterns to calculate similarities with the queries. However, this way is problematic. Lots of information of design patterns may be absent from design pattern books and many words would be out of vocabulary due to the content limitation of these books. To overcome these issues, a more comprehensive method should be constructed to estimate the relatedness between design patterns and natural language words. Motivated by Word2Vec, in this study, we propose DPWord2Vec that embeds design patterns and natural language words into vectors simultaneously. We first build a corpus containing more than 400 thousand documents extracted from design pattern books, Wikipedia, and Stack Overflow. Next, we redefine the concept of context window to associate design patterns with words. Then, the design pattern and word vector representations are learnt by leveraging an advanced word embedding method. The learnt design pattern and word vectors can be universally used in textual description based design pattern tasks. An evaluation shows that DPWord2Vec outperforms the baseline algorithms by 24.2-120.9 percent in measuring the similarities between design patterns and words in terms of Spearman&#x2019;s rank correlation coefficient. Moreover, we adopt DPWord2Vec on two typical design pattern tasks. In the design pattern tag recommendation task, the DPWord2Vec-based method outperforms two state-of-the-art algorithms by 6.6 and 32.7 percent respectively when considering <inline-formula><tex-math notation=\"LaTeX\">$Recall@10$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mo>@</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=\"jiang-ieq1-3017336.gif\"/></alternatives></inline-formula>. In the design pattern selection task, DPWord2Vec improves the existing methods by 6.5-70.7 percent in terms of MRR.",
        "keywords": [
            "Natural languages",
            "Task analysis",
            "Semantics",
            "Windows",
            "Vocabulary",
            "Software design"
        ]
    },
    {
        "title": "Heuristic and Neural Network Based Prediction of Project-Specific API Member Access.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3017794",
        "volume": "48",
        "abstract": "Code completion is to predict the rest of a statement a developer is typing. Although advanced code completion approaches have greatly improved the accuracy of code completion in modern IDEs, it remains challenging to predict project-specific API method invocations or field accesses because little knowledge about such elements could be learned in advance. To this end, in this paper we propose an accurate approach called HeeNAMA to suggesting the next project-specific API member access. HeeNAMA focuses on a specific but common case of code completion: suggesting the following member access whenever a project-specific API instance is followed by a dot on the right hand side of an assignment. By focusing on such a specific case, HeeNAMA can take full advantages of the context of the code completion, including the type of the left hand side expression of the assignment, the identifier on the left hand side, the type of the base instance, and similar assignments typed in before. All such information together enables highly accurate code completion. Given an incomplete assignment, HeeNAMA generates the initial candidate set according to the type of the base instance, and excludes those candidates that are not type compatible with the left hand side of the assignment. If the enclosing project contains assignments highly similar to the incomplete assignment, it makes suggestions based on such assignments. Otherwise, it selects the one from the initial candidate set that has the greatest lexical similarity with the left hand side of the assignment. Finally, it employs a neural network to filter out risky predictions, which guarantees high precision. Evaluation results on open-source applications suggest that compared to the state-of-the-art approaches and the state-of-the-practice tools HeeNAMA improves precision and recall by 70.68 and 25.23 percent, relatively.",
        "keywords": [
            "Hidden Markov models",
            "Neural networks",
            "Computational modeling",
            "Open source software",
            "Java",
            "Data mining",
            "Tools"
        ]
    },
    {
        "title": "Deep Learning Based Program Generation From Requirements Text: Are We There Yet?",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3018481",
        "volume": "48",
        "abstract": "To release developers from time-consuming software development, many approaches have been proposed to generate source code automatically according to software requirements. With significant advances in deep learning and natural language processing, deep learning-based approaches are proposed to generate source code from natural language descriptions. The key insight is that given a large corpus of software requirements and their corresponding implementations, advanced deep learning techniques may learn how to translate software requirements into source code that fulfill such requirements. Although such approaches are reported to be highly accurate, they are evaluated on datasets that are rather small, lack of diversity, and significantly different from real-world software requirements. To this end, we build a large scale dataset that is composed of longer requirements as well as validated implementations. We evaluate the state-of-the-art approaches on this new dataset, and the results suggest that their performance on our dataset is significantly lower than that on existing datasets concerning the common metrics, i.e., BLEU. Evaluation results also suggest that the generated programs often contain syntactic and semantical errors, and none of them can pass even a single predefined test case. Further analysis reveals that the state-of-the-art approaches learn little from software requirements, and most of the successfully generated statements are popular statements in the training programs. Based on this finding, we propose a popularity-based approach that always generates the most popular statements in training programs regardless of the input (software requirements). Evaluation results suggest that none of the state-of-the-art approaches can outperform this simple statistics-based approach. As a conclusion, deep learning-based program generation requires significant improvement in the future, and our dataset may serve as a basis for future research in this direction.",
        "keywords": [
            "Software",
            "Unified modeling language",
            "Object oriented modeling",
            "Syntactics",
            "Tools",
            "DSL",
            "Deep learning"
        ]
    },
    {
        "title": "Clustering Crowdsourced Test Reports of Mobile Applications Using Image Understanding.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3017514",
        "volume": "48",
        "abstract": "Crowdsourced testing has been widely used to improve software quality as it can detect various bugs and simulate real usage scenarios. Crowdsourced workers perform tasks on crowdsourcing platforms and present their experiences as test reports, which naturally generates an overwhelming number of test reports. Therefore, inspecting these reports becomes a time-consuming yet inevitable task. In recent years, many text-based prioritization and clustering techniques have been proposed to address this challenge. However, in mobile testing, test reports often consist of only short test descriptions but rich screenshots. Compared with the uncertainty of textual information, well-defined screenshots can often adequately express the mobile application&#x2019;s activity views. In this paper, by employing image-understanding techniques, we propose an approach for clustering crowdsourced test reports of mobile applications based on both textual and image features to assist the inspection procedure. We employ Spatial Pyramid Matching (SPM) to measure the similarity of the screenshots and use the natural-language-processing techniques to compute the textual distance of test reports. To validate our approach, we conducted an experiment on 6 industrial crowdsourced projects that contain more than 1600 test reports and 1400 screenshots. The results show that our approach is capable of outperforming the baselines by up to 37 percent regarding the APFD metric. Further, we analyze the parameter sensitivity of our approach and discuss the settings for different application scenarios.",
        "keywords": [
            "Testing",
            "Task analysis",
            "Mobile applications",
            "Computer bugs",
            "Software",
            "Mars",
            "Mobile handsets"
        ]
    },
    {
        "title": "Redundancy, Context, and Preference: An Empirical Study of Duplicate Pull Requests in OSS Projects.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3018726",
        "volume": "48",
        "abstract": "OSS projects are being developed by globally distributed contributors, who often collaborate through the pull-based model today. While this model lowers the barrier to entry for OSS developers by synthesizing, automating and optimizing the contribution process, coordination among an increasing number of contributors remains as a challenge due to the asynchronous and self-organized nature of distributed development. In particular, duplicate contributions, where multiple different contributors unintentionally submit duplicate pull requests to achieve the same goal, are an elusive problem that may waste effort in automated testing, code review and software maintenance. While the issue of duplicate pull requests has been highlighted, to what extent duplicate pull requests affect the development in OSS communities has not been well investigated. In this paper, we conduct a mixed-approach study to bridge this gap. Based on a comprehensive dataset constructed from 26 popular GitHub projects, we obtain the following findings: (a) Duplicate pull requests result in redundant human and computing resources, exerting a significant impact on the contribution and evaluation process. (b) Contributors&#x2019; inappropriate working patterns and the drawbacks of their collaborating environment might result in duplicate pull requests. (c) Compared to non-duplicate pull requests, duplicate pull requests have significantly different features, e.g., being submitted by inexperienced contributors, being fixing bugs, touching cold files, and solving tracked issues. (d) Integrators choosing between duplicate pull requests prefer to accept those with early submission time, accurate and high-quality implementation, broad coverage, test code, high maturity, deep discussion, and active response. Finally, actionable suggestions and implications are proposed for OSS practitioners.",
        "keywords": [
            "Collaboration",
            "Computer bugs",
            "Tools",
            "Cloning",
            "Synchronization",
            "Testing",
            "Encoding"
        ]
    },
    {
        "title": "Integrating an Ensemble Surrogate Model's Estimation into Test Data Generation.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3019406",
        "volume": "48",
        "abstract": "For the path coverage testing of a Message-Passing Interface (MPI) program, test data generation based on an evolutionary optimization algorithm (EOA) has been widely known. However, during the use of the above technique, it is necessary to evaluate the fitness of each evolutionary individual by executing the program, which is generally computationally expensive. In order to reduce the computational cost, this article proposes a method of integrating an ensemble surrogate model’s estimation into the process of generating test data. The proposed method first produces a number of test inputs using an EOA, and forms a training set together with their real fitness. Then, this article trains an ensemble surrogate model (ESM) based on the training set, which is employed to estimate the fitness of each individual. Finally, a small number of individuals with good estimations are selected to further execute the program, so as to have their real fitness for the subsequent evolution. This article applies the proposed method to seven benchmark MPI programs, which is compared with several state-of-the-art approaches. The experimental results show that the proposed method can generate test data with significantly low computational cost.",
        "keywords": [
            "Testing",
            "Estimation",
            "Data models",
            "Optimization",
            "Computational efficiency",
            "Training",
            "Sun"
        ]
    },
    {
        "title": "What Leads to a Confirmatory or Disconfirmatory Behavior of Software Testers?",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3019892",
        "volume": "48",
        "abstract": "<bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Background:</b>\n The existing literature in software engineering reports adverse effects of confirmation bias on software testing. Confirmation bias among software testers leads to confirmatory behavior, which is designing or executing relatively more specification consistent test cases (confirmatory behavior) than specification inconsistent test cases (disconfirmatory behavior). \n<bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Objective:</b>\n We aim to explore the antecedents to confirmatory and disconfirmatory behavior of software testers. Furthermore, we aim to understand why and how those antecedents lead to (dis)confirmatory behavior. \n<bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Method:</b>\n We follow grounded theory method for the analyses of the data collected through semi-structured interviews with twelve software testers. \n<bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Results:</b>\n We identified twenty antecedents to (dis)confirmatory behavior, and classified them in nine categories. Experience and Time are the two major categories. Experience is a disconfirmatory category, which also determines which behavior (confirmatory or disconfirmatory) occurs first among software testers, as an effect of other antecedents. Time Pressure is a confirmatory antecedent of the Time category. It also contributes to the confirmatory effects of antecedents of other categories. \n<bold xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Conclusion:</b>\n The disconfirmatory antecedents, especially that belong to the testing process, e.g., test suite reviews by project team members, may help circumvent the deleterious effects of confirmation bias in software testing. If a team’s resources permit, the designing and execution of a test suite could be divided among the test team members, as different perspectives of testers may help to detect more errors. The results of our study are based on a single context where dedicated testing teams focus on higher levels of testing. The study’s scope does not account for the testing performed by developers. Future work includes exploring other contexts to extend our results.",
        "keywords": [
            "Software",
            "Software testing",
            "Interviews",
            "Electronic mail",
            "Companies",
            "Decision making"
        ]
    },
    {
        "title": "Optimization of Software Release Planning Considering Architectural Dependencies, Cost, and Value.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3020013",
        "volume": "48",
        "abstract": "Within any incremental development paradigm, there exists a tension between the desire to deliver value to the customer early and the desire to reduce cost by avoiding architectural refactoring and rework in subsequent releases. What is lacking is an analytical framework that quantifies opportunities and risks of choosing one or the other of these strategies or a blend of the two. This article demonstrates the use of design structure and domain mapping matrices for analyzing architectural dependencies and proposes an optimization-based decision-making technique to support effective release planning. The optimization models recommend the order in which architectural elements and features should be implemented across different releases so as to: (a) minimize rework cost; (b) maximize early value delivery; or (c) optimize an integrated measure of cost and value. These analytic models can be applied earlier in the life cycle and, hence, provide timely information about the progress and changes that occur at each iteration.",
        "keywords": [
            "Planning",
            "Software",
            "Computer architecture",
            "Optimization",
            "Electronic mail",
            "Analytical models",
            "Matrix decomposition"
        ]
    },
    {
        "title": "CODIT: Code Editing With Tree-Based Neural Models.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3020502",
        "volume": "48",
        "abstract": "The way developers edit day-to-day code tends to be repetitive, often using existing code elements. Many researchers have tried to automate repetitive code changes by learning from specific change templates which are applied to limited scope. The advancement of deep neural networks and the availability of vast open-source evolutionary data opens up the possibility of automatically learning those templates from the wild. However, deep neural network based modeling for code changes and code in general introduces some specific problems that needs specific attention from research community. For instance, compared to natural language, source code vocabulary can be significantly larger. Further, good changes in code do not break its syntactic structure. Thus, deploying state-of-the-art neural network models without adapting the methods to the source code domain yields sub-optimal results. To this end, we propose a novel tree-based neural network system to model source code changes and learn code change patterns from the wild. Specifically, we propose a tree-based neural machine translation model to learn the probability distribution of changes in code. We realize our model with a change suggestion engine, \n<sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Codit</small>\n, and train the model with more than 24k real-world changes and evaluate it on 5k patches. Our evaluation shows the effectiveness of \n<sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Codit</small>\n in learning and suggesting patches. \n<sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Codit</small>\n can also learn specific bug fix pattern from bug fixing patches and can fix 25 bugs out of 80 bugs in Defects4J.",
        "keywords": [
            "Computer bugs",
            "Predictive models",
            "Reactive power",
            "Probability distribution",
            "Syntactics",
            "Neural networks",
            "Adaptation models"
        ]
    },
    {
        "title": "Watch Out for Extrinsic Bugs! A Case Study of Their Impact in Just-In-Time Bug Prediction Models on the OpenStack Project.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3021380",
        "volume": "48",
        "abstract": "Intrinsic bugs are bugs for which a bug-introducing change can be identified in the version control system of a software. In contrast, extrinsic bugs are caused by external changes to a software, such as errors in external APIs; thereby they do not have an explicit bug-introducing change in the version control system. Although most previous research literature has assumed that all bugs are of \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">intrinsic</i>\n nature, in a previous study, we show that not all bugs are intrinsic. This paper shows an example of how considering extrinsic bugs can affect software engineering research. Specifically, we study the impact of extrinsic bugs in Just-In-Time bug prediction by partially replicating a recent study by McIntosh and Kamei on JIT models. These models are trained using properties of earlier bug-introducing changes. Since extrinsic bugs do not have bug-introducing changes in the version control system, we manually curate McIntosh and Kamei’s dataset to distinguish between intrinsic and extrinsic bugs. Then, we address their original research questions, this time removing extrinsic bugs, to study whether bug-introducing changes are a moving target in Just-In-Time bug prediction. Finally, we study whether characteristics of intrinsic and extrinsic bugs are different. Our results show that intrinsic and extrinsic bugs are of different nature. When removing extrinsic bugs the performance is different up to 16 percent Area Under the Curve points. This indicates that our JIT models obtain a more accurate representation of the real world. We conclude that extrinsic bugs negatively impact Just-In-Time models. Furthermore, we offer evidence that extrinsic bugs should be further investigated, as they can significantly impact how software engineers understand bugs.",
        "keywords": [
            "Computer bugs",
            "Predictive models",
            "Software",
            "Data models",
            "Control systems",
            "Analytical models",
            "Context modeling"
        ]
    },
    {
        "title": "Efficient Summary Reuse for Software Regression Verification.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3021477",
        "volume": "48",
        "abstract": "Software systems evolve throughout their life cycles. Many revisions are produced over time. Verifying each revision of the software is impractical. Regression verification suggests reusing intermediate results from the previous verification runs. This paper studies regression verification via summary reuse. Not only procedure summaries, but also loop summaries are proposed to be reused. This paper proposes a fully automatic regression verification technique in the context of CEGAR. A lazy counterexample analysis technique is developed to improve the efficiency of summary reuse. We performed extensive experiments on two large sets of industrial programs (3,675 revisions of 488 Linux kernel device drivers). Results show that our summary reuse technique saves 84 to 93 percent analysis time of the regression verification.",
        "keywords": [
            "Task analysis",
            "Performance evaluation",
            "Linux",
            "Device drivers",
            "Safety",
            "Interpolation"
        ]
    },
    {
        "title": "The Effectiveness of Supervised Machine Learning Algorithms in Predicting Software Refactoring.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3021736",
        "volume": "48",
        "abstract": "Refactoring is the process of changing the internal structure of software to improve its quality without modifying its external behavior. Empirical studies have repeatedly shown that refactoring has a positive impact on the understandability and maintainability of software systems. However, before carrying out refactoring activities, developers need to identify refactoring opportunities. Currently, refactoring opportunity identification heavily relies on developers&#x2019; expertise and intuition. In this paper, we investigate the effectiveness of machine learning algorithms in predicting software refactorings. More specifically, we train six different machine learning algorithms (i.e., Logistic Regression, Naive Bayes, Support Vector Machine, Decision Trees, Random Forest, and Neural Network) with a dataset comprising over two million refactorings from 11,149 real-world projects from the Apache, F-Droid, and GitHub ecosystems. The resulting models predict 20 different refactorings at class, method, and variable-levels with an accuracy often higher than 90 percent. Our results show that (i) Random Forests are the best models for predicting software refactoring, (ii) process and ownership metrics seem to play a crucial role in the creation of better models, and (iii) models generalize well in different contexts.",
        "keywords": [
            "Biological system modeling",
            "Measurement",
            "Tools",
            "Software",
            "Predictive models",
            "Context modeling",
            "Prediction algorithms"
        ]
    },
    {
        "title": "A Fast Clustering Algorithm for Modularization of Large-Scale Software Systems.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3022212",
        "volume": "48",
        "abstract": "A software system evolves over time in order to meet the needs of users. Understanding a program is the most important step to apply new requirements. Clustering techniques through dividing a program into small and meaningful parts make it possible to understand the program. In general, clustering algorithms are classified into two categories: hierarchical and non-hierarchical algorithms (such as search-based approaches). While clustering problems generally tend to be NP-hard, search-based algorithms produce acceptable clustering and have time and space constraints and hence they are inefficient in large-scale software systems. Most algorithms which currently used in software clustering fields do not scale well when applied to large and very large applications. In this paper, we present a new and fast clustering algorithm, FCA, that can overcome space and time constraints of existing algorithms by performing operations on the dependency matrix and extracting other matrices based on a set of features. The experimental results on ten small-sized applications, ten folders with different functionalities from Mozilla Firefox, a large-sized application (namely ITK), and a very large-sized application (namely Chromium) demonstrate that the proposed algorithm achieves higher quality modularization compared with hierarchical algorithms. It can also compete with search-based algorithms and a clustering algorithm based on subsystem patterns. But the running time of the proposed algorithm is much shorter than that of the hierarchical and non-hierarchical algorithms. The source code of the proposed algorithm can be accessed at \n<uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/SoftwareMaintenanceLab</uri>\n.",
        "keywords": [
            "Clustering algorithms",
            "Software algorithms",
            "Search problems",
            "Software systems",
            "Semantics",
            "Software architecture"
        ]
    },
    {
        "title": "Code Review Knowledge Perception: Fusing Multi-Features for Salient-Class Location.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3021902",
        "volume": "48",
        "abstract": "Code review is a common software engineering practice of practical importance to reduce software defects. Review today is often with the help of specialized tools, such as Gerrit. However, even in a tool-supported code review involves a significant amount of human effort to understand the code change, because the information required to inspect code changes may distribute across multiple files that reviewers are not familiar with. Code changes are often organized as commits for review. In this paper, we found that most of the commits contain a salient class(es), which is saliently modified and causes the modification of the rest classes in a commit. Our user studies confirmed that identifying the salient class in a commit can facilitate reviewers in understanding code change. Inspired by the effectiveness of machine learning techniques in the classification field, we model the salient class identification as a binary classification problem and a number of discriminative features is extracted for a commit and used to characterize the salience of a class. The experiments results show that our approach achieves an accuracy of 88 percent. A user study with industrial developers shows that our approach can really improve the efficiency of reviewers understanding code changes in a reviewing scenario without using comment.",
        "keywords": [
            "Feature extraction",
            "Semantics",
            "Tools",
            "Couplings",
            "Open source software",
            "Knowledge engineering"
        ]
    },
    {
        "title": "Predicting Defective Lines Using a Model-Agnostic Technique.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3023177",
        "volume": "48",
        "abstract": "Defect prediction models are proposed to help a team prioritize the areas of source code files that need Software Quality Assurance (SQA) based on the likelihood of having defects. However, developers may waste their unnecessary effort on the whole file while only a small fraction of its source code lines are defective. Indeed, we find that as little as 1-3 percent of lines of a file are defective. Hence, in this work, we propose a novel framework (called <small>Line-DP</small>) to identify defective lines using a model-agnostic technique, i.e., an Explainable AI technique that provides information why the model makes such a prediction. Broadly speaking, our <small>Line-DP</small> first builds a file-level defect model using code token features. Then, our <small>Line-DP</small> uses a state-of-the-art model-agnostic technique (i.e., LIME) to identify risky tokens, i.e., code tokens that lead the file-level defect model to predict that the file will be defective. Then, the lines that contain risky tokens are predicted as defective lines. Through a case study of 32 releases of nine Java open source systems, our evaluation results show that our <small>Line-DP</small> achieves an average recall of 0.61, a false alarm rate of 0.47, a top 20&#x0025;LOC recall of 0.27, and an initial false alarm of 16, which are statistically better than six baseline approaches. Our evaluation shows that our <small>Line-DP</small> requires an average computation time of 10 seconds including model construction and defective line identification time. In addition, we find that 63 percent of defective lines that can be identified by our <small>Line-DP</small> are related to common defects (e.g., argument change, condition change). These results suggest that our <small>Line-DP</small> can effectively identify defective lines that contain common defects while requiring a smaller amount of inspection effort and a manageable computation cost. The contribution of this paper builds an important step towards line-level defect prediction by leveraging a model-agnostic technique.",
        "keywords": [
            "Predictive models",
            "Computational modeling",
            "Software quality",
            "Software systems",
            "Pipelines",
            "Software engineering"
        ]
    },
    {
        "title": "An Empirical Study of C++ Vulnerabilities in Crowd-Sourced Code Examples.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3023664",
        "volume": "48",
        "abstract": "Software developers share programming solutions in Q&#x0026;A sites like Stack Overflow, Stack Exchange, Android forum, and so on. The reuse of crowd-sourced code snippets can facilitate rapid prototyping. However, recent research shows that the shared code snippets may be of low quality and can even contain vulnerabilities. This paper aims to understand the nature and the prevalence of security vulnerabilities in crowd-sourced code examples. To achieve this goal, we investigate security vulnerabilities in the C++ code snippets shared on Stack Overflow over a period of 10 years. In collaborative sessions involving multiple human coders, we manually assessed each code snippet for security vulnerabilities following CWE (Common Weakness Enumeration) guidelines. From the 72,483 reviewed code snippets used in at least one project hosted on GitHub, we found a total of 99 vulnerable code snippets categorized into 31 types. Many of the investigated code snippets are still not corrected on Stack Overflow. The 99 vulnerable code snippets found in Stack Overflow were reused in a total of 2859 GitHub projects. To help improve the quality of code snippets shared on Stack Overflow, we developed a browser extension that allows Stack Overflow users to be notified for vulnerabilities in code snippets when they see them on the platform.",
        "keywords": [
            "Security",
            "C++ languages",
            "Androids",
            "Humanoid robots",
            "Tools",
            "Open source software"
        ]
    },
    {
        "title": "The Best Laid Plans or Lack Thereof: Security Decision-Making of Different Stakeholder Groups.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3023735",
        "volume": "48",
        "abstract": "Cyber security requirements are influenced by the priorities and decisions of a range of stakeholders. Board members and Chief Information Security Officers (CISOs) determine strategic priorities. Managers have responsibility for resource allocation and project management. Legal professionals concern themselves with regulatory compliance. Little is understood about how the security decision-making approaches of these different stakeholders contrast, and if particular groups of stakeholders have a better appreciation of security requirements during decision-making. Are risk analysts better decision makers than CISOs? Do security experts exhibit more effective strategies than board members? This paper explores the effect that different experience and diversity of expertise has on the quality of a team&#x0027;s cyber security decision-making and whether teams with members from more varied backgrounds perform better than those with more focused, homogeneous skill sets. Using data from 208 sessions and 948 players of a tabletop game run <i>in the wild</i> by a major national organization over 16 months, we explore how choices are affected by player background (e.g., cyber security experts versus risk analysts, board-level decision makers versus technical experts) and different team make-ups (homogeneous teams of security experts versus various mixes). We find that no group of experts makes significantly better game decisions than anyone else, and that their biases lead them to not fully comprehend what they are defending or how the defenses work.",
        "keywords": [
            "Games",
            "Stakeholders",
            "Computer security",
            "Decision making",
            "Organizations",
            "Investment"
        ]
    },
    {
        "title": "PerfJIT: Test-Level Just-in-Time Prediction for Performance Regression Introducing Commits.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3023955",
        "volume": "48",
        "abstract": "Performance issues may compromise user experiences, increase the cost resources, and cause field failures. One of the most prevalent performance issues is performance regression. Due to the importance and challenges in performance regression detection, prior research proposes various automated approaches that detect performance regressions. However, the performance regression detection is conducted after the system is built and deployed. Hence, large amounts of resources are still required to locate and fix performance regressions. In our paper, we propose an approach that automatically predicts whether a test would manifest performance regressions given a code commit. In particular, we extract both traditional metrics and performance-related metrics from the code changes that are associated with each test. For each commit, we build random forest classifiers that are trained from all prior commits to predict in this commit whether each test would manifest performance regression. We conduct case studies on three open-source systems (<i>Hadoop</i>, <i>Cassandra</i>, and <i>OpenJPA</i>). Our results show that our approach can predict tests that manifest performance regressions in a commit with high AUC values (on average 0.86). Our approach can drastically reduce the testing time needed to detect performance regressions. In addition, we find that our approach could be used to detect the introduction of six out of nine real-life performance issues from the subject systems during our studied period. Finally, we find that traditional metrics that are associated with size and code change histories are the most important factors in our models. Our approach and the study results can be leveraged by practitioners to effectively cope with performance regressions in a timely and proactive manner.",
        "keywords": [
            "Measurement",
            "Predictive models",
            "Software",
            "Task analysis",
            "Benchmark testing",
            "Logistics"
        ]
    },
    {
        "title": "Probabilistic Preference Planning Problem for Markov Decision Processes.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3024215",
        "volume": "48",
        "abstract": "The classical planning problem aims to find a sequence of permitted actions leading a system to a designed state, i.e., to achieve the system&#x2019;s task. However, in many realistic cases we also have requirements on how to complete the task, indicating that some behaviors and situations are more preferred than others. In this paper, we present the probabilistic preference-based planning problem (<inline-formula><tex-math notation=\"LaTeX\">$\\mathrm{P4}$</tex-math><alternatives><mml:math><mml:mrow><mml:mi mathvariant=\"normal\">P</mml:mi><mml:mn>4</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq1-3024215.gif\"/></alternatives></inline-formula>) for Markov decision processes, where the preferences are defined based on an enriched probabilistic LTL-style logic. We first recall <inline-formula><tex-math notation=\"LaTeX\">$\\mathrm{\\mathrm{P4} {}Solver}$</tex-math><alternatives><mml:math><mml:mrow><mml:mi mathvariant=\"normal\">P</mml:mi><mml:mn>4</mml:mn><mml:mrow/><mml:mi> Solver </mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq2-3024215.gif\"/></alternatives></inline-formula>, an SMT-based planner computing the preferred plan by reducing the problem to a quadratic programming one previously developed to solve <inline-formula><tex-math notation=\"LaTeX\">$\\mathrm{P4}$</tex-math><alternatives><mml:math><mml:mrow><mml:mi mathvariant=\"normal\">P</mml:mi><mml:mn>4</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq3-3024215.gif\"/></alternatives></inline-formula>. To improve computational efficiency and scalability, we then introduce a new encoding of the probabilistic preference-based planning problem as a multi-objective model checking one, and propose the corresponding planner <inline-formula><tex-math notation=\"LaTeX\">$\\mathrm{\\mathrm{P4} {}Solver} _{{MO}}$</tex-math><alternatives><mml:math><mml:mrow><mml:mi mathvariant=\"normal\">P</mml:mi><mml:mn>4</mml:mn><mml:mrow/><mml:msub><mml:mi> Solver </mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math><inline-graphic xlink:href=\"zhang-ieq4-3024215.gif\"/></alternatives></inline-formula>. We illustrate the efficacy of both planners on some selected case studies to show that the model checking-based algorithm is considerably more efficient than the quadratic-programming-based one.",
        "keywords": [
            "Planning",
            "Robots",
            "Markov processes",
            "Probabilistic logic",
            "Model checking",
            "Task analysis",
            "Software"
        ]
    },
    {
        "title": "Enabling Decision and Objective Space Exploration for Interactive Multi-Objective Refactoring.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3024814",
        "volume": "48",
        "abstract": "Due to the conflicting nature of quality measures, there are always multiple refactoring options to fix quality issues. Thus, interaction with developers is critical to inject their preferences. While several interactive techniques have been proposed, developers still need to examine large numbers of possible refactorings, which makes the interaction time-consuming. Furthermore, existing interactive tools are limited to the &#x201D;objective space&#x201D; to show developers the impacts of refactorings on quality attributes. However, the &#x201C;decision space&#x201D; is also important since developers may want to focus on specific code locations. In this paper, we propose an interactive approach that enables developers to pinpoint their preference simultaneously in the objective (quality metrics) and decision (code location) spaces. Developers may be interested in looking at refactoring strategies that can improve a specific quality attribute, such as extendibility (objective space), but such strategies may be related to different code locations (decision space). A plethora of solutions is generated at first using multi-objective search that tries to find the possible trade-offs between quality objectives. Then, an unsupervised learning algorithm clusters the trade-off solutions based on their quality metrics, and another clustering algorithm is applied within each cluster of the objective space to identify solutions related to different code locations. The objective and decision spaces can now be explored more efficiently by the developer, who can give feedback on a smaller number of solutions. This feedback is then used to generate constraints for the optimization process, to focus on the developer&#x0027;s regions of interest in both the decision and objective spaces. A manual validation of selected refactoring solutions by developers confirms that our approach outperforms state of the art refactoring techniques.",
        "keywords": [
            "Tools",
            "Clustering algorithms",
            "Measurement",
            "Search problems",
            "Manuals",
            "Software",
            "Space exploration"
        ]
    },
    {
        "title": "Real World Scrum A Grounded Theory of Variations in Practice.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3025317",
        "volume": "48",
        "abstract": "Scrum, the most popular agile method and project management framework, is widely reported to be used, adapted, misused, and abused in practice. However, not much is known about how Scrum actually works in practice, and critically, where, when, how and why it diverges from Scrum by the book. Through a Grounded Theory study involving semi-structured interviews of 45 participants from 30 companies and observations of five teams, we present our findings on how Scrum works in practice as compared to how it is presented in its formative books. We identify significant variations in these practices such as work breakdown, estimation, prioritization, assignment, the associated roles and artefacts, and discuss the underlying rationales driving the variations. Critically, we claim that not all variations are process misuse/abuse and propose a nuanced classification approach to understanding variations as <i>standard, necessary, contextual,</i> and <i>clear deviations</i> for successful Scrum use and adaptation.",
        "keywords": [
            "Scrum (Software development)",
            "Lead",
            "Software",
            "Project management",
            "Interviews",
            "Electric breakdown",
            "Estimation"
        ]
    },
    {
        "title": "Vuln4Real: A Methodology for Counting Actually Vulnerable Dependencies.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3025443",
        "volume": "48",
        "abstract": "Vulnerable dependencies are a known problem in today&#x2019;s free open-source software ecosystems because FOSS libraries are highly interconnected, and developers do not always update their dependencies. Our paper proposes Vuln4Real, the methodology for counting actually vulnerable dependencies, that addresses the over-inflation problem of academic and industrial approaches for reporting vulnerable dependencies in FOSS software, and therefore, caters to the needs of industrial practice for correct allocation of development and audit resources. To understand the industrial impact of a more precise methodology, we considered the 500 most popular FOSS Java libraries used by SAP in its own software. Our analysis included 25767 distinct library instances in Maven. We found that the proposed methodology has visible impacts on both ecosystem view and the individual library developer view of the situation of software dependencies: Vuln4Real significantly reduces the number of false alerts for deployed code (dependencies wrongly flagged as vulnerable), provides meaningful insights on the exposure to third-parties (and hence vulnerabilities) of a library, and automatically predicts when dependency maintenance starts lagging, so it may not receive updates for arising issues.",
        "keywords": [
            "Libraries",
            "Ecosystems",
            "Security",
            "Open source software",
            "Java",
            "Tools"
        ]
    },
    {
        "title": "Automated Generation of Consistent Graph Models With Multiplicity Reasoning.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3025732",
        "volume": "48",
        "abstract": "Advanced tools used in model-based systems engineering (MBSE) frequently represent their models as graphs. In order to test those tools, the automated generation of well-formed (or intentionally malformed) graph models is necessitated which is often carried out by solver-based model generation techniques. In many model generation scenarios, one needs more refined control over the generated unit tests to focus on the more relevant models. Type scopes allow to precisely define the required number of newly generated elements, thus one can avoid the generation of unrealistic and highly symmetric models having only a single type of elements. In this paper, we propose a 3-valued scoped partial modeling formalism, which innovatively extends partial graph models with predicate abstraction and counter abstraction. As a result, well-formedness constraints and multiplicity requirements can be evaluated in an approximated way on incomplete (unfinished) models by using advanced graph query engines with numerical solvers (e.g., IP or LP solvers). Based on the refinement of 3-valued scoped partial models, we propose an efficient model generation algorithm that generates models that are both well-formed and satisfy the scope requirements. We show that the proposed approach scales significantly better than existing SAT-solver techniques or the original graph solver without multiplicity reasoning. We illustrate our approach in a complex design-space exploration case study of collaborating satellites introduced by researchers at NASA JPL.",
        "keywords": [
            "Numerical models",
            "Tools",
            "Object oriented modeling",
            "Generators",
            "Unified modeling language",
            "Biological system modeling"
        ]
    },
    {
        "title": "Comparing Block-Based Programming Models for Two-Armed Robots.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3027255",
        "volume": "48",
        "abstract": "Modern industrial robots can work alongside human workers and coordinate with other robots. This means they can perform complex tasks, but doing so requires complex programming. Therefore, robots are typically programmed by experts, but there are not enough to meet the growing demand for robots. To reduce the need for experts, researchers have tried to make robot programming accessible to factory workers without programming experience. However, none of that previous work supports coordinating multiple robot arms that work on the same task. In this paper we present four block-based programming language designs that enable end-users to program two-armed robots. We analyze the benefits and trade-offs of each design on expressiveness and user cognition, and evaluate the designs based on a survey of 273 professional participants of whom 110 had no previous programming experience. We further present an interactive experiment based on a prototype implementation of the design we deem best. This experiment confirmed that novices can successfully use our prototype to complete realistic robotics tasks. This work contributes to making coordinated programming of robots accessible to end-users. It further explores how visual programming elements can make traditionally challenging programming tasks more beginner-friendly.",
        "keywords": [
            "Robot kinematics",
            "Programming profession",
            "Visualization",
            "Task analysis",
            "Manipulators"
        ]
    },
    {
        "title": "Experimental Evaluation of Test-Driven Development With Interns Working on a Real Industrial Project.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3027522",
        "volume": "48",
        "abstract": "Context: There is still little evidence on differences between Test-Driven Development and Test-Last Development, especially for real-world projects, so their impact on code/test quality is an ongoing research trend. An empirical comparison is presented, with 19 participants working on an industrial project developed for an energy market software company, implementing real-world requirements for one of the company&#x0027;s customers. <i>Objective:</i> Examine the impact of TDD and TLD on quality of the code and the tests. The aim is to evaluate if there is a significant difference in external code quality and test quality between these techniques. <i>Method:</i> The experiment is based on a randomized within-subjects block design, with participants working for three months on the same requirements using different techniques, changed from week to week, within three different competence blocks: Intermediate, Novice and Mixed. The resulting code was verified for process conformance. The participants developed only business logic and were separated from infrastructural concerns. A separate group of code repositories was used to work without unit tests, to verify that the requirements were not too easy for the participants. Also, it was analysed if there is any difference between the code created by shared efforts of developers with different competences and the code created by participants isolated in the competence blocks. The resulting implementations had LOC order of magnitude of 10k. <i>Results:</i> Statistically significant advantage of TDD in terms of external code quality (1.8 fewer bugs) and test quality (5 percentage points higher) than TLD. Additionally, TDD narrows the gap in code coverage between developers from different competence blocks. At the same time, TDD proved to have a considerable entry barrier and was hard to follow strictly, especially by Novices. Still, no significant difference w.r.t. code coverage has been observed between the Intermediate and the Novice developers - as opposed to TLD, which was easier to follow. Lastly, isolating the Intermediate developers from the Novices had significant impact on the code quality. <i>Conclusion:</i>TDD is a recommended technique for software projects with a long horizon or when it is critical to minimize the number of bugs and achieve high code coverage.",
        "keywords": [
            "Software",
            "Companies",
            "Testing",
            "Writing",
            "Programming",
            "Market research"
        ]
    },
    {
        "title": "Detecting Software Security Vulnerabilities Via Requirements Dependency Analysis.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3030745",
        "volume": "48",
        "abstract": "Cyber attacks targeting software applications have a tremendous impact on our daily life. For example, attackers have utilized vulnerabilities of web applications to steal and gain unauthorized use of sensitive data stored in these systems. Previous studies indicate that security testing is highly precise, and therefore is widely applied to validate individual security requirements. However, dependencies between security requirements may cause additional vulnerabilities. Manual dependency detection faces scalability challenges, e.g., a previous study shows that the pairwise dependency analysis of 40 requirements would take around 12 hours. In this article, we present a novel approach which integrates the interdependency among high-level security requirements, such as those documented in policies, regulations, and standards. We then use automated requirements tracing methods to identify product-level security requirements and their dependencies. Our manual analysis of HIPAA and FIPS 200 leads to the identification of five types of high-level security requirements dependencies, which further inform the automated tracing methods and guide the designs of system-level security tests. Experimental results on five projects in healthcare and education domains show the significant recall improvements at 81 percent. Our case study on a deployed production system uncovers four previously unknown vulnerabilities by using the detected requirements dependencies as test paths, demonstrating our approach&#x0027;s value in connecting requirements engineering with security testing.",
        "keywords": [
            "Security",
            "Software",
            "Testing",
            "Manuals",
            "Static analysis",
            "Regulation",
            "Scalability"
        ]
    },
    {
        "title": "Identifying Self-Admitted Technical Debts With Jitterbug: A Two-Step Approach.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3031401",
        "volume": "48",
        "abstract": "Keeping track of and managing Self-Admitted Technical Debts (SATDs) are important to maintaining a healthy software project. This requires much time and effort from human experts to identify the SATDs manually. The current automated solutions do not have satisfactory precision and recall in identifying SATDs to fully automate the process. To solve the above problems, we propose a two-step framework called <b>Jitterbug</b> for identifying SATDs. <b>Jitterbug</b> first identifies the &#x201C;easy to find&#x201D; SATDs automatically with close to 100 percent precision using a novel pattern recognition technique. Subsequently, machine learning techniques are applied to assist human experts in manually identifying the remaining &#x201C;hard to find&#x201D; SATDs with reduced human effort. Our simulation studies on ten software projects show that <b>Jitterbug</b> can identify SATDs more efficiently (with less human effort) than the prior state-of-the-art methods.",
        "keywords": [
            "Software",
            "Machine learning",
            "Pattern recognition",
            "Training",
            "Computer hacking",
            "Machine learning algorithms",
            "Estimation"
        ]
    },
    {
        "title": "Eyes on Code: A Study on Developers' Code Navigation Strategies.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3032064",
        "volume": "48",
        "abstract": "What code navigation strategies do developers use and what mechanisms do they employ to find relevant information? Do their strategies evolve over the course of longer tasks? Answers to these questions can provide insight to educators and software tool designers to support a wide variety of programmers as they tackle increasingly-complex software systems. However, little research to date has measured developers&#x2019; code navigation strategies in ecologically-valid settings, or analyzed how strategies progressed throughout a maintenance task. We propose a novel experimental design that more accurately represents the software maintenance process in terms of software complexity and IDE interactions. Using this framework, we conduct an eye-tracking study (n=36) of realistic bug-fixing tasks, dynamically and empirically identifying relevant code areas. We introduce a three-phase model to characterize developers&#x2019; navigation behavior supported by statistical variations in eye movements over time. We also propose quantifiable notion of &#x201C;thrashing&#x201D; with the code as a navigation activity. We find that thrashing is associated with lower effectiveness. Our results confirm that the relevance of various code elements changes over time, and that our proposed three-phase model is capable of capturing these significant changes. We discuss our findings and their implications for tool designers, educators, and the research community.",
        "keywords": [
            "Navigation",
            "Task analysis",
            "Computer bugs",
            "Tools",
            "Maintenance engineering",
            "Software",
            "Switches"
        ]
    },
    {
        "title": "Can Clean New Code Reduce Technical Debt Density?",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3032557",
        "volume": "48",
        "abstract": "While technical debt grows in absolute numbers as software systems evolve over time, the density of technical debt (technical debt divided by lines of code) is reduced in some cases. This can be explained by either the application of refactorings or the development of new artifacts with limited Technical Debt. In this paper we explore the second explanation, by investigating the relation between the amount of Technical Debt in new code and the evolution of Technical Debt in the system. To this end, we compare the Technical Debt Density of new code with existing code, and we investigate which of the three major types of code changes (additions, deletions and modifications) is primarily responsible for changes in the evolution of Technical Debt density. Furthermore, we study whether there is a relation between code quality practices and the &#x2018;cleanness&#x2019; of new code. To obtain the required data, we have performed a large-scale case study on twenty-seven open-source software projects by the Apache Software Foundation, analyzing 66,661 classes and 56,890 commits. The results suggest that writing &#x201C;clean&#x201D; (or at least &#x201C;cleaner&#x201D;) new code can be an efficient strategy for reducing Technical Debt Density, and thus preventing software decay over time. The findings also suggest that projects adopting an explicit policy for quality improvement, e.g., through discussions on code quality in board meetings, are associated with a higher frequency of cleaner new code commits. Therefore, we champion the establishment of processes that monitor the density of Technical Debt of new code to control the accumulation of Technical Debt in a software system.",
        "keywords": [
            "Open source software",
            "Writing",
            "Logic gates",
            "Market research",
            "Monitoring",
            "Maintenance engineering"
        ]
    },
    {
        "title": "A Survey on the Use of Computer Vision to Improve Software Engineering Tasks.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3032986",
        "volume": "48",
        "abstract": "Software engineering (SE) research has traditionally revolved around engineering the source code. However, novel approaches that analyze software through computer vision have been increasingly adopted in SE. These approaches allow analyzing the software from a different complementary perspective other than the source code, and they are used to either complement existing source code-based methods, or to overcome their limitations. The goal of this manuscript is to survey the use of computer vision techniques in SE with the aim of assessing their potential in advancing the field of SE research. We examined an extensive body of literature from top-tier SE venues, as well as venues from closely related fields (machine learning, computer vision, and human-computer interaction). Our inclusion criteria targeted papers applying computer vision techniques that address problems related to any area of SE. We collected an initial pool of 2,716 papers, from which we obtained 66 final relevant papers covering a variety of SE areas. We analyzed what computer vision techniques have been adopted or designed, for what reasons, how they are used, what benefits they provide, and how they are evaluated. Our findings highlight that visual approaches have been adopted in a wide variety of SE tasks, predominantly for effectively tackling software analysis and testing challenges in the web and mobile domains. The results also show a rapid growth trend of the use of computer vision techniques in SE research.",
        "keywords": [
            "Testing",
            "Visualization",
            "Software engineering",
            "Computer vision",
            "Software",
            "Task analysis",
            "Graphical user interfaces"
        ]
    },
    {
        "title": "Towards Security Threats of Deep Learning Systems: A Survey.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3034721",
        "volume": "48",
        "abstract": "Deep learning has gained tremendous success and great popularity in the past few years. However, deep learning systems are suffering several inherent weaknesses, which can threaten the security of learning models. Deep learning&#x2019;s wide use further magnifies the impact and consequences. To this end, lots of research has been conducted with the purpose of exhaustively identifying intrinsic weaknesses and subsequently proposing feasible mitigation. Yet few are clear about how these weaknesses are incurred and how effective these attack approaches are in assaulting deep learning. In order to unveil the security weaknesses and aid in the development of a robust deep learning system, we undertake an investigation on attacks towards deep learning, and analyze these attacks to conclude some findings in multiple views. In particular, we focus on four types of attacks associated with security threats of deep learning: model extraction attack, model inversion attack, poisoning attack and adversarial attack. For each type of attack, we construct its essential workflow as well as adversary capabilities and attack goals. Pivot metrics are devised for comparing the attack approaches, by which we perform quantitative and qualitative analyses. From the analysis, we have identified significant and indispensable factors in an attack vector, e.g., how to reduce queries to target models, what distance should be used for measuring perturbation. We shed light on 18 findings covering these approaches&#x2019; merits and demerits, success probability, deployment complexity and prospects. Moreover, we discuss other potential security weaknesses and possible mitigation which can inspire relevant research in this area.",
        "keywords": [
            "Deep learning",
            "Security",
            "Data models",
            "Privacy",
            "Predictive models",
            "Training data"
        ]
    },
    {
        "title": "How to Evaluate Solutions in Pareto-Based Search-Based Software Engineering: A Critical Review and Methodological Guidance.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3036108",
        "volume": "48",
        "abstract": "With modern requirements, there is an increasing tendency of considering multiple objectives/criteria simultaneously in many Software Engineering (SE) scenarios. Such a multi-objective optimization scenario comes with an important issue &#x2014; how to evaluate the outcome of optimization algorithms, which typically is a set of incomparable solutions (i.e., being Pareto nondominated to each other). This issue can be challenging for the SE community, particularly for practitioners of Search-Based SE (SBSE). On one hand, multi-objective optimization could still be relatively new to SE/SBSE researchers, who may not be able to identify the right evaluation methods for their problems. On the other hand, simply following the evaluation methods for general multi-objective optimization problems may not be appropriate for specific SBSE problems, especially when the problem nature or decision maker&#x2019;s preferences are explicitly/implicitly known. This has been well echoed in the literature by various inappropriate/inadequate selection and inaccurate/misleading use of evaluation methods. In this paper, we first carry out a systematic and critical review of quality evaluation for multi-objective optimization in SBSE. We survey 717 papers published between 2009 and 2019 from 36 venues in seven repositories, and select 95 prominent studies, through which we identify five important but overlooked issues in the area. We then conduct an in-depth analysis of quality evaluation indicators/methods and general situations in SBSE, which, together with the identified issues, enables us to codify a methodological guidance for selecting and using evaluation methods in different SBSE scenarios.",
        "keywords": [
            "Software engineering",
            "Pareto optimization",
            "Computer science",
            "Systematics",
            "Indexes",
            "Licenses"
        ]
    },
    {
        "title": "ATOM: Commit Message Generation Based on Abstract Syntax Tree and Hybrid Ranking.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3038681",
        "volume": "48",
        "abstract": "Commit messages record code changes (e.g., feature modifications and bug repairs) in natural language, and are useful for program comprehension. Due to the frequent updates of software and time cost, developers are generally unmotivated to write commit messages for code changes. Therefore, automating the message writing process is necessitated. Previous studies on commit message generation have been benefited from generation models or retrieval models, but the code structure of changed code, i.e., AST, which can be important for capturing code semantics, has not been explicitly involved. Moreover, although generation models have the advantages of synthesizing commit messages for new code changes, they are not easy to bridge the semantic gap between code and natural languages which could be mitigated by retrieval models. In this paper, we propose a novel commit message generation model, named ATOM, which explicitly incorporates the abstract syntax tree for representing code changes and integrates both retrieved and generated messages through hybrid ranking. Specifically, the hybrid ranking module can prioritize the most accurate message from both retrieved and generated messages regarding one code change. We evaluate the proposed model ATOM on our dataset crawled from 56 popular Java repositories. Experimental results demonstrate that ATOM increases the state-of-the-art models by 30.72 percent in terms of BLEU-4 (an accuracy measure that is widely used to evaluate text generation systems). Qualitative analysis also demonstrates the effectiveness of ATOM in generating accurate code commit messages.",
        "keywords": [
            "Syntactics",
            "Semantics",
            "Atomic measurements",
            "Hybrid power systems",
            "Benchmark testing",
            "Writing",
            "Java"
        ]
    },
    {
        "title": "Human Values in Software Engineering: Contrasting Case Studies of Practice.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3038802",
        "volume": "48",
        "abstract": "The growing diffusion of software in society and its influence on people demands from its creators that their work carefully considers human values such as transparency, social responsibility, and equality. But how do software practitioners address human values in software engineering practice? We interviewed 31 software practitioners from two organizations, each having a strong values framework, with the aim to understand: (a) practitioners&#x2019; perceptions of human values and their role in software engineering; (b) practices that practitioners use to address human values in software; and (c) challenges they face during this process. We report our findings from two contrasting case organizations on how practitioners &#x201C;engineer&#x201D; values in their unique organizational settings. We found evidence that organizational culture significantly contributes to how values are addressed in software. We summarize recommendations from the practitioners to support proactive engineering of values-conscious software.",
        "keywords": [
            "Software",
            "Software engineering",
            "Privacy",
            "Security",
            "Companies",
            "Human factors",
            "Artificial intelligence"
        ]
    },
    {
        "title": "An Empirical Study of Release Note Production and Usage in Practice.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3038881",
        "volume": "48",
        "abstract": "The release note is one of the most important software artifacts that serves as a communication bridge between development teams and users. Release notes contain a set of crucial information, such as descriptions of enhancements, improvements, potential issues, development, evolution, testing, and maintenance of projects throughout the whole development life cycle. A comprehensive understanding of the characteristics of release notes and how to best document one for different targeted users would be highly beneficial. However, the release note is often neglected and has not to date been systematically investigated by researchers. In this paper, we conducted a descriptive case study to investigate release note production and usage in practice. We first performed a large scale empirical study of 32,425 release notes in 1,000 GitHub projects to understand the characteristics of real-world release notes, and eight categories of information identified that are normally documented in release notes. We then conducted interviews with 15 professionals and an online survey with 314 respondents to investigate their opinions on release notes in practice. Our results show that both release note producers and users consider that well-formed release notes impact software activities (e.g., software evolution) positively. We summarised 27 statements about release notes grouped into eight topics based on participants&#x2019; opinions. Our study uncovers significant discrepancies between release note producers and users in perceiving release notes. Based on these findings, we provide a set of release note production and usage guidelines for practitioners and highlight future research directions.",
        "keywords": [
            "Software",
            "Production",
            "Software development management",
            "Stakeholders",
            "Feature extraction",
            "Testing",
            "Task analysis"
        ]
    },
    {
        "title": "How Software Developers Mitigate Their Errors When Developing Code.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3040554",
        "volume": "48",
        "abstract": "Code remains largely hand-made by humans and, as such, writing code is prone to error. Many previous studies have focused on the technical reasons for these errors and provided developers with increasingly sophisticated tools. Few studies have looked in detail at why code errors have been made from a human perspective. We use Human Error Theory to frame our exploratory study and use semi-structured interviews to uncover a preliminary understanding of the errors developers make while coding. We look particularly at the Skill-based (SB) errors reported by 27 professional software developers. We found that the complexity of the development environment is one of the most frequently reported reasons for errors. Maintaining concentration and focus on a particular task also underpins many developer errors. We found that developers struggle with effective mitigation strategies for their errors, reporting strategies largely based on improving their own willpower to concentrate better on coding tasks. We discuss how using Reason&#x2019;s Swiss Cheese model may help reduce errors during software development. This model ensures that layers of tool, process and management mitigation are in place to prevent developer errors from causing system failures.",
        "keywords": [
            "Software",
            "Task analysis",
            "Encoding",
            "Psychology",
            "Interviews",
            "Software engineering",
            "Footwear"
        ]
    },
    {
        "title": "Learning From Mistakes: Machine Learning Enhanced Human Expert Effort Estimates.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3040793",
        "volume": "48",
        "abstract": "In this paper, we introduce a novel approach to predictive modeling for software engineering, named Learning From Mistakes (LFM). The core idea underlying our proposal is to automatically learn from past estimation errors made by human experts, in order to predict the characteristics of their future misestimates, therefore resulting in improved future estimates. We show the feasibility of LFM by investigating whether it is possible to predict the type, severity and magnitude of errors made by human experts when estimating the development effort of software projects, and whether it is possible to use these predictions to enhance future estimations. To this end we conduct a thorough empirical study investigating 402 maintenance and new development industrial software projects. The results of our study reveal that the type, severity and magnitude of errors are all, indeed, predictable. Moreover, we find that by exploiting these predictions, we can obtain significantly better estimates than those provided by random guessing, human experts and traditional machine learners in 31 out of the 36 cases considered (86 percent), with large and very large effect sizes in the majority of these cases (81 percent). This empirical evidence opens the door to the development of techniques that use the power of machine learning, coupled with the observation that human errors are predictable, to support engineers in estimation tasks rather than replacing them with machine-provided estimates.",
        "keywords": [
            "Software",
            "Predictive models",
            "Companies",
            "Software engineering",
            "Estimation error",
            "Task analysis",
            "Software measurement"
        ]
    },
    {
        "title": "A Wizard of Oz Study Simulating API Usage Dialogues With a Virtual Assistant.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3040935",
        "volume": "48",
        "abstract": "Virtual Assistant technology is rapidly proliferating to improve productivity in a variety of tasks. While several virtual assistants for everyday tasks are well-known (e.g., Siri, Cortana, Alexa), assistants for specialty tasks such as software engineering are rarer. One key reason software engineering assistants are rare is that very few experimental datasets are available and suitable for training the AI that is the bedrock of current virtual assistants. In this paper, we present a set of Wizard of Oz experiments that we designed to build a dataset for creating a virtual assistant. Our target is a hypothetical virtual assistant for helping programmers use APIs. In our experiments, we recruited 30 professional programmers to complete programming tasks using two APIs. The programmers interacted with a simulated virtual assistant for help &#x2013; the programmers were not aware that the assistant was actually operated by human experts. We then annotated the dialogue acts in the corpus along four dimensions: illocutionary intent, API information type(s), backward-facing function, and traceability to specific API components. We observed a diverse range of interactions that will facilitate the development of dialogue strategies for virtual assistants for API usage.",
        "keywords": [
            "Task analysis",
            "Software engineering",
            "Training data",
            "Annotations",
            "Programming profession",
            "Documentation",
            "Navigation"
        ]
    },
    {
        "title": "Software Module Clustering: An In-Depth Literature Analysis.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3042553",
        "volume": "48",
        "abstract": "Software module clustering is an unsupervised learning method used to cluster software entities (e.g., classes, modules, or files) with similar features. The obtained clusters may be used to study, analyze, and understand the software entities&#x2019; structure and behavior. Implementing software module clustering with optimal results is challenging. Accordingly, researchers have addressed many aspects of software module clustering in the past decade. Thus, it is essential to present the research evidence that has been published in this area. In this study, 143 research papers from well-known literature databases that examined software module clustering were reviewed to extract useful data. The obtained data were then used to answer several research questions regarding state-of-the-art clustering approaches, applications of clustering in software engineering, clustering processes, clustering algorithms, and evaluation methods. Several research gaps and challenges in software module clustering are discussed in this paper to provide a useful reference for researchers in this field.",
        "keywords": [
            "Software",
            "Search problems",
            "Data mining",
            "Software engineering",
            "Software algorithms",
            "Clustering algorithms",
            "Systematics"
        ]
    },
    {
        "title": "XPro: A Model to Explain the Limited Adoption and Implementation of Experimentation in Software Startups.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3042610",
        "volume": "48",
        "abstract": "Software startups develop innovative, software-intensive products or services. Such innovativeness translates into uncertainty regarding a matching need for a product from potential customers, representing a possible determinant reason for startup failure. Research has shown that experimentation, an approach based on the use of experiments to guide several aspects of software development, could improve these companies&#x2019; success rate by fostering the evaluation of assumptions about customers&#x2019; needs before developing a full-fledged product. Nevertheless, software startups are not using experimentation as expected. In this study, we investigated the reasons behind such a mismatch between theory and practice. To achieve it, we performed a qualitative survey study of 106 failed software startups. We built the eXperimentation Progression model (XPro), demonstrating that the effective adoption and implementation of experimentation is a staged process: first, teams should be aware of experimentation, then they need to develop an intention to experiment, perform the experiments, analyze the results, and finally act based on the obtained learning. Based on the XPro model, we further identified 25 inhibitors that prevent a team from progressing along the stages properly. Our findings inform researchers of how to develop practices and techniques to improve experimentation adoption in software startups. Practitioners could learn various factors that could lead to their startup failure so they could take action to avoid them.",
        "keywords": [
            "Software",
            "Business",
            "Technological innovation",
            "Companies",
            "Inhibitors",
            "Uncertainty",
            "Testing"
        ]
    },
    {
        "title": "How do Practitioners Perceive the Relevance of Requirements Engineering Research?",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3042747",
        "volume": "48",
        "abstract": "<i>Context</i>: The relevance of Requirements Engineering (RE) research to practitioners is vital for a long-term dissemination of research results to everyday practice. Some authors have speculated about a mismatch between research and practice in the RE discipline. However, there is not much evidence to support or refute this perception. <i>Objective</i>: This article presents the results of a study aimed at gathering evidence from practitioners about their perception of the relevance of RE research and at understanding the factors that influence that perception. <i>Method</i>: We conducted a questionnaire-based survey of industry practitioners with expertise in RE. The participants rated the perceived relevance of 435 scientific papers presented at five top RE-related conferences. <i>Results</i>: The 153 participants provided a total of 2,164 ratings. The practitioners rated RE research as essential or worthwhile in a majority of cases. However, the percentage of non-positive ratings is still higher than we would like. Among the factors that affect the perception of relevance are the research&#x0027;s links to industry, the research method used, and respondents&#x2019; roles. The reasons for positive perceptions were primarily related to the relevance of the problem and the soundness of the solution, while the causes for negative perceptions were more varied. The respondents also provided suggestions for future research, including topics researchers have studied for decades, like elicitation or requirement quality criteria. <i>Conclusions</i>: The study is valuable for both researchers and practitioners. Researchers can use the reasons respondents gave for positive and negative perceptions and the suggested research topics to help make their research more appealing to practitioners and thus more prone to industry adoption. Practitioners can benefit from the overall view of contemporary RE research by learning about research topics that they may not be familiar with, and compare their perception with those of their colleagues to self-assess their positioning towards more academic research.",
        "keywords": [
            "Industries",
            "Protocols",
            "Requirements engineering",
            "Buildings",
            "Tools",
            "Testing",
            "Task analysis"
        ]
    },
    {
        "title": "Control and Discovery of Environment Behaviour.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3044532",
        "volume": "48",
        "abstract": "An important ability of self-adaptive systems is to be able to autonomously understand the environment in which they operate and use this knowledge to control the environment behaviour in such a way that system goals are achieved. How can this be achieved when the environment is unknown? Two phase solutions that require a full discovery of environment behaviour before computing a strategy that can guarantee the goals or report the non-existence of such a strategy (i.e., unrealisability) are impractical as the environment may exhibit adversarial behaviour to avoid full discovery. In this paper we formalise a control and discovery problem for reactive system environments. In our approach a strategy must be produced that will, for every environment, guarantee that unrealisablity will be correctly concluded or system goals will be achieved by controlling the environment behaviour. We present a solution applicable to environments characterisable as labeled transition systems (LTS). We use modal transition systems (MTS) to represent partial knowledge of environment behaviour, and rely on MTS controller synthesis to make exploration decisions. Each decision either contributes more knowledge about the environment&#x0027;s behaviour or contributes to achieving the system goals. We present an implementation restricted to GR(1) goals and show its viability.",
        "keywords": [
            "Protocols",
            "Learning automata",
            "Testing",
            "Sensors",
            "Process control",
            "Knowledge engineering",
            "Internet"
        ]
    },
    {
        "title": "Revisiting Test Impact Analysis in Continuous Testing From the Perspective of Code Dependencies.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3045914",
        "volume": "48",
        "abstract": "In continuous testing, developers execute automated test cases once or even several times per day to ensure the quality of the integrated code. Although continuous testing helps ensure the quality of the code and reduces maintenance effort, it also significantly increases test execution overhead. In this paper, we empirically evaluate the effectiveness of test impact analysis from the perspective of code dependencies in the continuous testing setting. We first applied test impact analysis to one year of software development history in 11 large-scale open-source systems. We found that even though the number of changed files is small in daily commits (median ranges from 3 to 28 files), around 50 percent or more of the test cases are still impacted and need to be executed. Motivated by our finding, we further studied the code dependencies between source code files and test cases, and among test cases. We found that 1) test cases often focus on testing the integrated behaviour of the systems and 15 percent of the test cases have dependencies with more than 20 source code files; 2) 18 percent of the test cases have dependencies with other test cases, and test case inheritance is the most common cause of test case dependencies; and 3) we documented four dependency-related test smells that we uncovered in our manual study. Our study provides the first step towards studying and understanding the effectiveness of test impact analysis in the continuous testing setting and provides insights on improving test design and execution.",
        "keywords": [
            "Testing",
            "Software",
            "Maintenance engineering",
            "Manuals",
            "Computer bugs",
            "Automation",
            "Tools"
        ]
    },
    {
        "title": "Sequential Model Optimization for Software Effort Estimation.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3047072",
        "volume": "48",
        "abstract": "Many methods have been proposed to estimate how much effort is required to build and maintain software. Much of that research tries to recommend a single method &#x2013; an approach that makes the dubious assumption that one method can handle the diversity of software project data. To address this drawback, we apply a configuration technique called &#x201C;ROME&#x201D; (Rapid Optimizing Methods for Estimation), which uses sequential model-based optimization (SMO) to find what configuration settings of effort estimation techniques work best for a particular data set. We test this method using data from 1161 traditional waterfall projects and 120 contemporary projects (from GitHub). In terms of magnitude of relative error and standardized accuracy, we find that ROME achieves better performance than the state-of-the-art methods for both traditional waterfall and contemporary projects. In addition, we conclude that we should not recommend <i>one</i> method for estimation. Rather, it is better to search through a wide range of different methods to find what works best for the local data. To the best of our knowledge, this is the largest effort estimation experiment yet attempted and the only one to test its methods on traditional waterfall and contemporary projects.",
        "keywords": [
            "Estimation",
            "Software",
            "Tools",
            "Optimization",
            "Data models",
            "Task analysis",
            "Mathematical model"
        ]
    },
    {
        "title": "Requirements of API Documentation: A Case Study into Computer Vision Services.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3047088",
        "volume": "48",
        "abstract": "Using cloud-based computer vision services is gaining traction, where developers access AI-powered components through familiar RESTful APIs, not needing to orchestrate large training and inference infrastructures or curate/label training datasets. However, while these APIs <i>seem</i> familiar to use, their non-deterministic run-time behaviour and evolution is not adequately communicated to developers. Therefore, improving these services&#x2019; API documentation is paramount&#x2014;more extensive documentation facilitates the development process of intelligent software. In a prior study, we extracted 34 API documentation artefacts from 21 seminal works, devising a taxonomy of five key requirements to produce quality API documentation. We extend this study in two ways. First, by surveying 104 developers of varying experience to understand what API documentation artefacts are of <i>most value</i> to practitioners. Second, identifying which of these highly-valued artefacts are or are not well-documented through a case study in the emerging computer vision service domain. We identify: (i) several gaps in the software engineering literature, where aspects of API documentation understanding is/is not extensively investigated; and (ii) where industry vendors (in contrast) document artefacts to better serve their end-developers. We provide a set of recommendations to enhance intelligent software documentation for both vendors and the wider research community.",
        "keywords": [
            "Documentation",
            "Taxonomy",
            "Computer vision",
            "Usability",
            "Guidelines",
            "Measurement",
            "Tools"
        ]
    },
    {
        "title": "The Relevance of Classic Fuzz Testing: Have We Solved This One?",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3047766",
        "volume": "48",
        "abstract": "As fuzz testing has passed its 30th anniversary, and in the face of the incredible progress in fuzz testing techniques and tools, the question arises if the classic, basic fuzz technique is still useful and applicable? In that tradition, we have updated the basic fuzz tools and testing scripts and applied them to a large collection of Unix utilities on Linux, FreeBSD, and MacOS. As before, our failure criteria was whether the program crashed or hung. We found that 9 crash or hang out of 74 utilities on Linux, 15 out of 78 utilities on FreeBSD, and 12 out of 76 utilities on MacOS. A total of 24 different utilities failed across the three platforms. We note that these failure rates are somewhat higher than our in previous 1995, 2000, and 2006 studies of the reliability of command line utilities. In the basic fuzz tradition, we debugged each failed utility and categorized the causes the failures. Classic categories of failures, such as pointer and array errors and not checking return codes, were still broadly present in the current results. In addition, we found a couple of new categories of failures appearing. We present examples of these failures to illustrate the programming practices that allowed them to happen. As a side note, we tested the limited number of utilities available in a modern programming language (Rust) and found them to be of no better reliability than the standard ones.",
        "keywords": [
            "Tools",
            "Testing",
            "Software",
            "Operating systems",
            "Fuzzing",
            "Software reliability",
            "Linux"
        ]
    },
    {
        "title": "Accelerating Continuous Integration by Caching Environments and Inferring Dependencies.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3048335",
        "volume": "48",
        "abstract": "To facilitate the rapid release cadence of modern software (on the order of weeks, days, or even hours), software development organizations invest in practices like Continuous Integration (CI), where each change submitted by developers is built (e.g., compiled, tested, linted) to detect problematic changes early. A fast and efficient build process is crucial to provide timely CI feedback to developers. If CI feedback is too slow, developers may switch contexts to other tasks, which is known to be a costly operation for knowledge workers. Thus, minimizing the build execution time for CI services is an important task. While recent work has made several important advances in the acceleration of CI builds, optimizations often depend upon explicitly defined build dependency graphs (e.g., make, Gradle, CloudBuild, Bazel). These hand-maintained graphs may be (a) underspecified, leading to incorrect build behaviour; or (b) overspecified, leading to missed acceleration opportunities. In this paper, we propose <small>Kotinos</small>&#x2014;a language-agnostic approach to infer data from which build acceleration decisions can be made without relying upon build specifications. After inferring this data, our approach accelerates CI builds by caching the build environment and skipping unaffected build steps. <small>Kotinos</small> is at the core of a commercial CI service with a growing customer base. To evaluate <small>Kotinos</small>, we mine 14,364 historical CI build records spanning three proprietary and seven open-source software projects. We find that: (1) at least 87.9 percent of the builds activate at least one <small>Kotinos</small> acceleration; and (2) 74 percent of accelerated builds achieve a speed-up of two-fold with respect to their non-accelerated counterparts. Moreover, (3) the benefits of <small>Kotinos</small> can also be replicated in open source software systems; and (4) <small>Kotinos</small> imposes minimal resource overhead (i.e., <inline-formula><tex-math notation=\"LaTeX\">$&#x003C;$</tex-math><alternatives><mml:math><mml:mo>&#x003C;</mml:mo></mml:math><inline-graphic xlink:href=\"gallaba-ieq1-3048335.gif\"/></alternatives></inline-formula> 1 percent median CPU usage, 2 MB &#x2013; 2.2 GB median memory usage, and 0.4 GB &#x2013; 5.2 GB median storage overhead) and does not compromise build outcomes. Our results suggest that migration to <small>Kotinos</small> yields substantial benefits with minimal investment of effort (e.g., no migration of build systems is necessary).",
        "keywords": [
            "Acceleration",
            "Software",
            "Tools",
            "Statistics",
            "Sociology",
            "Organizations",
            "Testing"
        ]
    },
    {
        "title": "A Method to Assess and Argue for Practical Significance in Software Engineering.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2020.3048991",
        "volume": "48",
        "abstract": "A key goal of empirical research in software engineering is to assess practical significance, which answers the question whether the observed effects of some compared treatments show a relevant difference in practice in realistic scenarios. Even though plenty of standard techniques exist to assess statistical significance, connecting it to practical significance is not straightforward or routinely done; indeed, only a few empirical studies in software engineering assess practical significance in a principled and systematic way. In this paper, we argue that Bayesian data analysis provides suitable tools to assess practical significance rigorously. We demonstrate our claims in a case study comparing different test techniques. The case study&#x0027;s data was previously analyzed (Afzal <i>et al.</i>, 2015) using standard techniques focusing on statistical significance. Here, we build a multilevel model of the same data, which we fit and validate using Bayesian techniques. Our method is to apply cumulative prospect theory on top of the statistical model to quantitatively connect our statistical analysis output to a practically meaningful context. This is then the basis both for assessing and arguing for practical significance. Our study demonstrates that Bayesian analysis provides a technically rigorous yet practical framework for empirical software engineering. A substantial side effect is that any uncertainty in the underlying data will be propagated through the statistical model, and its effects on practical significance are made clear. Thus, in combination with cumulative prospect theory, Bayesian analysis supports seamlessly assessing practical significance in an empirical software engineering context, thus potentially clarifying and extending the relevance of research for practitioners.",
        "keywords": [
            "Bayes methods",
            "Data models",
            "Software engineering",
            "Statistical analysis",
            "Analytical models",
            "Testing",
            "Decision making"
        ]
    },
    {
        "title": "The Effect of Feature Characteristics on the Performance of Feature Location Techniques.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2021.3049735",
        "volume": "48",
        "abstract": "<i>Feature Location (FL)</i> is a core software maintenance activity that aims to locate observable functionalities in the source code. Given its key role in software change, a vast array of Feature Location Techniques (FLTs) have been proposed but, as more and more FLTs are introduced, the <i>selection of an appropriate FLT</i> is an increasingly difficult problem. One consideration is the <i>characteristics of the features</i> being sought. For example, in the code associated with the feature, programmers may have named identifiers consistently, and with meaningful naming conventions, or not, and this may impact on the suitability of different FLTs. The suggestion that such characteristics matter has implicit support in the literature: An analysis of existing FLT empirical studies reveals that the system under study can often have a stronger impact on FLT performance than differing FLTs themselves. To understand this interaction between feature characteristics and FLTs better, this paper proposes <i>a suite of feature-characteristic metrics</i> that are postulated to control FLTs&#x2019; performance, holistically across FLTs and impacting on individual FLTs to different degrees. To evaluate the suite, a controlled experiment is performed, using 878 features, to probe the relationship between the metrics and the performance of four FTL techniques: three commonly-used techniques and one state-of-the-art technique. The evaluation is performed using four commonly used evaluation measures and extended by employing 41 other established source-code metrics as extraneous variables. Results of the empirical evaluation suggest that the feature-metric suite presented impacts FLT performance holistically, and impacts different FLTs to different degrees. Thus, this paper moves towards the more standard selection of appropriate FLTs, with respect to the prominent feature characteristics in the software systems under study, and more rigorous consideration of the features selected to compare FLTs.",
        "keywords": [
            "Measurement",
            "Feature extraction",
            "Software maintenance",
            "Task analysis",
            "Software systems",
            "Gold",
            "Standards"
        ]
    },
    {
        "title": "Within-Project Defect Prediction of Infrastructure-as-Code Using Product and Process Metrics.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2021.3051492",
        "volume": "48",
        "abstract": "Infrastructure-as-code (IaC) is the DevOps practice enabling management and provisioning of infrastructure through the definition of machine-readable files, hereinafter referred to as <i>IaC scripts</i>. Similarly to other source code artefacts, these files may contain defects that can preclude their correct functioning. In this paper, we aim at assessing the role of <i>product</i> and <i>process</i> metrics when predicting defective IaC scripts. We propose a fully integrated machine-learning framework for IaC Defect Prediction, that allows for repository crawling, metrics collection, model building, and evaluation. To evaluate it, we analyzed 104 projects and employed five machine-learning classifiers to compare their performance in flagging suspicious defective IaC scripts. The key results of the study report <small>Random Forest</small> as the best-performing model, with a median AUC-PR of 0.93 and MCC of 0.80. Furthermore, at least for the collected projects, product metrics identify defective IaC scripts more accurately than process metrics. Our findings put a baseline for investigating IaC Defect Prediction and the relationship between the product and process metrics, and IaC scripts&#x2019; quality.",
        "keywords": [
            "Measurement",
            "Software",
            "Predictive models",
            "Machine learning",
            "Radon",
            "Cloud computing",
            "Task analysis"
        ]
    },
    {
        "title": "The Effects of Human Aspects on the Requirements Engineering Process: A Systematic Literature Review.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2021.3051898",
        "volume": "48",
        "abstract": "Requirements Engineering (RE) requires the collaboration of various roles in SE, such as requirements engineers, stakeholders and other developers, and it is thus a very highly human dependent process in software engineering (SE). Identifying how <i>&#x201C;human aspects&#x201D;</i> &#x2013; such as personality, motivation, emotions, communication, gender, culture and geographic distribution &#x2013; might impact on the RE process would assist us in better supporting successful RE. The main objective of this paper is to systematically review primary studies that have investigated the effects of various human aspects on the RE process. We wanted to identify if any critical human aspects have been found, and what might be the relationships between different human aspects impacting the RE process. A systematic literature review (SLR) was conducted and identified 474 initial primary research studies. These were eventually filtered down to 74 relevant, high-quality primary studies. No primary study to date was found to focus on identifying what are the most influential human aspects on the RE process. Among the studied human aspects, the effects of communication have been considered in many studies of RE. Other human aspects such as personality, motivation and gender have mainly been investigated to date in relation to more general SE studies that include RE as one phase. Findings show that studying more than one human aspect together is beneficial, as this reveals relationships between various human aspects and how they together impact the RE process. However, the majority of these studied combinations of human aspects are unique. From 56.8 percent of studies that identified the effects of human aspects on RE, 40.5 percent identified the positive impact, 30.9 percent negative, 26.2 percent identified both impacts whereas 2.3 percent mentioned that there was no impact. This implies that a variety of human aspects positively or negatively affects the RE process and a well-defined theoretical analysis on the effects of different human aspects on RE remains to be defined and practically evaluated. The findings of this SLR help researchers who are investigating the impact of various human aspects on the RE process by identifying well-studied research areas, and highlight new areas that should be focused on in future research.",
        "keywords": [
            "Software",
            "Systematics",
            "Software engineering",
            "Bibliographies",
            "Stakeholders",
            "Meteorology",
            "Taxonomy"
        ]
    },
    {
        "title": "A Survey on the Adoption of Patterns for Engineering Software for the Cloud.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2021.3052177",
        "volume": "48",
        "abstract": "This work takes as a starting point a collection of patterns for engineering software for the cloud and tries to find how they are regarded and adopted by professionals. Existing literature assesses the adoption of cloud computing with a focus on business and technological aspects and falls short in grasping a holistic view of the underlying approaches. Other authors delve into how independent patterns can be discovered (mined) and verified, but do not provide insights on their adoption. We investigate (1) the relevance of the patterns for professional software developers, (2) the extent to which product and company characteristics influence their adoption, and (3) how adopting some patterns might correlate with the likelihood of adopting others. For this purpose, we survey practitioners using an online questionnaire (n = 102). Among other findings, we conclude that most companies use these patterns, with the overwhelming majority (97 percent) using at least one. We observe that the mean pattern adoption tends to increase as companies mature, namely when varying the <i>product operation complexity</i>, <i>active monthly users</i>, and <i>company size</i>. Finally, we search for correlations in the adoption of specific patterns and attempt to infer causation, providing further clues on how some practices depend or influence the adoption of others. We conclude that the adoption of some practices correlates with specific company and product characteristics, and find relationships between the patterns that were not covered by the original pattern language and which might deserve further investigation.",
        "keywords": [
            "Cloud computing",
            "Software",
            "Companies",
            "Monitoring",
            "Industries",
            "Containers",
            "Scalability"
        ]
    },
    {
        "title": "Enhancement of Mutation Testing via Fuzzy Clustering and Multi-Population Genetic Algorithm.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2021.3052987",
        "volume": "48",
        "abstract": "Mutation testing, a fundamental software testing technique, which is a typical way to evaluate the adequacy of a test suite. In mutation testing, a set of mutants are generated by seeding the different classes of faults into a program under test. Test data shall be generated in the way that as many mutants can be killed as possible. Thanks to numerous tools to implement mutation testing for different languages, a huge amount of mutants are normally generated even for small-sized programs. However, a large number of mutants not only leads to a high cost of mutation testing, but also make the corresponding test data generation a non-trivial task. In this paper, we make use of intelligent technologies to improve the effectiveness and efficiency of mutation testing from two perspectives. A machine learning technique, namely fuzzy clustering, is applied to categorize mutants into different clusters. Then, a multi-population genetic algorithm via individual sharing is employed to generate test data for killing the mutants in different clusters in parallel when the problem of test data generation as an optimization one. A comprehensive framework, termed as <inline-formula><tex-math notation=\"LaTeX\">$\\mathbf {FUZGENMUT}$</tex-math><alternatives><mml:math><mml:mi mathvariant=\"bold\">FUZGENMUT</mml:mi></mml:math><inline-graphic xlink:href=\"gong-ieq1-3052987.gif\"/></alternatives></inline-formula>, is thus developed to implement the proposed techniques. The experiments based on nine programs of various sizes show that fuzzy clustering can help to reduce the cost of mutation testing effectively, and that the multi-population genetic algorithm improves the efficiency of test data generation while delivering the high mutant-killing capability. The results clearly indicate that the huge potential of using intelligent technologies to enhance the efficacy and thus the practicality of mutation testing.",
        "keywords": [
            "Testing",
            "Genetic algorithms",
            "Sorting",
            "Clustering algorithms",
            "Syntactics",
            "Statistics",
            "Sociology"
        ]
    },
    {
        "title": "Boosting API Recommendation With Implicit Feedback.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2021.3053111",
        "volume": "48",
        "abstract": "Developers often need to use appropriate APIs to program efficiently, but it is usually a difficult task to identify the exact one they need from a vast list of candidates. To ease the burden, a multitude of API recommendation approaches have been proposed. However, most of the currently available API recommenders do not support the effective integration of user feedback into the recommendation loop. In this paper, we propose a framework, BRAID (<b>B</b>oosting <b>R</b>ecommend<b>A</b>tion with <b>I</b>mplicit Fee<b>D</b>back), which leverages learning-to-rank and active learning techniques to boost recommendation performance. By exploiting user feedback information, we train a learning-to-rank model to re-rank the recommendation results. In addition, we speed up the feedback learning process with active learning. Existing query-based API recommendation approaches can be plugged into BRAID. We select three state-of-the-art API recommendation approaches as baselines to demonstrate the performance enhancement of BRAID measured by Hit@k (Top-k), MAP, and MRR. Empirical experiments show that, with acceptable overheads, the recommendation performance improves steadily and substantially with the increasing percentage of feedback data, comparing with the baselines.",
        "keywords": [
            "Task analysis",
            "Software",
            "Feature extraction",
            "Training",
            "Programming",
            "History",
            "Engines"
        ]
    },
    {
        "title": "Are You Still Working on This? An Empirical Study on Pull Request Abandonment.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2022,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2021.3053403",
        "volume": "48",
        "abstract": "The great success of numerous community-based open source software (OSS) is based on volunteers continuously submitting contributions, but ensuring sustainability is a persistent challenge in OSS communities. Although the motivations behind and barriers to OSS contributors&#x2019; joining and retention have been extensively studied, the impacts of, reasons for and solutions to contribution abandonment at the individual level have not been well studied, especially for pull-based development. To bridge this gap, we present an empirical study on pull request abandonment based on a sizable dataset. We manually examine 321 abandoned pull requests on GitHub and then quantify the manual observations by surveying 710 OSS developers. We find that while the lack of integrators&#x2019; responsiveness and the lack of contributors&#x2019; time and interest remain the main reasons that deter contributors from participation, limitations during the processes of patch updating and consensus reaching can also cause abandonment. We also show the significant impacts of pull request abandonment on project management and maintenance. Moreover, we elucidate the strategies used by project integrators to cope with abandoned pull requests and highlight the need for a practical handover mechanism. We discuss the actionable suggestions and implications for OSS practitioners and tool builders, which can help to upgrade the infrastructure and optimize the mechanisms of OSS communities.",
        "keywords": [
            "Tools",
            "Collaboration",
            "Sustainable development",
            "Open source software",
            "Manuals",
            "Maintenance engineering",
            "Computer bugs"
        ]
    }
]