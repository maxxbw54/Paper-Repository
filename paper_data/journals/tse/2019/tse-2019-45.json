[
    {
        "title": "Editorial: State of the Journal.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2885501",
        "volume": "45",
        "abstract": "Presents the introductory editorial for this issue of the publication.",
        "keywords": null
    },
    {
        "title": "A Rigorous Framework for Specification, Analysis and Enforcement of Access Control Policies.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2765640",
        "volume": "45",
        "abstract": "Access control systems are widely used means for the protection of computing systems. They are defined in terms of access control policies regulating the access to system resources. In this paper, we introduce a formally-defined, fully-implemented framework for specification, analysis and enforcement of attribute-based access control policies. The framework rests on FACPL, a language with a compact, yet expressive, syntax for specification of real-world access control policies and with a rigorously defined denotational semantics. The framework enables the automated verification of properties regarding both the authorisations enforced by single policies and the relationships among multiple policies. Effectiveness and performance of the analysis rely on a semantic-preserving representation of FACPL policies in terms of SMT formulae and on the use of efficient SMT solvers. Our analysis approach explicitly addresses some crucial aspects of policy evaluation, such as missing attributes, erroneous values and obligations, which are instead overlooked in other proposals. The framework is supported by Java-based tools, among which an Eclipse-based IDE offering a tailored development and analysis environment for FACPL policies and a Java library for policy enforcement. We illustrate the framework and its formal ingredients by means of an e-Health case study, while its effectiveness is assessed by means of performance stress tests and experiments on a well-established benchmark.",
        "keywords": [
            "Semantics",
            "Authorization",
            "Tools",
            "Syntactics",
            "Proposals",
            "Java"
        ]
    },
    {
        "title": "Automatic Software Repair: A Survey.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2755013",
        "volume": "45",
        "abstract": "Despite their growing complexity and increasing size, modern software applications must satisfy strict release requirements that impose short bug fixing and maintenance cycles, putting significant pressure on developers who are responsible for timely producing high-quality software. To reduce developers workload, repairing and healing techniques have been extensively investigated as solutions for efficiently repairing and maintaining software in the last few years. In particular, repairing solutions have been able to automatically produce useful fixes for several classes of bugs that might be present in software programs. A range of algorithms, techniques, and heuristics have been integrated, experimented, and studied, producing a heterogeneous and articulated research framework where automatic repair techniques are proliferating. This paper organizes the knowledge in the area by surveying a body of 108 papers about automatic software repair techniques, illustrating the algorithms and the approaches, comparing them on representative examples, and discussing the open challenges and the empirical evidence reported so far.",
        "keywords": [
            "Software",
            "Maintenance engineering",
            "Debugging",
            "Computer bugs",
            "Software algorithms",
            "Fault diagnosis",
            "Conferences"
        ]
    },
    {
        "title": "Listening to the Crowd for the Release Planning of Mobile Apps.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2759112",
        "volume": "45",
        "abstract": "The market for mobile apps is getting bigger and bigger, and it is expected to be worth over 100 Billion dollars in 2020. To have a chance to succeed in such a competitive environment, developers need to build and maintain high-quality apps, continuously astonishing their users with the coolest new features. Mobile app marketplaces allow users to release reviews. Despite reviews are aimed at recommending apps among users, they also contain precious information for developers, reporting bugs and suggesting new features. To exploit such a source of information, developers are supposed to manually read user reviews, something not doable when hundreds of them are collected per day. To help developers dealing with such a task, we developed CLAP (Crowd Listener for releAse Planning), a web application able to (i) categorize user reviews based on the information they carry out, (ii) cluster together related reviews, and (iii) prioritize the clusters of reviews to be implemented when planning the subsequent app release. We evaluated all the steps behind CLAP, showing its high accuracy in categorizing and clustering reviews and the meaningfulness of the recommended prioritizations. Also, given the availability of CLAP as a working tool, we assessed its applicability in industrial environments.",
        "keywords": [
            "Data mining",
            "Internet",
            "Mobile applications",
            "Google",
            "Mobile communication",
            "Pattern clustering"
        ]
    },
    {
        "title": "What Makes a Great Manager of Software Engineers?",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2768368",
        "volume": "45",
        "abstract": "Having great managers is as critical to success as having a good team or organization. In general, a great manager is seen as fuelling the team they manage, enabling it to use its full potential. Though software engineering research studies factors that may affect the performance and productivity of software engineers and teams (like tools and skills), it has overlooked the software engineering manager. The software industry's growth and change in the last decades is creating a need for a domain-specific view of management. On the one hand, experts are questioning how the abundant work in management applies to software engineering. On the other hand, practitioners are looking to researchers for evidence-based guidance on how to manage software teams. We conducted a mixed methods empirical study of software engineering management at Microsoft to investigate what manager attributes developers and engineering managers perceive important and why. We present a conceptual framework of manager attributes, and find that technical skills are not the sign of greatness for an engineering manager. Through statistical analysis we identify how engineers and managers relate in their views, and how software engineering differs from other knowledge work groups in its perceptions about what makes great managers. We present strategies for putting the attributes to use, discuss implications for research and practice, and offer avenues for further work.",
        "keywords": [
            "Software engineering",
            "Software",
            "Organizations",
            "Knowledge engineering",
            "Productivity",
            "Interviews",
            "Psychology"
        ]
    },
    {
        "title": "A Systematic Literature Review and Meta-Analysis on Cross Project Defect Prediction.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2770124",
        "volume": "45",
        "abstract": "Background: Cross project defect prediction (CPDP) recently gained considerable attention, yet there are no systematic efforts to analyse existing empirical evidence. Objective: To synthesise literature to understand the state-of-the-art in CPDP with respect to metrics, models, data approaches, datasets and associated performances. Further, we aim to assess the performance of CPDP versus within project DP models. Method: We conducted a systematic literature review. Results from primary studies are synthesised (thematic, meta-analysis) to answer research questions. Results: We identified 30 primary studies passing quality assessment. Performance measures, except precision, vary with the choice of metrics. Recall, precision, f-measure, and AUC are the most common measures. Models based on Nearest-Neighbour and Decision Tree tend to perform well in CPDP, whereas the popular naïve Bayes yields average performance. Performance of ensembles varies greatly across f-measure and AUC. Data approaches address CPDP challenges using row/column processing, which improve CPDP in terms of recall at the cost of precision. This is observed in multiple occasions including the meta-analysis of CPDP versus WPDP. NASA and Jureczko datasets seem to favour CPDP over WPDP more frequently. Conclusion: CPDP is still a challenge and requires more research before trustworthy applications can take place. We provide guidelines for further research.",
        "keywords": [
            "Object oriented modeling",
            "Systematics",
            "Measurement",
            "Bibliographies",
            "Predictive models",
            "Context modeling",
            "Data models"
        ]
    },
    {
        "title": "Automated Refactoring of OCL Constraints with Search.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2774829",
        "volume": "45",
        "abstract": "Object Constraint Language (OCL) constraints are typically used to provide precise semantics to models developed with the Unified Modeling Language (UML). When OCL constraints evolve regularly, it is essential that they are easy to understand and maintain. For instance, in cancer registries, to ensure the quality of cancer data, more than one thousand medical rules are defined and evolve regularly. Such rules can be specified with OCL. It is, therefore, important to ensure the understandability and maintainability of medical rules specified with OCL. To tackle such a challenge, we propose an automated search-based OCL constraint refactoring approach (SBORA) by defining and applying four semantics-preserving refactoring operators (i.e., Context Change, Swap, Split and Merge) and three OCL quality metrics (Complexity, Coupling, and Cohesion) to measure the understandability and maintainability of OCL constraints. We evaluate SBORA along with six commonly used multi-objective search algorithms (e.g., Indicator-Based Evolutionary Algorithm (IBEA)) by employing four case studies from different domains: healthcare (i.e., cancer registry system from Cancer Registry of Norway (CRN)), Oil&Gas (i.e., subsea production systems), warehouse (i.e., handling systems), and an open source case study named SEPA. Results show: 1) IBEA achieves the best performance among all the search algorithms and 2) the refactoring approach along with IBEA can manage to reduce on average 29.25 percent Complexity and 39 percent Coupling and improve 47.75 percent Cohesion, as compared to the original OCL constraint set from CRN. To further test the performance of SBORA, we also applied it to refactor an OCL constraint set specified on the UML 2.3 metamodel and we obtained positive results. Furthermore, we conducted a controlled experiment with 96 subjects and results show that the understandability and maintainability of the original constraint set can be improved significantly from the perspectives of the 96 participants of the controlled experiment.",
        "keywords": [
            "Cancer",
            "Unified modeling language",
            "Measurement",
            "Couplings",
            "Complexity theory",
            "Semantics",
            "Computational modeling"
        ]
    },
    {
        "title": "Gray Computing: A Framework for Computing with Background JavaScript Tasks.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2772812",
        "volume": "45",
        "abstract": "Website visitors are performing increasingly complex computational work on the websites' behalf, such as validating forms, rendering animations, and producing data visualizations. In this article, we explore the possibility of increasing the work offloaded to web visitors' browsers. The idle computing cycles of web visitors can be turned into a large-scale distributed data processing engine, which we term gray computing. Past research has looked primarily at either volunteer computing with specialized clients or browser-based volunteer computing where the visitors keep their browsers open to a single web page for a long period of time. This article provides a comprehensive analysis of the architecture, performance, security, cost effectiveness, user experience, and other issues of gray computing distributed data processing engines with heterogeneous computing power, non-uniform page view times, and high computing pool volatility. Several real-world applications are examined and gray computing is shown to be cost effective for a number of complex tasks ranging from computer vision to bioinformatics to cryptology.",
        "keywords": [
            "Distributed processing",
            "Browsers",
            "Data processing",
            "Web pages",
            "Cloud computing"
        ]
    },
    {
        "title": "Toward a Smell-Aware Bug Prediction Model.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2770122",
        "volume": "45",
        "abstract": "Code smells are symptoms of poor design and implementation choices. Previous studies empirically assessed the impact of smells on code quality and clearly indicate their negative impact on maintainability, including a higher bug-proneness of components affected by code smells. In this paper, we capture previous findings on bug-proneness to build a specialized bug prediction model for smelly classes. Specifically, we evaluate the contribution of a measure of the severity of code smells (i.e., code smell intensity) by adding it to existing bug prediction models based on both product and process metrics, and comparing the results of the new model against the baseline models. Results indicate that the accuracy of a bug prediction model increases by adding the code smell intensity as predictor. We also compare the results achieved by the proposed model with the ones of an alternative technique which considers metrics about the history of code smells in files, finding that our model works generally better. However, we observed interesting complementarities between the set of buggy and smelly classes correctly classified by the two models. By evaluating the actual information gain provided by the intensity index with respect to the other metrics in the model, we found that the intensity index is a relevant feature for both product and process metrics-based models. At the same time, the metric counting the average number of code smells in previous versions of a class considered by the alternative model is also able to reduce the entropy of the model. On the basis of this result, we devise and evaluate a smell-aware combined bug prediction model that included product, process, and smell-related features. We demonstrate how such model classifies bug-prone code components with an F-Measure at least 13 percent higher than the existing state-of-the-art models.",
        "keywords": [
            "Computer bugs",
            "Measurement",
            "Predictive models",
            "Indexes",
            "Software",
            "Complexity theory",
            "Entropy"
        ]
    },
    {
        "title": "Automatically Exploring Tradeoffs Between Software Output Fidelity and Energy Costs.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2775634",
        "volume": "45",
        "abstract": "Data centers account for a significant fraction of global energy consumption and represent a growing business cost. Most current approaches to reducing energy use in data centers treat it as a hardware, compiler, or scheduling problem. This article focuses instead on the software level, showing how to reduce the energy used by programs when they execute. By combining insights from search-based software engineering, mutational robustness, profile-guided optimization, and approximate computing, the Producing Green Applications Using Genetic Exploration (PowerGAUGE) algorithm finds variants of individual programs that use less energy than the original. We apply hardware, software, and statistical techniques to manage the complexity of accurately assigning physical energy measurements to particular processes. In addition, our approach allows, but does not require, relaxing output quality requirements to achieve greater non-functional improvements. PowerGAUGE optimizations are validated using physical performance measurements. Experimental results on PARSEC benchmarks and two larger programs show average energy reductions of 14% when requiring the preservation of original output quality and 41% when allowing for human-acceptable levels of error.",
        "keywords": [
            "Energy consumption",
            "Optimization",
            "Hardware",
            "Genetic algorithms",
            "Software",
            "Energy measurement"
        ]
    },
    {
        "title": "Competition-Based Crowdsourcing Software Development: A Multi-Method Study from a Customer Perspective.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2774297",
        "volume": "45",
        "abstract": "Crowdsourcing is emerging as an alternative outsourcing strategy which is gaining increasing attention in the software engineering community. However, crowdsourcing software development involves complex tasks which differ significantly from the micro-tasks that can be found on crowdsourcing platforms such as Amazon Mechanical Turk which are much shorter in duration, are typically very simple, and do not involve any task interdependencies. To achieve the potential benefits of crowdsourcing in the software development context, companies need to understand how this strategy works, and what factors might affect crowd participation. We present a multi-method qualitative and quantitative theory-building research study. First, we derive a set of key concerns from the crowdsourcing literature as an initial analytical framework for an exploratory case study in a Fortune 500 company. We complement the case study findings with an analysis of 13,602 crowdsourcing competitions over a ten-year period on the very popular Topcoder crowdsourcing platform. Drawing from our empirical findings and the crowdsourcing literature, we propose a theoretical model of crowd interest and actual participation in crowdsourcing competitions. We evaluate this model using Structural Equation Modeling. Among the findings are that the level of prize and duration of competitions do not significantly increase crowd interest in competitions.",
        "keywords": [
            "Crowdsourcing",
            "Software",
            "Outsourcing",
            "Mathematical model",
            "Companies"
        ]
    },
    {
        "title": "Developer Testing in the IDE: Patterns, Beliefs, and Behavior.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2776152",
        "volume": "45",
        "abstract": "Software testing is one of the key activities to achieve software quality in practice. Despite its importance, however, we have a remarkable lack of knowledge on how developers test in real-world projects. In this paper, we report on a large-scale field study with 2,443 software engineers whose development activities we closely monitored over 2.5 years in four integrated development environments (IDEs). Our findings, which largely generalized across the studied IDEs and programming languages Java and C#, question several commonly shared assumptions and beliefs about developer testing: half of the developers in our study do not test; developers rarely run their tests in the IDE; most programming sessions end without any test execution; only once they start testing, do they do it extensively; a quarter of test cases is responsible for three quarters of all test failures; 12 percent of tests show flaky behavior; Test-Driven Development (TDD) is not widely practiced; and software developers only spend a quarter of their time engineering tests, whereas they think they test half of their time. We summarize these practices of loosely guiding one's development efforts with the help of testing in an initial summary on Test-Guided Development (TGD), a behavior we argue to be closer to the development reality of most developers than TDD.",
        "keywords": [
            "Testing",
            "Software",
            "Visualization",
            "Servers",
            "Java",
            "Androids",
            "Humanoid robots"
        ]
    },
    {
        "title": "Integrating Technical Debt Management and Software Quality Management Processes: A Normative Framework and Field Tests.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2774832",
        "volume": "45",
        "abstract": "Despite the increasing awareness of the importance of managing technical debt in software product development, systematic processes for implementing technical debt management in software production have not been readily available. In this paper we report on the development and field tests of a normative process framework that systematically incorporates steps for managing technical debt in commercial software production. The framework integrates processes required for technical debt management with existing software quality management processes prescribed by the project management body of knowledge (PMBOK), and it contributes to the further development of software-specific extensions to the PMBOK. We partnered with three commercial software product development organizations to implement the framework in real-world software production settings. All three organizations, irrespective of their varying software process maturity levels, were able to adopt the proposed framework and integrate the prescribed technical debt management processes with their existing software quality management processes. Our longitudinal observations and case-study interviews indicate that the organizations were able to accrue economic benefits from the adoption and use of the integrated framework.",
        "keywords": [
            "Software quality",
            "Business",
            "Economics",
            "Product development",
            "Tools",
            "Systematics"
        ]
    },
    {
        "title": "MSeer - An Advanced Technique for Locating Multiple Bugs in Parallel.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2776912",
        "volume": "45",
        "abstract": "In practice, a program may contain multiple bugs. The simultaneous presence of these bugs may deteriorate the effectiveness of existing fault-localization techniques to locate program bugs. While it is acceptable to use all failed and successful tests to identify suspicious code for programs with exactly one bug, it is not appropriate to use the same approach for programs with multiple bugs because the due-to relationship between failed tests and underlying bugs cannot be easily identified. One solution is to generate fault-focused clusters by grouping failed tests caused by the same bug into the same clusters. We propose MSeer-an advanced fault localization technique for locating multiple bugs in parallel. Our major contributions include the use of (1) a revised Kendall tau distance to measure the distance between two failed tests, (2) an innovative approach to simultaneously estimate the number of clusters and assign initial medoids to these clusters, and (3) an improved K-medoids clustering algorithm to better identify the due-to relationship between failed tests and their corresponding bugs. Case studies on 840 multiple-bug versions of seven programs suggest that MSeer performs better in terms of effectiveness and efficiency than two other techniques for locating multiple bugs in parallel.",
        "keywords": [
            "Computer bugs",
            "Clustering algorithms",
            "Measurement",
            "Software",
            "Indexes",
            "Debugging",
            "Runtime"
        ]
    },
    {
        "title": "An Empirical Study on API Usages.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2782280",
        "volume": "45",
        "abstract": "API libraries provide thousands of APIs, and are essential in daily programming tasks. To understand their usages, it has long been a hot research topic to mine specifications that formally define legal usages for APIs. Furthermore, researchers are working on many other research topics on APIs. Although the research on APIs is intensively studied, many fundamental questions on APIs are still open. For example, the answers to open questions, such as which format can naturally define API usages and in which case, are still largely unknown. We notice that many such open questions are not concerned with concrete usages of specific APIs, but usages that describe how to use different types of APIs. To explore these questions, in this paper, we conduct an empirical study on API usages, with an emphasis on how different types of APIs are used. Our empirical results lead to nine findings on API usages. For example, we find that single-type usages are mostly strict orders, but multi-type usages are more complicated since they include both strict orders and partial orders. Based on these findings, for the research on APIs, we provide our suggestions on the four key aspects such as the challenges, the importance of different API elements, usage patterns, and pitfalls in designing evaluations. Furthermore, we interpret our findings, and present our insights on data sources, extraction techniques, mining techniques, and formats of specifications for the research of mining specifications.",
        "keywords": [
            "Libraries",
            "Data mining",
            "Programming",
            "Law",
            "Tools",
            "Concrete"
        ]
    },
    {
        "title": "Automatic Generation of Tests to Exploit XML Injection Vulnerabilities in Web Applications.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2778711",
        "volume": "45",
        "abstract": "Modern enterprise systems can be composed of many web services (e.g., SOAP and RESTful). Users of such systems might not have direct access to those services, and rather interact with them through a single-entry point which provides a GUI (e.g., a web page or a mobile app). Although the interactions with such entry point might be secure, a hacker could trick such systems to send malicious inputs to those internal web services. A typical example is XML injection targeting SOAP communications. Previous work has shown that it is possible to automatically generate such kind of attacks using search-based techniques. In this paper, we improve upon previous results by providing more efficient techniques to generate such attacks. In particular, we investigate four different algorithms and two different fitness functions. A large empirical study, involving also two industrial systems, shows that our technique is effective at automatically generating XML injection attacks.",
        "keywords": [
            "XML",
            "Simple object access protocol",
            "Testing",
            "Service-oriented architecture"
        ]
    },
    {
        "title": "Combining Code and Requirements Coverage with Execution Cost for Test Suite Reduction.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2777831",
        "volume": "45",
        "abstract": "Test suites tend to become large and complex after software evolution iterations, thus increasing effort and cost to execute regression testing. In this context, test suite reduction approaches could be applied to identify subsets of original test suites that preserve the capability of satisfying testing requirements and revealing faults. In this paper, we propose Multi-Objective test suites REduction (named MORE+): a three-dimension approach for test suite reduction. The first dimension is the structural one and concerns the information on how test cases in a suite exercise the under-test application. The second dimension is functional and concerns how test cases exercise business application requirements. The third dimension is the cost and concerns the time to execute test cases. We define MORE+ as a multi-objective approach that reduces test suites so maximizing their capability in revealing faults according to the three considered dimensions. We have compared MORE+ with seven baseline approaches on 20 Java applications. Results showed, in particular, the effectiveness of MORE+ in reducing test suites with respect to these baselines, i.e., significantly more faults are revealed with test suites reduced by applying MORE+.",
        "keywords": [
            "Testing",
            "Software",
            "Business",
            "Java",
            "Space exploration",
            "Large scale integration",
            "Software engineering"
        ]
    },
    {
        "title": "On the Multiple Sources and Privacy Preservation Issues for Heterogeneous Defect Prediction.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2780222",
        "volume": "45",
        "abstract": "Heterogeneous defect prediction (HDP) refers to predicting defect-proneness of software modules in a target project using heterogeneous metric data from other projects. Existing HDP methods mainly focus on predicting target instances with single source. In practice, there exist plenty of external projects. Multiple sources can generally provide more information than a single project. Therefore, it is meaningful to investigate whether the HDP performance can be improved by employing multiple sources. However, a precondition of conducting HDP is that the external sources are available. Due to privacy concerns, most companies are not willing to share their data. To facilitate data sharing, it is essential to study how to protect the privacy of data owners before they release their data. In this paper, we study the above two issues in HDP. Specifically, to utilize multiple sources effectively, we propose a multi-source selection based manifold discriminant alignment (MSMDA) approach. To protect the privacy of data owners, a sparse representation based double obfuscation algorithm is designed and applied to HDP. Through a case study of 28 projects, our results show that MSMDA can achieve better performance than a range of baseline methods. The improvement is 3.4-15.3 percent in g-measure and 3.0-19.1 percent in AUG.",
        "keywords": [
            "Measurement",
            "Software",
            "Data privacy",
            "Privacy",
            "Predictive models",
            "Training data",
            "Companies"
        ]
    },
    {
        "title": "Smart Bound Selection for the Verification of UML/OCL Class Diagrams.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2777830",
        "volume": "45",
        "abstract": "Correctness of UML class diagrams annotated with OCL constraints can be checked using bounded verification techniques, e.g., SAT or constraint programming (CP) solvers. Bounded verification detects faults efficiently but, on the other hand, the absence of faults does not guarantee a correct behavior outside the bounded domain. Hence, choosing suitable bounds is a non-trivial process as there is a trade-off between the verification time (faster for smaller domains) and the confidence in the result (better for larger domains). Unfortunately, bounded verification tools provide little support in the bound selection process. In this paper, we present a technique that can be used to (i) automatically infer verification bounds whenever possible, (ii) tighten a set of bounds proposed by the user and (iii) guide the user in the bound selection process. This approach may increase the usability of UML/OCL bounded verification tools and improve the efficiency of the verification process.",
        "keywords": [
            "Unified modeling language",
            "Tools",
            "Sociology",
            "Statistics",
            "Analytical models",
            "Computational modeling",
            "Software"
        ]
    },
    {
        "title": "Automatic Detection and Removal of Ineffective Mutants for the Mutation Analysis of Relational Database Schemas.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2786286",
        "volume": "45",
        "abstract": "Data is one of an organization's most valuable and strategic assets. Testing the relational database schema, which protects the integrity of this data, is of paramount importance. Mutation analysis is a means of estimating the fault-finding “strength” of a test suite. As with program mutation, however, relational database schema mutation results in many “ineffective” mutants that both degrade test suite quality estimates and make mutation analysis more time consuming. This paper presents a taxonomy of ineffective mutants for relational database schemas, summarizing the root causes of ineffectiveness with a series of key patterns evident in database schemas. On the basis of these, we introduce algorithms that automatically detect and remove ineffective mutants. In an experimental study involving the mutation analysis of 34 schemas used with three popular relational database management systems-HyperSQL, PostgreSQL, and SQLite-the results show that our algorithms can identify and discard large numbers of ineffective mutants that can account for up to 24 percent of mutants, leading to a change in mutation score for 33 out of 34 schemas. The tests for seven schemas were found to achieve 100 percent scores, indicating that they were capable of detecting and killing all non-equivalent mutants. The results also reveal that the execution cost of mutation analysis may be significantly reduced, especially with “heavyweight” DBMSs like PostgreSQL.",
        "keywords": [
            "Relational databases",
            "Algorithm design and analysis",
            "Testing",
            "Taxonomy",
            "Google",
            "Software"
        ]
    },
    {
        "title": "Coverage-Based Greybox Fuzzing as Markov Chain.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2785841",
        "volume": "45",
        "abstract": "Coverage-based Greybox Fuzzing (CGF) is a random testing approach that requires no program analysis. A new test is generated by slightly mutating a seed input. If the test exercises a new and interesting path, it is added to the set of seeds; otherwise, it is discarded. We observe that most tests exercise the same few “high-frequency” paths and develop strategies to explore significantly more paths with the same number of tests by gravitating towards low-frequency paths. We explain the challenges and opportunities of CGF using a Markov chain model which specifies the probability that fuzzing the seed that exercises path i generates an input that exercises path j. Each state (i.e., seed) has an energy that specifies the number of inputs to be generated from that seed. We show that CGF is considerably more efficient if energy is inversely proportional to the density of the stationary distribution and increases monotonically every time that seed is chosen. Energy is controlled with a power schedule. We implemented several schedules by extending AFL. In 24 hours, AFLFast exposes 3 previously unreported CVEs that are not exposed by AFL and exposes 6 previously unreported CVEs 7x faster than AFL. AFLFast produces at least an order of magnitude more unique crashes than AFL. We compared AFLFast to the symbolic executor Klee. In terms of vulnerability detection, AFLFast is significantly more effective than Klee on the same subject programs that were discussed in the original Klee paper. In terms of code coverage, AFLFast only slightly outperforms Klee while a combination of both tools achieves best results by mitigating the individual weaknesses.",
        "keywords": [
            "Schedules",
            "Markov processes",
            "Computer crashes",
            "Search problems",
            "Tools",
            "Systematics"
        ]
    },
    {
        "title": "Decomposition-Based Approach for Model-Based Test Generation.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2781231",
        "volume": "45",
        "abstract": "Model-based test generation by model checking is a well-known testing technique that, however, suffers from the state explosion problem of model checking and it is, therefore, not always applicable. In this paper, we address this issue by decomposing a system model into suitable subsystem models separately analyzable. Our technique consists in decomposing that portion of a system model that is of interest for a given testing requirement, into a tree of subsystems by exploiting information on model variable dependency. The technique generates tests for the whole system model by merging tests built from those subsystems. We measure and report effectiveness and efficiency of the proposed decomposition-based test generation approach, both in terms of coverage and time.",
        "keywords": [
            "Model checking",
            "Unified modeling language",
            "Valves",
            "Silicon",
            "Explosions",
            "Presses"
        ]
    },
    {
        "title": "The Good, the Bad and the Ugly: A Study of Security Decisions in a Cyber-Physical Systems Game.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2782813",
        "volume": "45",
        "abstract": "Stakeholders' security decisions play a fundamental role in determining security requirements, yet, little is currently understood about how different stakeholder groups within an organisation approach security and the drivers and tacit biases underpinning their decisions. We studied and contrasted the security decisions of three demographics-security experts, computer scientists and managers-when playing a tabletop game that we designed and developed. The game tasks players with managing the security of a cyber-physical environment while facing various threats. Analysis of 12 groups of players (4 groups in each of our demographics) reveals strategies that repeat in particular demographics, e.g., managers and security experts generally favoring technological solutions over personnel training, which computer scientists preferred. Surprisingly, security experts were not ipso facto better players-in some cases, they made very questionable decisions-yet they showed a higher level of confidence in themselves. We classified players' decision-making processes, i.e., procedure-, experience-, scenario- or intuition-driven. We identified decision patterns, both good practices and typical errors and pitfalls. Our game provides a requirements sandbox in which players can experiment with security risks, learn about decision-making and its consequences, and reflect on their own perception of security.",
        "keywords": [
            "Games",
            "Data security",
            "Cyber-physical systems",
            "Decision making"
        ]
    },
    {
        "title": "Automatic Identification and Classification of Software Development Video Tutorial Fragments.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2779479",
        "volume": "45",
        "abstract": "Software development video tutorials have seen a steep increase in popularity in recent years. Their main advantage is that they thoroughly illustrate how certain technologies, programming languages, etc. are to be used. However, they come with a caveat: there is currently little support for searching and browsing their content. This makes it difficult to quickly find the useful parts in a longer video, as the only options are watching the entire video, leading to wasted time, or fast-forwarding through it, leading to missed information. We present an approach to mine video tutorials found on the web and enable developers to query their contents as opposed to just their metadata. The video tutorials are processed and split into coherent fragments, such that only relevant fragments are returned in response to a query. Moreover, fragments are automatically classified according to their purpose, such as introducing theoretical concepts, explaining code implementation steps, or dealing with errors. This allows developers to set filters in their search to target a specific type of video fragment they are interested in. In addition, the video fragments in CodeTube are complemented with information from other sources, such as Stack Overflow discussions, giving more context and useful information for understanding the concepts.",
        "keywords": [
            "Tutorials",
            "Java",
            "Software",
            "YouTube",
            "Indexes",
            "Androids",
            "Humanoid robots"
        ]
    },
    {
        "title": "Automatic Loop Summarization via Path Dependency Analysis.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2788018",
        "volume": "45",
        "abstract": "Analyzing loops is very important for various software engineering tasks such as bug detection, test case generation and program optimization. However, loops are very challenging structures for program analysis, especially when (nested) loops contain multiple paths that have complex interleaving relationships. In this paper, we propose the path dependency automaton (PDA) to capture the dependencies among the multiple paths in a loop. Based on the PDA, we first propose a loop classification to understand the complexity of loop summarization. Then, we propose a loop analysis framework, named Proteus, which takes a loop program and a set of variables of interest as inputs and summarizes path-sensitive loop effects (i.e., disjunctive loop summary) on the variables of interest. An algorithm is proposed to traverse the PDA to summarize the effect for all possible executions in the loop. We have evaluated Proteus using loops from five open-source projects and two well-known benchmarks and applying the disjunctive loop summary to three applications: loop bound analysis, program verification and test case generation. The evaluation results have demonstrated that Proteus can compute a more precise bound than the existing loop bound analysis techniques; Proteus can significantly outperform the state-of-the-art tools for loop program verification; and Proteus can help generate test cases for deep loops within one second, while symbolic execution tools KLEE and Pex either need much more time or fail.",
        "keywords": [
            "Debugging",
            "Automata",
            "Benchmark testing",
            "Public domain software"
        ]
    },
    {
        "title": "ConPredictor: Concurrency Defect Prediction in Real-World Applications.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2791521",
        "volume": "45",
        "abstract": "Concurrent programs are difficult to test due to their inherent non-determinism. To address this problem, testing often requires the exploration of thread schedules of a program; this can be time-consuming when applied to real-world programs. Software defect prediction has been used to help developers find faults and prioritize their testing efforts. Prior studies have used machine learning to build such predicting models based on designed features that encode the characteristics of programs. However, research has focused on sequential programs; to date, no work has considered defect prediction for concurrent programs, with program characteristics distinguished from sequential programs. In this paper, we present ConPredictor, an approach to predict defects specific to concurrent programs by combining both static and dynamic program metrics. Specifically, we propose a set of novel static code metrics based on the unique properties of concurrent programs. We also leverage additional guidance from dynamic metrics constructed based on mutation analysis. Our evaluation on four large open source projects shows that ConPredictor improved both within-project defect prediction and cross-project defect prediction compared to traditional features.",
        "keywords": [
            "Concurrent computing",
            "Predictive models",
            "Software",
            "Programming",
            "Testing",
            "Synchronization"
        ]
    },
    {
        "title": "makeSense: Simplifying the Integration of Wireless Sensor Networks into Business Processes.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2787585",
        "volume": "45",
        "abstract": "A wide gap exists between the state of the art in developing Wireless Sensor Network (WSN) software and current practices concerning the design, execution, and maintenance of business processes. WSN software is most often developed based on low-level OS abstractions, whereas business process development leverages high-level languages and tools. This state of affairs places WSNs at the fringe of industry. The makeSense system addresses this problem by simplifying the integration of WSNs into business processes. Developers use BPMN models extended with WSN-specific constructs to specify the application behavior across both traditional business process execution environments and the WSN itself, which is to be equipped with application-specific software. We compile these models into a high-level intermediate language-also directly usable by WSN developers-and then into OS-specific deployment-ready binaries. Key to this process is the notion of meta-abstraction, which we define to capture fundamental patterns of interaction with and within the WSN. The concrete realization of meta-abstractions is application-specific; developers tailor the system configuration by selecting concrete abstractions out of the existing codebase or by providing their own. Our evaluation of makeSense shows that i) users perceive our approach as a significant advance over the state of the art, providing evidence of the increased developer productivity when using makeSense; ii) in large-scale simulations, our prototype exhibits an acceptable system overhead and good scaling properties, demonstrating the general applicability of makeSense; and, iii) our prototype-including the complete tool-chain and underlying system support-sustains a real-world deployment where estimates by domain specialists indicate the potential for drastic reductions in the total cost of ownership compared to wired and conventional WSN-based solutions.",
        "keywords": [
            "Wireless sensor networks",
            "Business",
            "Programming",
            "Ventilation",
            "Software",
            "Concrete"
        ]
    },
    {
        "title": "\"Sampling\" as a Baseline Optimizer for Search-Based Software Engineering.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2790925",
        "volume": "45",
        "abstract": "Increasingly, Software Engineering (SE) researchers use search-based optimization techniques to solve SE problems with multiple conflicting objectives. These techniques often apply CPU-intensive evolutionary algorithms to explore generations of mutations to a population of candidate solutions. An alternative approach, proposed in this paper, is to start with a very large population and sample down to just the better solutions. We call this method “Sway”, short for “the sampling way”. This paper compares Sway versus state-of-the-art search-based SE tools using seven models: five software product line models; and two other software process control models (concerned with project management, effort estimation, and selection of requirements) during incremental agile development. For these models, the experiments of this paper show that Sway is competitive with corresponding state-of-the-art evolutionary algorithms while requiring orders of magnitude fewer evaluations. Considering the simplicity and effectiveness of Sway, we, therefore, propose this approach as a baseline method for search-based software engineering models, especially for models that are very slow to execute.",
        "keywords": [
            "Software engineering",
            "Optimization",
            "Software",
            "Numerical models",
            "Evolutionary computation",
            "Estimation",
            "Sociology"
        ]
    },
    {
        "title": "Text Filtering and Ranking for Security Bug Report Prediction.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2787653",
        "volume": "45",
        "abstract": "Security bug reports can describe security critical vulnerabilities in software products. Bug tracking systems may contain thousands of bug reports, where relatively few of them are security related. Therefore finding unlabelled security bugs among them can be challenging. To help security engineers identify these reports quickly and accurately, text-based prediction models have been proposed. These can often mislabel security bug reports due to a number of reasons such as class imbalance, where the ratio of non-security to security bug reports is very high. More critically, we have observed that the presence of security related keywords in both security and non-security bug reports can lead to the mislabelling of security bug reports. This paper proposes FARSEC, a framework for filtering and ranking bug reports for reducing the presence of security related keywords. Before building prediction models, our framework identifies and removes non-security bug reports with security related keywords. We demonstrate that FARSEC improves the performance of text-based prediction models for security bug reports in 90 percent of cases. Specifically, we evaluate it with 45,940 bug reports from Chromium and four Apache projects. With our framework, we mitigate the class imbalance issue and reduce the number of mislabelled security bug reports by 38 percent.",
        "keywords": [
            "Security",
            "Computer bugs",
            "Predictive models",
            "Software",
            "Data models",
            "Measurement",
            "Buildings"
        ]
    },
    {
        "title": "Correction of \"A Comparative Study to Benchmark Cross-Project Defect Prediction Approaches\".",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2790413",
        "volume": "45",
        "abstract": "Unfortunately, the article “A Comparative Study to Benchmark Cross-project Defect Prediction Approaches” has a problem in the statistical analysis which was pointed out almost immediately after the pre-print of the article appeared online. While the problem does not negate the contribution of the the article and all key findings remain the same, it does alter some rankings of approaches used in the study. Within this correction, we will explain the problem, how we resolved it, and present the updated results.",
        "keywords": [
            "Sociology",
            "Measurement",
            "Benchmark testing",
            "Statistical analysis",
            "Ranking (statistics)",
            "Terminology"
        ]
    },
    {
        "title": "A Deep Learning Model for Estimating Story Points.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2792473",
        "volume": "45",
        "abstract": "Although there has been substantial research in software analytics for effort estimation in traditional software projects, little work has been done for estimation in agile projects, especially estimating the effort required for completing user stories or issues. Story points are the most common unit of measure used for estimating the effort involved in completing a user story or resolving an issue. In this paper, we propose a prediction model for estimating story points based on a novel combination of two powerful deep learning architectures: long short-term memory and recurrent highway network. Our prediction system is end-to-end trainable from raw input data to prediction outcomes without any manual feature engineering. We offer a comprehensive dataset for story points-based estimation that contains 23,313 issues from 16 open source projects. An empirical evaluation demonstrates that our approach consistently outperforms three common baselines (Random Guessing, Mean, and Median methods) and six alternatives (e.g., using Doc2Vec and Random Forests) in Mean Absolute Error, Median Absolute Error, and the Standardized Accuracy.",
        "keywords": [
            "Project management",
            "Software architecture",
            "Learning (artificial intelligence)",
            "Deep learning"
        ]
    },
    {
        "title": "Design Rule Spaces: A New Model for Representing and Analyzing Software Architecture.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2797899",
        "volume": "45",
        "abstract": "In this paper, we propose an architecture model called Design Rule Space (DRSpace). We model the architecture of a software system as multiple overlapping DRSpaces, reflecting the fact that any complex software system must contain multiple aspects, features, patterns, etc. We show that this model provides new ways to analyze software quality. In particular, we introduce an Architecture Root detection algorithm that captures DRSpaces containing large numbers of a project's bug-prone files, which are called Architecture Roots (ArchRoots). After investigating ArchRoots calculated from 15 open source projects, the following observations become clear: from 35 to 91 percent of a project's most bug-prone files can be captured by just 5 ArchRoots, meaning that bug-prone files are likely to be architecturally connected. Furthermore, these ArchRoots tend to live in the system for significant periods of time, serving as the major source of bug-proneness and high maintainability costs. Moreover, each ArchRoot reveals multiple architectural flaws that propagate bugs among files and this will incur high maintenance costs over time. The implication of our study is that the quality, in terms of bug-proneness, of a large, complex software project cannot be fundamentally improved without first fixing its architectural flaws.",
        "keywords": [
            "Computer bugs",
            "Computer architecture",
            "Software architecture",
            "Production facilities",
            "Analytical models",
            "Software systems"
        ]
    },
    {
        "title": "The Impact of Automated Parameter Optimization on Defect Prediction Models.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2794977",
        "volume": "45",
        "abstract": "Defect prediction models-classifiers that identify defect-prone software modules-have configurable parameters that control their characteristics (e.g., the number of trees in a random forest). Recent studies show that these classifiers underperform when default settings are used. In this paper, we study the impact of automated parameter optimization on defect prediction models. Through a case study of 18 datasets, we find that automated parameter optimization: (1) improves AUC performance by up to 40 percentage points; (2) yields classifiers that are at least as stable as those trained using default settings; (3) substantially shifts the importance ranking of variables, with as few as 28 percent of the top-ranked variables in optimized classifiers also being top-ranked in non-optimized classifiers; (4) yields optimized settings for 17 of the 20 most sensitive parameters that transfer among datasets without a statistically significant drop in performance; and (5) adds less than 30 minutes of additional computation to 12 of the 26 studied classification techniques. While widely-used classification techniques like random forest and support vector machines are not optimization-sensitive, traditionally overlooked techniques like C5.0 and neural networks can actually outperform widely-used techniques after optimization is applied. This highlights the importance of exploring the parameter space when using parameter-sensitive classification techniques.",
        "keywords": [
            "Optimization",
            "Predictive models",
            "Computational modeling",
            "Software",
            "Neural networks",
            "Computational efficiency",
            "Power system stability"
        ]
    },
    {
        "title": "Toward Methodological Guidelines for Process Theories and Taxonomies in Software Engineering.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2796554",
        "volume": "45",
        "abstract": "Software engineering is increasingly concerned with theory because the foundational knowledge comprising theories provides a crucial counterpoint to the practical knowledge expressed through methods and techniques. Fortunately, much guidance is available for generating and evaluating theories for explaining why things happen (variance theories). Unfortunately, little guidance is available concerning theories for explaining how things happen (process theories), or theories for analyzing and understanding situations (taxonomies). This paper therefore attempts to clarify the nature and functions of process theories and taxonomies in software engineering research, and to synthesize methodological guidelines for their generation and evaluation. It further advances the key insight that most process theories are taxonomies with additional propositions, which helps inform their evaluation. The proposed methodological guidance has many benefits: it provides a concise summary of existing guidance from reference disciplines, it adapts techniques from reference disciplines to the software engineering context, and it promotes approaches that better facilitate scientific consensus.",
        "keywords": [
            "Taxonomy",
            "Software",
            "Software engineering",
            "Guidelines",
            "Measurement",
            "Knowledge engineering",
            "Organisms"
        ]
    },
    {
        "title": "A Systematic Literature Review of Applications of the Physics of Notations.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2802910",
        "volume": "45",
        "abstract": "INTRODUCTION: The Physics of Notations (PoN) is a theory for the design of cognitively effective visual notations, emphasizing the need for design grounded in objective and verifiable rationale. Although increasingly applied, no systematic analysis of PoN applications has yet been performed to assess the theory's efficacy in practice. OBJECTIVES: Our primary objective was to assess the scope and verifiability of PoN applications. METHOD: We performed a systematic literature review (SLR) of peer-reviewed PoN applications. We analyzed what visual notations have been evaluated and designed using the PoN, for what reasons, to what degree applications consider requirements of their notation's users, and how verifiable these applications are. RESULTS: Seventy PoN applications were analyzed. We found major differences between applications evaluating existing notations and applications designing new notations. Particularly, in the case of new notations, we found that most applications adopted the PoN with little critical thought towards it, rarely considered its suitability for a particular context, and typically treated and discussed the PoN with few, if any, verifiable details and data. CONCLUSION: The results warrant consideration for those applying the PoN to do so carefully, and show the need for additional means to guide designers in systematically applying the PoN.",
        "keywords": [
            "Visualization",
            "Unified modeling language",
            "Semantics",
            "Complexity theory",
            "Physics"
        ]
    },
    {
        "title": "A Systematic Review of Interaction in Search-Based Software Engineering.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2803055",
        "volume": "45",
        "abstract": "Search-Based Software Engineering (SBSE) has been successfully applied to automate a wide range of software development activities. Nevertheless, in those software engineering problems where human evaluation and preference are crucial, such insights have proved difficult to characterize in search, and solutions might not look natural when that is the expectation. In an attempt to address this, an increasing number of researchers have reported the incorporation of the 'human-in-the-loop' during search and interactive SBSE has attracted significant attention recently. However, reported results are fragmented over different development phases, and a great variety of novel interactive approaches and algorithmic techniques have emerged. To better integrate these results, we have performed a systematic literature review of interactive SBSE. From a total of 669 papers, 26 primary studies were identified. To enable their analysis, we formulated a classification scheme focused on four crucial aspects of interactive search, i.e., the problem formulation, search technique, interactive approach, and the empirical framework. Our intention is that the classification scheme affords a methodological approach for interactive SBSE. Lastly, as well as providing a detailed cross analysis, we identify and discuss some open issues and potential future trends for the research community.",
        "keywords": [
            "Software",
            "Software engineering",
            "Search problems",
            "Optimization",
            "Systematics",
            "Market research",
            "Software metrics"
        ]
    },
    {
        "title": "Instance Migration Validity for Dynamic Evolution of Data-Aware Processes.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2802925",
        "volume": "45",
        "abstract": "Likely more than many other software artifacts, business processes constantly evolve to adapt to ever changing application requirements. To enable dynamic process evolution, where changes are applied to in-flight processes, running process instances have to be migrated. On the one hand, as many instances as possible should be migrated to the changed process. On the other hand, the validity to migrate an instance should be guaranteed to avoid introducing dynamic change bugs after migration. As our theoretical results show, when the state of variables is taken into account, migration validity of data-aware process instances is undecidable. Based on the trace of an instance, existing approaches leverage trace replaying to check migration validity. However, they err on the side of caution, not identifying many instances as potentially safe to migrate. We present a more relaxed migration validity checking approach based on the dependence graph of a trace. We evaluate effectiveness and efficiency of our approach experimentally showing that it allows for more instances to safely migrate than for existing approaches and that it scales in the number of instances checked.",
        "keywords": [
            "Process control",
            "Business data processing",
            "Debugging",
            "Software architecture"
        ]
    },
    {
        "title": "Verification Templates for the Analysis of User Interface Software Design.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2804939",
        "volume": "45",
        "abstract": "The paper describes templates for model-based analysis of usability and safety aspects of user interface software design. The templates crystallize general usability principles commonly addressed in user-centred safety requirements, such as the ability to undo user actions, the visibility of operational modes, and the predictability of user interface behavior. These requirements have standard forms across different application domains, and can be instantiated as properties of specific devices. The modeling and analysis process is carried out using the Prototype Verification System (PVS), and is further facilitated by structuring the specification of the device using a format that is designed to be generic across interactive systems. A concrete case study based on a commercial infusion pump is used to illustrate the approach. A detailed presentation of the automated verification process using PVS shows how failed proof attempts provide precise information about problematic user interface software features.",
        "keywords": [
            "User interfaces",
            "Safety",
            "ISO Standards",
            "Usability"
        ]
    },
    {
        "title": "VT-Revolution: Interactive Programming Video Tutorial Authoring and Watching System.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2802916",
        "volume": "45",
        "abstract": "Procedural knowledge describes actions and manipulations that are carried out to complete programming tasks. An effective way to document procedural knowledge is programming video tutorials. Unlike text-based software artifacts and tutorials that can be effectively searched and linked using information retrieval techniques, the streaming nature of programming videos limits the ways to explore the captured workflows and interact with files, code and program output in the videos. Existing solutions to adding interactive workflow and elements to programming videos have a dilemma between the level of desired interaction and the efforts required for authoring tutorials. In this work, we tackle this dilemma by designing and building a programming video tutorial authoring system that leverages operating system level instrumentation to log workflow history while tutorial authors are creating programming videos, and the corresponding tutorial watching system that enhances the learning experience of video tutorials by providing programming-specific workflow history and timeline-based browsing interactions. Our tutorial authoring system does not incur any additional burden on tutorial authors to make programming videos interactive. Given a programming video accompanied by synchronously-logged workflow history, our tutorial watching system allows tutorial watchers to freely explore the captured workflows and interact with files, code and program output in the tutorial. We conduct a user study of 135 developers to evaluate the design and effectiveness of our system in helping developers learn programming knowledge in video tutorials.",
        "keywords": [
            "Tutorials",
            "Programming",
            "Streaming media",
            "Tools",
            "Task analysis",
            "History",
            "Software"
        ]
    },
    {
        "title": "Asymmetric Release Planning: Compromising Satisfaction against Dissatisfaction.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2810895",
        "volume": "45",
        "abstract": "Maximizing satisfaction from offering features as part of the upcoming release(s) is different from minimizing dissatisfaction gained from not offering features. This asymmetric behavior has never been utilized for product release planning. We study Asymmetric Release Planning (ARP) by accommodating asymmetric feature evaluation. We formulated and solved ARP as a bi-criteria optimization problem. In its essence, it is the search for optimized trade-offs between maximum stakeholder satisfaction and minimum dissatisfaction. Different techniques including a continuous variant of Kano analysis are available to predict the impact on satisfaction and dissatisfaction with a product release from offering or not offering a feature. As a proof of concept,we validated the proposed solution approach called Satisfaction-Dissatisfaction Optimizer (SDO) via a real-world case study project. From running three replications with varying effort capacities, we demonstrate that SDO generates optimized trade-off solutions being (i) of a different value profile and different structure, (ii) superior to the application of random search and heuristics in terms of quality and completeness, and (iii) superior to the usage of manually generated solutions generated from managers of the case study company. A survey with 20 stakeholders evaluated the applicability and usefulness of the generated results.",
        "keywords": [
            "Planning",
            "Stakeholders",
            "Software engineering",
            "Software",
            "Streaming media",
            "Mathematical model",
            "Optimization"
        ]
    },
    {
        "title": "Fully Reflective Execution Environments: Virtual Machines for More Flexible Software.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2812715",
        "volume": "45",
        "abstract": "VMs are complex pieces of software that implement programming language semantics in an efficient, portable, and secure way. Unfortunately, mainstream VMs provide applications with few mechanisms to alter execution semantics or memory management at run time. We argue that this limits the evolvability and maintainability of running systems for both, the application domain, e.g., to support unforeseen requirements, and the VM domain, e.g., to modify the organization of objects in memory. This work explores the idea of incorporating reflective capabilities into the VM domain and analyzes its impact in the context of software adaptation tasks. We characterize the notion of a fully reflective VM, a kind of VM that provides means for its own observability and modifiability at run time. This enables programming languages to adapt the underlying VM to changing requirements. We propose a reference architecture for such VMs and present TruffleMATE as a prototype for this architecture. We evaluate the mechanisms TruffleMATE provides to deal with unanticipated dynamic adaptation scenarios for security, optimization, and profiling aspects. In contrast to existing alternatives, we observe that TruffleMATE is able to handle all scenarios, using less than 50 lines of code for each, and without interfering with the application's logic.",
        "keywords": [
            "Software",
            "Memory management",
            "Task analysis",
            "Virtual machining",
            "Semantics",
            "Shape"
        ]
    },
    {
        "title": "How Developers Diagnose Potential Security Vulnerabilities with a Static Analysis Tool.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2810116",
        "volume": "45",
        "abstract": "While using security tools to resolve security defects, software developers must apply considerable effort. Success depends on a developer's ability to interact with tools, ask the right questions, and make strategic decisions. To build better security tools and subsequently help developers resolve defects more accurately and efficiently, we studied the defect resolution process-from the questions developers ask to their strategies for answering them. In this paper, we report on an exploratory study with novice and experienced software developers. We equipped them with Find Security Bugs, a security-oriented static analysis tool, and observed their interactions with security vulnerabilities in an open-source system that they had previously contributed to. We found that they asked questions not only about security vulnerabilities, associated attacks, and fixes, but also questions about the software itself, the social ecosystem that built the software, and related resources and tools. We describe the strategic successes and failures we observed and how future tools can leverage our findings to encourage better strategies.",
        "keywords": [
            "Tools",
            "Task analysis",
            "Software",
            "Static analysis",
            "Computer bugs",
            "SQL injection"
        ]
    },
    {
        "title": "Predictive Mutation Testing.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2809496",
        "volume": "45",
        "abstract": "Test suites play a key role in ensuring software quality. A good test suite may detect more faults than a poor-quality one. Mutation testing is a powerful methodology for evaluating the fault-detection ability of test suites. In mutation testing, a large number of mutants may be generated and need to be executed against the test suite under evaluation to check how many mutants the test suite is able to detect, as well as the kind of mutants that the current test suite fails to detect. Consequently, although highly effective, mutation testing is widely recognized to be also computationally expensive, inhibiting wider uptake. To alleviate this efficiency concern, we propose Predictive Mutation Testing (PMT): the first approach to predicting mutation testing results without executing mutants. In particular, PMT constructs a classification model, based on a series of features related to mutants and tests, and uses the model to predict whether a mutant would be killed or remain alive without executing it. PMT has been evaluated on 163 real-world projects under two application scenarios (cross-version and cross-project). The experimental results demonstrate that PMT improves the efficiency of mutation testing by up to 151.4X while incurring only a small accuracy loss. It achieves above 0.80 AUC values for the majority of projects, indicating a good tradeoff between the efficiency and effectiveness of predictive mutation testing. Also, PMT is shown to perform well on different tools and tests, be robust in the presence of imbalanced data, and have high predictability (over 60 percent confidence) when predicting the execution results of the majority of mutants.",
        "keywords": [
            "Predictive models",
            "Pattern classification",
            "Software testing",
            "Sensitivity analysis",
            "Software quality",
            "Machine learning"
        ]
    },
    {
        "title": "Test Generation and Test Prioritization for Simulink Models with Dynamic Behavior.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2811489",
        "volume": "45",
        "abstract": "All engineering disciplines are founded and rely on models, although they may differ on purposes and usages of modeling. Among the different disciplines, the engineering of Cyber Physical Systems (CPSs) particularly relies on models with dynamic behaviors (i.e., models that exhibit time-varying changes). The Simulink modeling platform greatly appeals to CPS engineers since it captures dynamic behavior models. It further provides seamless support for two indispensable engineering activities: (1) automated verification of abstract system models via model simulation, and (2) automated generation of system implementation via code generation. We identify three main challenges in the verification and testing of Simulink models with dynamic behavior, namely incompatibility, oracle and scalability challenges. We propose a Simulink testing approach that attempts to address these challenges. Specifically, we propose a black-box test generation approach, implemented based on meta-heuristic search, that aims to maximize diversity in test output signals generated by Simulink models. We argue that in the CPS domain test oracles are likely to be manual and therefore the main cost driver of testing. In order to lower the cost of manual test oracles, we propose a test prioritization algorithm to automatically rank test cases generated by our test generation algorithm according to their likelihood to reveal a fault. Engineers can then select, according to their test budget, a subset of the most highly ranked test cases. To demonstrate scalability, we evaluate our testing approach using industrial Simulink models. Our evaluation shows that our test generation and test prioritization approaches outperform baseline techniques that rely on random testing and structural coverage.",
        "keywords": [
            "Software packages",
            "Testing",
            "Tools",
            "Computational modeling",
            "Vehicle dynamics",
            "Scalability"
        ]
    },
    {
        "title": "A Screening Test for Disclosed Vulnerabilities in FOSS Components.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2816033",
        "volume": "45",
        "abstract": "Free and Open Source Software (FOSS) components are ubiquitous in both proprietary and open source applications. Each time a vulnerability is disclosed in a FOSS component, a software vendor using this component in an application must decide whether to update the FOSS component, patch the application itself, or just do nothing as the vulnerability is not applicable to the older version of the FOSS component used. This is particularly challenging for enterprise software vendors that consume thousands of FOSS components and offer more than a decade of support and security fixes for their applications. Moreover, customers expect vendors to react quickly on disclosed vulnerabilities-in case of widely discussed vulnerabilities such as Heartbleed, within hours. To address this challenge, we propose a screening test: a novel, automatic method based on thin slicing, for estimating quickly whether a given vulnerability is present in a consumed FOSS component by looking across its entire repository. We show that our screening test scales to large open source projects (e.g., Apache Tomcat, Spring Framework, Jenkins) that are routinely used by large software vendors, scanning thousands of commits and hundred thousands lines of code in a matter of minutes. Further, we provide insights on the empirical probability that, on the above mentioned projects, a potentially vulnerable component might not actually be vulnerable after all.",
        "keywords": [
            "Security",
            "Maintenance engineering",
            "Tools",
            "Jacobian matrices",
            "Patents",
            "Open source software"
        ]
    },
    {
        "title": "Creating Rich and Representative Personas by Discovering Affordances.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2826537",
        "volume": "45",
        "abstract": "During the last decade, information system designers have used the persona technique to put user needs and preferences at the center of all development decisions. Persona development teams draw on qualitative data, quantitative data or a combination of both to develop personas that are representative of the target users. Despite the benefits of both approaches, qualitative methods are limited by the cognitive capabilities of the experts, whereas quantitative methods lack contextual richness. To gain the advantages of both approaches, this article suggests a mixed qualitative-quantitative approach to create user personas based on the patterns of the affordances they actualize rather than merely the actions they take. It enriches personas by referring to the purposes fulfilled through affordance actualizations, and it grounds personas in readily available objective log data. This study illustrates the practical value of the proposed methodology by empirically creating personas based on real user data. Furthermore, it demonstrates its value by having practitioners compare the suggested method to that of qualitative-only and quantitative-only methods.",
        "keywords": [
            "Software",
            "Maintenance engineering",
            "Task analysis",
            "Aging",
            "Interviews",
            "Human computer interaction"
        ]
    },
    {
        "title": "Detecting Bugs by Discovering Expectations and Their Violations.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2816639",
        "volume": "45",
        "abstract": "Code mining has been proven to be a promising approach to inferring implicit programming rules for finding software bugs. However, existing methods may report large numbers of false positives and false negatives. In this paper, we propose a novel approach called EAntMiner to improve the effectiveness of code mining. EAntMiner elaborately reduces noises from statements irrelevant to interesting rules and different implementation forms of the same logic. During preprocessing, we employ program slicing to decompose the original source repository into independent sub-repositories. In each sub-repository, statements irrelevant to critical operations (automatically extracted from source code) are excluded and various semantics-equivalent implementations are normalized into a canonical form as far as possible. Moreover, to tackle the challenge that some bugs are difficult to be detected by mining frequent patterns as rules, we further developed a kNN-based method to identify them. We have implemented EAntMiner and evaluated it on four large-scale C systems. EAntMiner successfully detected 105 previously unknown bugs that have been confirmed by corresponding development communities. A set of comparative evaluations also demonstrate that EAntMiner can effectively improve the precision of code mining.",
        "keywords": [
            "Computer bugs",
            "Data mining",
            "Linux",
            "Programming",
            "Kernel",
            "Semantics"
        ]
    },
    {
        "title": "Network-Clustered Multi-Modal Bug Localization.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2810892",
        "volume": "45",
        "abstract": "Developers often spend much effort and resources to debug a program. To help the developers debug, numerous information retrieval (IR)-based and spectrum-based bug localization techniques have been devised. IR-based techniques process textual information in bug reports, while spectrum-based techniques process program spectra (i.e., a record of which program elements are executed for each test case). While both techniques ultimately generate a ranked list of program elements that likely contain a bug, they only consider one source of information-either bug reports or program spectra-which is not optimal. In light of this deficiency, this paper presents a new approach dubbed Network-clustered Multi-modal Bug Localization (NetML), which utilizes multi-modal information from both bug reports and program spectra to localize bugs. NetML facilitates an effective bug localization by carrying out a joint optimization of bug localization error and clustering of both bug reports and program elements (i.e., methods). The clustering is achieved through the incorporation of network Lasso regularization, which incentivizes the model parameters of similar bug reports and similar program elements to be close together. To estimate the model parameters of both bug reports and methods, NetML employs an adaptive learning procedure based on Newton method that updates the parameters on a per-feature basis. Extensive experiments on 355 real bugs from seven software systems have been conducted to benchmark NetML against various state-of-the-art localization methods. The results show that NetML surpasses the best-performing baseline by 31.82, 22.35, 19.72, and 19.24 percent, in terms of the number of bugs successfully localized when a developer inspects the top 1, 5, and 10 methods and Mean Average Precision (MAP), respectively.",
        "keywords": [
            "Computer bugs",
            "Adaptation models",
            "Optimization",
            "Debugging",
            "Task analysis",
            "Computational modeling",
            "Information retrieval"
        ]
    },
    {
        "title": "UniDoSA: The Unified Specification and Detection of Service Antipatterns.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2819180",
        "volume": "45",
        "abstract": "Service-based Systems (SBSs) are developed on top of diverse Service-Oriented Architecture (SOA) technologies or architectural styles. Like any other complex systems, SBSs face both functional and non-functional changes at the design or implementation-level. Such changes may degrade the design quality and quality of service (QoS) of the services in SBSs by introducing poor solutions-service antipatterns. The presence of service antipatterns in SBSs may hinder the future maintenance and evolution of SBSs. Assessing the quality of design and QoS of SBSs through the detection of service antipatterns may ease their maintenance and evolution. However, the current literature lacks a unified approach for modelling and evaluating the design of SBSs in term of design quality and QoS. To address this lack, this paper presents a meta-model unifying the three main service technologies: REST, SCA, and SOAP. Using the meta-model, it describes a unified approach, UniDoSA (Unified Specification and Detection of Service Antipatterns), supported by a framework, SOFA (Service Oriented Framework for Antipatterns), for modelling and evaluating the design quality and QoS of SBSs. We apply and validate UniDoSA on: (1) 18 RESTful APIs, (2) two SCA systems with more than 150 services, and (3) more than 120 SOAP Web services. With a high precision and recall, the detection results provide evidence of the presence of service antipatterns in SBSs, which calls for future studies of their impact on QoS.",
        "keywords": [
            "Simple object access protocol",
            "Quality of service",
            "Service-oriented architecture",
            "Maintenance engineering",
            "DSL"
        ]
    },
    {
        "title": "How Do Static and Dynamic Test Case Prioritization Techniques Perform on Modern Software Systems? An Extensive Study on GitHub Projects.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2822270",
        "volume": "45",
        "abstract": "Test Case Prioritization (TCP) is an increasingly important regression testing technique for reordering test cases according to a pre-defined goal, particularly as agile practices gain adoption. To better understand these techniques, we perform the first extensive study aimed at empirically evaluating four static TCP techniques, comparing them with state-of-research dynamic TCP techniques across several quality metrics. This study was performed on 58 real-word Java programs encompassing 714 KLoC and results in several notable observations. First, our results across two effectiveness metrics (the Average Percentage of Faults Detected APFD and the cost cognizant APFDc) illustrate that at test-class granularity, these metrics tend to correlate, but this correlation does not hold at test-method granularity. Second, our analysis shows that static techniques can be surprisingly effective, particularly when measured by APFDc. Third, we found that TCP techniques tend to perform better on larger programs, but that program size does not affect comparative performance measures between techniques. Fourth, software evolution does not significantly impact comparative performance results between TCP techniques. Fifth, neither the number nor type of mutants utilized dramatically impact measures of TCP effectiveness under typical experimental settings. Finally, our similarity analysis illustrates that highly prioritized test cases tend to uncover dissimilar faults.",
        "keywords": [
            "Testing",
            "Measurement",
            "Computer bugs",
            "Software systems",
            "Java",
            "Fault detection"
        ]
    },
    {
        "title": "Bellwethers: A Baseline Method for Transfer Learning.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2821670",
        "volume": "45",
        "abstract": "Software analytics builds quality prediction models for software projects. Experience shows that (a) the more projects studied, the more varied are the conclusions; and (b) project managers lose faith in the results of software analytics if those results keep changing. To reduce this conclusion instability, we propose the use of “bellwethers”: given N projects from a community the bellwether is the project whose data yields the best predictions on all others. The bellwethers offer a way to mitigate conclusion instability because conclusions about a community are stable as long as this bellwether continues as the best oracle. Bellwethers are also simple to discover (just wrap a for-loop around standard data miners). When compared to other transfer learning methods (TCA+, transfer Naive Bayes, value cognitive boosting), using just the bellwether data to construct a simple transfer learner yields comparable predictions. Further, bellwethers appear in many SE tasks such as defect prediction, effort estimation, and bad smell detection. We hence recommend using bellwethers as a baseline method for transfer learning against which future work should be compared.",
        "keywords": [
            "Estimation",
            "Software",
            "Software engineering",
            "Task analysis",
            "Benchmark testing",
            "Complexity theory",
            "Analytical models"
        ]
    },
    {
        "title": "Microtask Programming.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2823327",
        "volume": "45",
        "abstract": "Traditional forms of Crowdsourcing such as open source software development harness crowd contributions to democratize the creation of software. However, potential contributors must first overcome joining barriers forcing casually committed contributors to spend days or weeks onboarding and thereby reducing participation. To more effectively harness potential contributions from the crowd, we propose a method for programming in which work occurs entirely through microtasks, offering contributors short, self-contained tasks such as implementing part of a function or updating a call site invoking a function to match a change made to the function. In microtask programming, microtasks involve changes to a single artifact, are automatically generated as necessary by the system, and nurture quality through iteration. A study examining the feasibility of microtask programming to create small programs found that developers were able to complete 1008 microtasks, onboard and submit their first microtask in less than 15 minutes, complete all types of microtasks in less than 5 minutes on average, and create 490 lines of code and 149 unit tests. The results demonstrate the potential feasibility as well as revealing a number of important challenges to address to successfully scale microtask programming to larger and more complex programs.",
        "keywords": [
            "Programming",
            "Task analysis",
            "Crowdsourcing",
            "Programming environments",
            "Public domain software",
            "Collaborative software"
        ]
    },
    {
        "title": "Accurate and Scalable Cross-Architecture Cross-OS Binary Code Search with Emulation.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2827379",
        "volume": "45",
        "abstract": "Different from source code clone detection, clone detection (similar code search) in binary executables faces big challenges due to the gigantic differences in the syntax and the structure of binary code that result from different configurations of compilers, architectures and OSs. Existing studies have proposed different categories of features for detecting binary code clones, including CFG structures, n-gram in CFG, input/output values, etc. In our previous study and the tool BinGo, to mitigate the huge gaps in CFG structures due to different compilation scenarios, we propose a selective inlining technique to capture the complete function semantics by inlining relevant library and user-defined functions. However, only features of input/output values are considered in BinGo. In this study, we propose to incorporate features from different categories (e.g., structural features and high-level semantic features) for accuracy improvement and emulation for efficiency improvement. We empirically compare our tool, BinGo-E, with the pervious tool BinGo and the available state-of-the-art tools of binary code search in terms of search accuracy and performance. Results show that BinGo-E achieves significantly better accuracies than BinGo for cross-architecture matching, cross-OS matching, cross-compiler matching and intra-compiler matching. Additionally, in the new task of matching binaries of forked projects, BinGo-E also exhibits a better accuracy than the existing benchmark tool. Meanwhile, BinGo-E takes less time than BinGo during the process of matching.",
        "keywords": [
            "Binary codes",
            "Semantics",
            "Tools",
            "Feature extraction",
            "Cloning",
            "Syntactics",
            "Emulation"
        ]
    },
    {
        "title": "Approximate Oracles and Synergy in Software Energy Search Spaces.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2827066",
        "volume": "45",
        "abstract": "Reducing the energy consumption of software systems through optimisation techniques such as genetic improvement is gaining interest. However, efficient and effective improvement of software systems requires a better understanding of the code-change search space. One important choice practitioners have is whether to preserve the system's original output or permit approximation, with each scenario having its own search space characteristics. When output preservation is a hard constraint, we report that the maximum energy reduction achievable by the modification operators is 2.69 percent (0.76 percent on average). By contrast, this figure increases dramatically to 95.60 percent (33.90 percent on average) when approximation is permitted, indicating the critical importance of approximate output quality assessment for code optimisation. We investigate synergy, a phenomenon that occurs when simultaneously applied source code modifications produce an effect greater than their individual sum. Our results reveal that 12.0 percent of all joint code modifications produced such a synergistic effect, though 38.5 percent produce an antagonistic interaction in which simultaneously applied modifications are less effective than when applied individually. This highlights the need for more advanced search-based techniques.",
        "keywords": [
            "Energy consumption",
            "Energy measurement",
            "Software systems",
            "Optimization",
            "Aggregates",
            "Genetics"
        ]
    },
    {
        "title": "A Systematic Evaluation of Static API-Misuse Detectors.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2827384",
        "volume": "45",
        "abstract": "Application Programming Interfaces (APIs) often have usage constraints, such as restrictions on call order or call conditions. API misuses, i.e., violations of these constraints, may lead to software crashes, bugs, and vulnerabilities. Though researchers developed many API-misuse detectors over the last two decades, recent studies show that API misuses are still prevalent. Therefore, we need to understand the capabilities and limitations of existing detectors in order to advance the state of the art. In this paper, we present the first-ever qualitative and quantitative evaluation that compares static API-misuse detectors along the same dimensions, and with original author validation. To accomplish this, we develop MUC, a classification of API misuses, and MUBENCHPIPE, an automated benchmark for detector comparison, on top of our misuse dataset, MUBENCH. Our results show that the capabilities of existing detectors vary greatly and that existing detectors, though capable of detecting misuses, suffer from extremely low precision and recall. A systematic root-cause analysis reveals that, most importantly, detectors need to go beyond the naive assumption that a deviation from the most-frequent usage corresponds to a misuse and need to obtain additional usage examples to train their models. We present possible directions towards more-powerful API-misuse detectors.",
        "keywords": [
            "Detectors",
            "Benchmark testing",
            "Classification",
            "Systematics",
            "Computer bugs"
        ]
    },
    {
        "title": "Integrative Double Kaizen Loop (IDKL): Towards a Culture of Continuous Learning and Sustainable Improvements for Software Organizations.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2829722",
        "volume": "45",
        "abstract": "In the past decades, software organizations have been relying on implementing process improvement methods to advance quality, productivity, and predictability of their development and maintenance efforts. However, these methods have proven to be challenging to implement in many situations, and when implemented, their benefits are often not sustained. Commonly, the workforce requires guidance during the initial deployment, but what happens after the guidance stops? Why do not traditional improvement methods deliver the desired results? And, how do we maintain the improvements when they are realized? In response to these questions, we have combined social and organizational learning methods with Lean's continuous improvement philosophy, Kaizen, which has resulted in an IDKL model that has successfully promoted continuous learning and improvement. The IDKL has evolved through a real-life project with an industrial partner; the study employed ethnographic action research with 231 participants and had lasted for almost 3 years. The IDKL requires employees to continuously apply small improvements to the daily routines of the work-procedures. The small improvements by themselves are unobtrusive. However, the IDKL has helped the industrial partner to implant continuous improvement as a daily habit. This has led to realizing sustainable and noticeable improvements. The findings show that on average, Lead Time has dropped by 46 percent, Process Cycle Efficiency has increased by 137 percent, First-Pass Process Yield has increased by 27 percent, and Customer Satisfaction has increased by 25 percent.",
        "keywords": [
            "Continuous improvement",
            "Research and development",
            "Standards organizations",
            "Learning systems"
        ]
    },
    {
        "title": "Automating Change-Level Self-Admitted Technical Debt Determination.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2831232",
        "volume": "45",
        "abstract": "Technical debt (TD) is a metaphor to describe the situation where developers introduce suboptimal solutions during software development to achieve short-term goals that may affect the long-term software quality. Prior studies proposed different techniques to identify TD, such as identifying TD through code smells or by analyzing source code comments. Technical debt identified using comments is known as Self-Admitted Technical Debt (SATD) and refers to TD that is introduced intentionally. Compared with TD identified by code metrics or code smells, SATD is more reliable since it is admitted by developers using comments. Thus far, all of the state-of-the-art approaches identify SATD at the file-level. In essence, they identify whether a file has SATD or not. However, all of the SATD is introduced through software changes. Previous studies that identify SATD at the file-level in isolation cannot describe the TD context related to multiple files. Therefore, it is beneficial to identify the SATD once a change is being made. We refer to this type of TD identification as “Change-level SATD Determination”, which determines whether or not a change introduces SATD. Identifying SATD at the change-level can help to manage and control TD by understanding the TD context through tracing the introducing changes. To build a change-level SATD Determination model, we first identify TD from source code comments in source code files of all versions. Second, we label the changes that first introduce the SATD comments as TD-introducing changes. Third, we build the determination model by extracting 25 features from software changes that are divided into three dimensions, namely diffusion, history and message, respectively. To evaluate the effectiveness of our proposed model, we perform an empirical study on 7 open source projects containing a total of 100,011 software changes. The experimental results show that our model achieves a promising and better performance than four baselines in terms of AUC and cost-effectiveness (i.e., percentage of TD-introducing changes identified when inspecting 20 percent of changed LOC). On average across the 7 experimental projects, our model achieves AUC of 0.82, cost-effectiveness of 0.80, which is a significant improvement over the comparison baselines used. In addition, we found that “Diffusion” is the most discriminative dimension among the three dimensions of features for determining TD-introducing changes.",
        "keywords": [
            "Feature extraction",
            "Java",
            "Labeling",
            "Software quality"
        ]
    },
    {
        "title": "LEILA: Formal Tool for Identifying Mobile Malicious Behaviour.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2834344",
        "volume": "45",
        "abstract": "With the increasing diffusion of mobile technologies, nowadays mobile devices represent an irreplaceable tool to perform several operations, from posting a status on a social network to transfer money between bank accounts. As a consequence, mobile devices store a huge amount of private and sensitive information and this is the reason why attackers are developing very sophisticated techniques to extort data and money from our devices. This paper presents the design and the implementation of LEILA (formaL tool for idEntifying mobIle maLicious behAviour), a tool targeted at Android malware families detection. LEILA is based on a novel approach that exploits model checking to analyse and verify the Java Bytecode that is produced when the source code is compiled. After a thorough description of the method used for Android malware families detection, we report the experiments we have conducted using LEILA. The experiments demonstrated that the tool is effective in detecting malicious behaviour and, especially, in localizing the payload within the code: we evaluated real-world malware belonging to several widespread families obtaining an accuracy ranging between 0.97 and 1.",
        "keywords": [
            "Malware",
            "Androids",
            "Humanoid robots",
            "Payloads",
            "Computer security",
            "Model checking",
            "Automata"
        ]
    },
    {
        "title": "A Comprehensive Investigation of the Role of Imbalanced Learning for Software Defect Prediction.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2836442",
        "volume": "45",
        "abstract": "Context: Software defect prediction (SDP) is an important challenge in the field of software engineering, hence much research work has been conducted, most notably through the use of machine learning algorithms. However, class-imbalance typified by few defective components and many non-defective ones is a common occurrence causing difficulties for these methods. Imbalanced learning aims to deal with this problem and has recently been deployed by some researchers, unfortunately with inconsistent results. Objective: We conduct a comprehensive experiment to explore (a) the basic characteristics of this problem; (b) the effect of imbalanced learning and its interactions with (i) data imbalance, (ii) type of classifier, (iii) input metrics and (iv) imbalanced learning method. Method: We systematically evaluate 27 data sets, 7 classifiers, 7 types of input metrics and 17 imbalanced learning methods (including doing nothing) using an experimental design that enables exploration of interactions between these factors and individual imbalanced learning algorithms. This yields 27 × 7 × 7 × 17 = 22491 results. The Matthews correlation coefficient (MCC) is used as an unbiased performance measure (unlike the more widely used F1 and AUC measures). Results: (a) we found a large majority (87 percent) of 106 public domain data sets exhibit moderate or low level of imbalance (imbalance ratio <; 10; median = 3.94); (b) anything other than low levels of imbalance clearly harm the performance of traditional learning for SDP; (c) imbalanced learning is more effective on the data sets with moderate or higher imbalance, however negative results are always possible; (d) type of classifier has most impact on the improvement in classification performance followed by the imbalanced learning method itself. Type of input metrics is not influential. (e) only 52% of the combinations of Imbalanced Learner and Classifier have a significant positive effect. Conclusion: This paper offers two practical guidelines. First, imbalanced learning should only be considered for moderate or highly imbalanced SDP data sets. Second, the appropriate combination of imbalanced method and classifier needs to be carefully chosen to ameliorate the imbalanced learning problem for SDP. In contrast, the indiscriminate application of imbalanced learning can be harmful.",
        "keywords": [
            "Software measurement",
            "Boosting",
            "Machine learning algorithms",
            "Bagging",
            "Computer bugs"
        ]
    },
    {
        "title": "CHiP: A Configurable Hybrid Parallel Covering Array Constructor.",
        "venue_name": "tse",
        "year": 2019,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2018.2837759",
        "volume": "45",
        "abstract": "We present a configurable, hybrid, and parallel covering array constructor, called CHiP. CHiP is parallel in that it utilizes vast amount of parallelism provided by graphics processing units (GPUs). CHiP is hybrid in that it bundles the bests of two construction approaches for computing covering arrays; a metaheuristic search-based approach for efficiently covering a large portion of the required combinations and a constraint satisfaction-based approach for effectively covering the remaining hard-to-cover-by-chance combinations. CHiP is configurable in that a trade-off between covering array sizes and construction times can be made. We have conducted a series of experiments, in which we compared the efficiency and effectiveness of CHiP to those of a number of existing constructors by using both full factorial designs and well-known benchmarks. In these experiments, we report new upper bounds on covering array sizes, demonstrating the effectiveness of CHiP, and the first results for a higher coverage strength, demonstrating the scalability of CHiP.",
        "keywords": [
            "Simulated annealing",
            "Graphics processing units",
            "Parallel processing",
            "Benchmark testing",
            "Upper bound",
            "Scalability"
        ]
    }
]