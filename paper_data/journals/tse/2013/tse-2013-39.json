[
    {
        "title": "Ant Colony Optimization for Software Project Scheduling and Staffing with an Event-Based Scheduler.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.17",
        "volume": "39",
        "abstract": "Research into developing effective computer aided techniques for planning software projects is important and challenging for software engineering. Different from projects in other fields, software projects are people-intensive activities and their related resources are mainly human resources. Thus, an adequate model for software project planning has to deal with not only the problem of project task scheduling but also the problem of human resource allocation. But as both of these two problems are difficult, existing models either suffer from a very large search space or have to restrict the flexibility of human resource allocation to simplify the model. To develop a flexible and effective model for software project planning, this paper develops a novel approach with an event-based scheduler (EBS) and an ant colony optimization (ACO) algorithm. The proposed approach represents a plan by a task list and a planned employee allocation matrix. In this way, both the issues of task scheduling and employee allocation can be taken into account. In the EBS, the beginning time of the project, the time when resources are released from finished tasks, and the time when employees join or leave the project are regarded as events. The basic idea of the EBS is to adjust the allocation of employees at events and keep the allocation unchanged at nonevents. With this strategy, the proposed method enables the modeling of resource conflict and task preemption and preserves the flexibility in human resource allocation. To solve the planning problem, an ACO algorithm is further designed. Experimental results on 83 instances demonstrate that the proposed method is very promising.",
        "keywords": [
            "Software",
            "Resource management",
            "Planning",
            "Humans",
            "Project management",
            "Job shop scheduling",
            "Search problems"
        ]
    },
    {
        "title": "Empirical Principles and an Industrial Case Study in Retrieving Equivalent Requirements via Natural Language Processing Techniques.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2011.122",
        "volume": "39",
        "abstract": "Though very important in software engineering, linking artifacts of the same type (clone detection) or different types (traceability recovery) is extremely tedious, error-prone, and effort-intensive. Past research focused on supporting analysts with techniques based on Natural Language Processing (NLP) to identify candidate links. Because many NLP techniques exist and their performance varies according to context, it is crucial to define and use reliable evaluation procedures. The aim of this paper is to propose a set of seven principles for evaluating the performance of NLP techniques in identifying equivalent requirements. In this paper, we conjecture, and verify, that NLP techniques perform on a given dataset according to both ability and the odds of identifying equivalent requirements correctly. For instance, when the odds of identifying equivalent requirements are very high, then it is reasonable to expect that NLP techniques will result in good performance. Our key idea is to measure this random factor of the specific dataset(s) in use and then adjust the observed performance accordingly. To support the application of the principles we report their practical application to a case study that evaluates the performance of a large number of NLP techniques for identifying equivalent requirements in the context of an Italian company in the defense and aerospace domain. The current application context is the evaluation of NLP techniques to identify equivalent requirements. However, most of the proposed principles seem applicable to evaluating any estimation technique aimed at supporting a binary decision (e.g., equivalent/nonequivalent), with the estimate in the range [0,1] (e.g., the similarity provided by the NLP), when the dataset(s) is used as a benchmark (i.e., testbed), independently of the type of estimator (i.e., requirements text) and of the estimation method (e.g., NLP).",
        "keywords": [
            "Natural language processing",
            "Context",
            "Semantics",
            "Measurement",
            "Matrix decomposition",
            "Monitoring",
            "Thesauri"
        ]
    },
    {
        "title": "Identifying and Summarizing Systematic Code Changes via Rule Inference.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.16",
        "volume": "39",
        "abstract": "Programmers often need to reason about how a program evolved between two or more program versions. Reasoning about program changes is challenging as there is a significant gap between how programmers think about changes and how existing program differencing tools represent such changes. For example, even though modification of a locking protocol is conceptually simple and systematic at a code level, diff extracts scattered text additions and deletions per file. To enable programmers to reason about program differences at a high level, this paper proposes a rule-based program differencing approach that automatically discovers and represents systematic changes as logic rules. To demonstrate the viability of this approach, we instantiated this approach at two different abstraction levels in Java: first at the level of application programming interface (API) names and signatures, and second at the level of code elements (e.g., types, methods, and fields) and structural dependences (e.g., method-calls, field-accesses, and subtyping relationships). The benefit of this approach is demonstrated through its application to several open source projects as well as a focus group study with professional software engineers from a large e-commerce company.",
        "keywords": [
            "Systematics",
            "Syntactics",
            "Inference algorithms",
            "Cloning",
            "Software",
            "Semantics",
            "Libraries"
        ]
    },
    {
        "title": "Language-Independent and Automated Software Composition: The FeatureHouse Experience.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2011.120",
        "volume": "39",
        "abstract": "Superimposition is a composition technique that has been applied successfully in many areas of software development. Although superimposition is a general-purpose concept, it has been (re)invented and implemented individually for various kinds of software artifacts. We unify languages and tools that rely on superimposition by using the language-independent model of feature structure trees (FSTs). On the basis of the FST model, we propose a general approach to the composition of software artifacts written in different languages. Furthermore, we offer a supporting framework and tool chain, called FEATUREHOUSE. We use attribute grammars to automate the integration of additional languages. In particular, we have integrated Java, C#, C, Haskell, Alloy, and JavaCC. A substantial number of case studies demonstrate the practicality and scalability of our approach and reveal insights into the properties that a language must have in order to be ready for superimposition. We discuss perspectives of our approach and demonstrate how we extended FEATUREHOUSE with support for XML languages (in particular, XHTML, XMI/UML, and Ant) and alternative composition approaches (in particular, aspect weaving). Rounding off our previous work, we provide here a holistic view of the FEATUREHOUSE approach based on rich experience with numerous languages and case studies and reflections on several years of research.",
        "keywords": [
            "Software",
            "Java",
            "Grammar",
            "Databases",
            "Printers",
            "Latches",
            "Unified modeling language"
        ]
    },
    {
        "title": "On Fault Representativeness of Software Fault Injection.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2011.124",
        "volume": "39",
        "abstract": "The injection of software faults in software components to assess the impact of these faults on other components or on the system as a whole, allowing the evaluation of fault tolerance, is relatively new compared to decades of research on hardware fault injection. This paper presents an extensive experimental study (more than 3.8 million individual experiments in three real systems) to evaluate the representativeness of faults injected by a state-of-the-art approach (G-SWFIT). Results show that a significant share (up to 72 percent) of injected faults cannot be considered representative of residual software faults as they are consistently detected by regression tests, and that the representativeness of injected faults is affected by the fault location within the system, resulting in different distributions of representative/nonrepresentative faults across files and functions. Therefore, we propose a new approach to refine the faultload by removing faults that are not representative of residual software faults. This filtering is essential to assure meaningful results and to reduce the cost (in terms of number of faults) of software fault injection campaigns in complex software. The proposed approach is based on classification algorithms, is fully automatic, and can be used for improving fault representativeness of existing software fault injection approaches.",
        "keywords": [
            "Software",
            "Testing",
            "Fault tolerance",
            "Fault tolerant systems",
            "Hardware",
            "Fault location",
            "Emulation"
        ]
    },
    {
        "title": "Performance Specification and Evaluation with Unified Stochastic Probes and Fluid Analysis.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.1",
        "volume": "39",
        "abstract": "Rapid and accessible performance evaluation of complex software systems requires two critical features: the ability to specify useful performance metrics easily and the capability to analyze massively distributed architectures, without recourse to large compute clusters. We present the unified stochastic probe, a performance specification mechanism for process algebra models that combines many existing ideas: state and action-based activation, location-based specification, many-probe specification, and immediate signaling. These features, between them, allow the precise and compositional construction of complex performance measurements. The paper shows how a subset of the stochastic probe language can be used to specify common response-time measures in massive process algebra models. The second contribution of the paper is to show how these response-time measures can be analyzed using so-called fluid techniques to produce rapid results. In doing this, we extend the fluid approach to incorporate immediate activities and a new type of response-time measure. Finally, we calculate various response-time measurements on a complex distributed wireless network of O(10^{129}) states in size.",
        "keywords": [
            "Probes",
            "Stochastic processes",
            "Analytical models",
            "Algebra",
            "Computational modeling",
            "Semantics",
            "Syntactics"
        ]
    },
    {
        "title": "Systematic Elaboration of Scalability Requirements through Goal-Obstacle Analysis.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.12",
        "volume": "39",
        "abstract": "Scalability is a critical concern for many software systems. Despite the recognized importance of considering scalability from the earliest stages of development, there is currently little support for reasoning about scalability at the requirements level. This paper presents a goal-oriented approach for eliciting, modeling, and reasoning about scalability requirements. The approach consists of systematically identifying scalability-related obstacles to the satisfaction of goals, assessing the likelihood and severity of these obstacles, and generating new goals to deal with them. The result is a consolidated set of requirements in which important scalability concerns are anticipated through the precise, quantified specification of scaling assumptions and scalability goals. The paper presents results from applying the approach to a complex, large-scale financial fraud detection system.",
        "keywords": [
            "Scalability",
            "Software",
            "Batch production systems",
            "Educational institutions",
            "Analytical models",
            "Natural languages"
        ]
    },
    {
        "title": "Editorial: State of the Journal.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.5",
        "volume": "39",
        "abstract": null,
        "keywords": null
    },
    {
        "title": "Automated Behavioral Testing of Refactoring Engines.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.19",
        "volume": "39",
        "abstract": "Refactoring is a transformation that preserves the external behavior of a program and improves its internal quality. Usually, compilation errors and behavioral changes are avoided by preconditions determined for each refactoring transformation. However, to formally define these preconditions and transfer them to program checks is a rather complex task. In practice, refactoring engine developers commonly implement refactorings in an ad hoc manner since no guidelines are available for evaluating the correctness of refactoring implementations. As a result, even mainstream refactoring engines contain critical bugs. We present a technique to test Java refactoring engines. It automates test input generation by using a Java program generator that exhaustively generates programs for a given scope of Java declarations. The refactoring under test is applied to each generated program. The technique uses SafeRefactor, a tool for detecting behavioral changes, as an oracle to evaluate the correctness of these transformations. Finally, the technique classifies the failing transformations by the kind of behavioral change or compilation error introduced by them. We have evaluated this technique by testing 29 refactorings in Eclipse JDT, NetBeans, and the JastAdd Refactoring Tools. We analyzed 153,444 transformations, and identified 57 bugs related to compilation errors, and 63 bugs related to behavioral changes.",
        "keywords": [
            "Java",
            "Metals",
            "Engines",
            "Computer bugs",
            "Testing",
            "Automatic programming",
            "Unified modeling language"
        ]
    },
    {
        "title": "Centroidal Voronoi Tessellations - A New Approach to Random Testing.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.18",
        "volume": "39",
        "abstract": "Although Random Testing (RT) is low cost and straightforward, its effectiveness is not satisfactory. To increase the effectiveness of RT, researchers have developed Adaptive Random Testing (ART) and Quasi-Random Testing (QRT) methods which attempt to maximize the test case coverage of the input domain. This paper proposes the use of Centroidal Voronoi Tessellations (CVT) to address this problem. Accordingly, a test case generation method, namely, Random Border CVT (RBCVT), is proposed which can enhance the previous RT methods to improve their coverage of the input space. The generated test cases by the other methods act as the input to the RBCVT algorithm and the output is an improved set of test cases. Therefore, RBCVT is not an independent method and is considered as an add-on to the previous methods. An extensive simulation study and a mutant-based software testing investigation have been performed to demonstrate the effectiveness of RBCVT against the ART and QRT methods. Results from the experimental frameworks demonstrate that RBCVT outperforms previous methods. In addition, a novel search algorithm has been incorporated into RBCVT reducing the order of computational complexity of the new approach. To further analyze the RBCVT method, randomness analysis was undertaken demonstrating that RBCVT has the same characteristics as ART methods in this regard.",
        "keywords": [
            "Subspace constraints",
            "Software testing",
            "Generators",
            "Software algorithms",
            "Power capacitors",
            "Runtime"
        ]
    },
    {
        "title": "Class Schema Evolution for Persistent Object-Oriented Software: Model, Empirical Study, and Automated Support.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2011.123",
        "volume": "39",
        "abstract": "With the wide support for object serialization in object-oriented programming languages, persistent objects have become commonplace and most large object-oriented software systems rely on extensive amounts of persistent data. Such systems also evolve over time. Retrieving previously persisted objects from classes whose schema has changed is, however, difficult, and may lead to invalidating the consistency of the application. The ESCHER framework addresses these issues through an IDE-integrated approach that handles class schema evolution by managing versions of the code and generating transformation functions automatically. The infrastructure also enforces class invariants to prevent the introduction of potentially corrupt objects. This paper describes a model for class attribute changes, a measure for class evolution robustness, four empirical studies, and the design and implementation of the ESCHER system.",
        "keywords": [
            "Object oriented modeling",
            "Java",
            "Databases",
            "Software",
            "Robustness",
            "Dictionaries",
            "Atomic measurements"
        ]
    },
    {
        "title": "How Programmers Debug, Revisited: An Information Foraging Theory Perspective.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2010.111",
        "volume": "39",
        "abstract": "Many theories of human debugging rely on complex mental constructs that offer little practical advice to builders of software engineering tools. Although hypotheses are important in debugging, a theory of navigation adds more practical value to our understanding of how programmers debug. Therefore, in this paper, we reconsider how people go about debugging in large collections of source code using a modern programming environment. We present an information foraging theory of debugging that treats programmer navigation during debugging as being analogous to a predator following scent to find prey in the wild. The theory proposes that constructs of scent and topology provide enough information to describe and predict programmer navigation during debugging, without reference to mental states such as hypotheses. We investigate the scope of our theory through an empirical study of 10 professional programmers debugging a real-world open source program. We found that the programmers' verbalizations far more often concerned scent-following than hypotheses. To evaluate the predictiveness of our theory, we created an executable model that predicted programmer navigation behavior more accurately than comparable models that did not consider information scent. Finally, we discuss the implications of our results for enhancing software engineering tools.",
        "keywords": [
            "Debugging",
            "Navigation",
            "Topology",
            "Programming environments",
            "Predictive models",
            "Approximation methods"
        ]
    },
    {
        "title": "Integer Linear Programming-Based Property Checking for Asynchronous Reactive Systems.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2011.1",
        "volume": "39",
        "abstract": "Asynchronous reactive systems form the basis of a wide range of software systems, for instance in the telecommunications domain. It is highly desirable to rigorously show that these systems are correctly designed. However, traditional formal approaches to the verification of these systems are often difficult because asynchronous reactive systems usually possess extremely large or even infinite state spaces. We propose an integer linear program (ILP) solving-based property checking framework that concentrates on the local analysis of the cyclic behavior of each individual component of a system. We apply our framework to the checking of the buffer boundedness and livelock freedom properties, both of which are undecidable for asynchronous reactive systems with an infinite state space. We illustrate the application of the proposed checking methods to Promela, the input language of the SPIN model checker. While the precision of our framework remains an issue, we propose a counterexample guided abstraction refinement procedure based on the discovery of dependences among control flow cycles. We have implemented prototype tools with which we obtained promising experimental results on real-life system models.",
        "keywords": [
            "Unified modeling language",
            "Complexity theory",
            "Analytical models",
            "Message passing",
            "Integer linear programming",
            "Mathematical model",
            "Cost accounting"
        ]
    },
    {
        "title": "Toward Comprehensible Software Fault Prediction Models Using Bayesian Network Classifiers.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.20",
        "volume": "39",
        "abstract": "Software testing is a crucial activity during software development and fault prediction models assist practitioners herein by providing an upfront identification of faulty software code by drawing upon the machine learning literature. While especially the Naive Bayes classifier is often applied in this regard, citing predictive performance and comprehensibility as its major strengths, a number of alternative Bayesian algorithms that boost the possibility of constructing simpler networks with fewer nodes and arcs remain unexplored. This study contributes to the literature by considering 15 different Bayesian Network (BN) classifiers and comparing them to other popular machine learning techniques. Furthermore, the applicability of the Markov blanket principle for feature selection, which is a natural extension to BN theory, is investigated. The results, both in terms of the AUC and the recently introduced H-measure, are rigorously tested using the statistical framework of Demšar. It is concluded that simple and comprehensible networks with less nodes can be constructed using BN classifiers other than the Naive Bayes classifier. Furthermore, it is found that the aspects of comprehensibility and predictive performance need to be balanced out, and also the development context is an item which should be taken into account during model selection.",
        "keywords": [
            "Software",
            "Predictive models",
            "Bayesian methods",
            "Measurement",
            "Capability maturity model",
            "Probability distribution",
            "Machine learning"
        ]
    },
    {
        "title": "Using Dependency Structures for Prioritization of Functional Test Suites.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.26",
        "volume": "39",
        "abstract": "Test case prioritization is the process of ordering the execution of test cases to achieve a certain goal, such as increasing the rate of fault detection. Increasing the rate of fault detection can provide earlier feedback to system developers, improving fault fixing activity and, ultimately, software delivery. Many existing test case prioritization techniques consider that tests can be run in any order. However, due to functional dependencies that may exist between some test cases-that is, one test case must be executed before another-this is often not the case. In this paper, we present a family of test case prioritization techniques that use the dependency information from a test suite to prioritize that test suite. The nature of the techniques preserves the dependencies in the test ordering. The hypothesis of this work is that dependencies between tests are representative of interactions in the system under test, and executing complex interactions earlier is likely to increase the fault detection rate, compared to arbitrary test orderings. Empirical evaluations on six systems built toward industry use demonstrate that these techniques increase the rate of fault detection compared to the rates achieved by the untreated order, random orders, and test suites ordered using existing \"coarse-grained” techniques based on function coverage.",
        "keywords": [
            "Fault detection",
            "Digital signal processing",
            "Schedules",
            "Software",
            "Complexity theory",
            "Testing",
            "Weight measurement"
        ]
    },
    {
        "title": "Whole Test Suite Generation.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.14",
        "volume": "39",
        "abstract": "Not all bugs lead to program crashes, and not always is there a formal specification to check the correctness of a software test's outcome. A common scenario in software testing is therefore that test data are generated, and a tester manually adds test oracles. As this is a difficult task, it is important to produce small yet representative test sets, and this representativeness is typically measured using code coverage. There is, however, a fundamental problem with the common approach of targeting one coverage goal at a time: Coverage goals are not independent, not equally difficult, and sometimes infeasible—the result of test generation is therefore dependent on the order of coverage goals and how many of them are feasible. To overcome this problem, we propose a novel paradigm in which whole test suites are evolved with the aim of covering all coverage goals at the same time while keeping the total size as small as possible. This approach has several advantages, as for example, its effectiveness is not affected by the number of infeasible targets in the code. We have implemented this novel approach in the EvoSuite tool, and compared it to the common approach of addressing one goal at a time. Evaluated on open source libraries and an industrial case study for a total of 1,741 classes, we show that EvoSuite achieved up to 188 times the branch coverage of a traditional approach targeting single branches, with up to 62 percent smaller test suites.",
        "keywords": [
            "Software",
            "Genetic algorithms",
            "Search problems",
            "Arrays",
            "Genetic programming",
            "Software testing"
        ]
    },
    {
        "title": "A Quantitative Approach to Input Generation in Real-Time Testing of Stochastic Systems.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.42",
        "volume": "39",
        "abstract": "In the process of testing of concurrent timed systems, input generation identifies values of temporal parameters that let the Implementation Under Test (IUT) execute selected cases. However, when some parameters are not under control of the driver, test execution may diverge from the selected input and produce an inconclusive behavior. We formulate the problem on the basis of an abstraction of the IUT which we call partially stochastic Time Petri Net (psTPN), where controllable parameters are modeled as nondeterministic values and noncontrollable parameters as random variables with general (GEN) distribution. With reference to this abstraction, we derive the analytical form of the probability that the IUT runs along a selected behavior as a function of choices taken on controllable parameters. In the applicative perspective of real-time testing, this identifies a theoretical upper limit on the probability of a conclusive result, thus providing a means to plan the number of test repetitions that are necessary to guarantee a given probability of test-case coverage. It also provides a constructive technique for an optimal or suboptimal approach to input generation and a way to characterize the probability of conclusive testing under other suboptimal strategies.",
        "keywords": [
            "Stochastic processes",
            "Timing",
            "Real time systems",
            "Testing",
            "Tin",
            "Vectors",
            "Automata"
        ]
    },
    {
        "title": "Alloy Meets the Algebra of Programming: A Case Study.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.15",
        "volume": "39",
        "abstract": "Relational algebra offers to software engineering the same degree of conciseness and calculational power as linear algebra in other engineering disciplines. Binary relations play the role of matrices with similar emphasis on multiplication and transposition. This matches with Alloy's lemma “everything is a relation” and with the relational basis of the Algebra of Programming (AoP). Altogether, it provides a simple and coherent approach to checking and calculating programs from abstract models. In this paper, we put Alloy and the Algebra of Programming together in a case study originating from the Verifiable File System mini-challenge put forward by Joshi and Holzmann: verifying the refinement of an abstract file store model into a journaled (Flash) data model catering to wear leveling and recovery from power loss. Our approach relies on diagrams to graphically express typed assertions. It interweaves model checking (in Alloy) with calculational proofs in a way which offers the best of both worlds. This provides ample evidence of the positive impact in software verification of Alloy's focus on relations, complemented by induction-free proofs about data structures such as stores and lists.",
        "keywords": [
            "Metals",
            "Software",
            "Programming",
            "Matrices",
            "Calculus",
            "Cognition"
        ]
    },
    {
        "title": "Assessing the Effectiveness of Sequence Diagrams in the Comprehension of Functional Requirements: Results from a Family of Five Experiments.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.27",
        "volume": "39",
        "abstract": "Modeling is a fundamental activity within the requirements engineering process and concerns the construction of abstract descriptions of requirements that are amenable to interpretation and validation. The choice of a modeling technique is critical whenever it is necessary to discuss the interpretation and validation of requirements. This is particularly true in the case of functional requirements and stakeholders with divergent goals and different backgrounds and experience. This paper presents the results of a family of experiments conducted with students and professionals to investigate whether the comprehension of functional requirements is influenced by the use of dynamic models that are represented by means of the UML sequence diagrams. The family contains five experiments performed in different locations and with 112 participants of different abilities and levels of experience with UML. The results show that sequence diagrams improve the comprehension of the modeled functional requirements in the case of high ability and more experienced participants.",
        "keywords": [
            "Unified modeling language",
            "Object oriented modeling",
            "Analytical models",
            "Computational modeling",
            "Software systems",
            "Materials"
        ]
    },
    {
        "title": "Coordination Breakdowns and Their Impact on Development Productivity and Software Failures.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.32",
        "volume": "39",
        "abstract": "The success of software development projects depends on carefully coordinating the effort of many individuals across the multiple stages of the development process. In software engineering, modularization is the traditional technique intended to reduce the interdependencies among modules that constitute a system. Reducing technical dependencies, the theory argues, results in a reduction of work dependencies between teams developing interdependent modules. Although that research stream has been quite influential, it considers a static view of the problem of coordination in engineering activities. Building on a dynamic view of coordination, we studied the relationship between socio-technical congruence and software quality and development productivity. In order to investigate the generality of our findings, our analyses were performed on two large-scale projects from two companies with distinct characteristics in terms of product and process maturity. Our results revealed that the gaps between coordination requirements and the actual coordination activities carried out by the developers significantly increased software failures. Our analyses also showed that higher levels of congruence are associated with improved development productivity. Finally, our results showed the congruence between dependencies and coordinative actions is critical both in mature development settings as well as in novel and dynamic development contexts.",
        "keywords": [
            "Productivity",
            "Programming",
            "Software quality",
            "Context",
            "Complexity theory",
            "Organizations"
        ]
    },
    {
        "title": "Elaborating Requirements Using Model Checking and Inductive Learning.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.41",
        "volume": "39",
        "abstract": "The process of Requirements Engineering (RE) includes many activities, from goal elicitation to requirements specification. The aim is to develop an operational requirements specification that is guaranteed to satisfy the goals. In this paper, we propose a formal, systematic approach for generating a set of operational requirements that are complete with respect to given goals. We show how the integration of model checking and inductive learning can be effectively used to do this. The model checking formally verifies the satisfaction of the goals and produces counterexamples when incompleteness in the operational requirements is detected. The inductive learning process then computes operational requirements from the counterexamples and user-provided positive examples. These learned operational requirements are guaranteed to eliminate the counterexamples and be consistent with the goals. This process is performed iteratively until no goal violation is detected. The proposed framework is a rigorous, tool-supported requirements elaboration technique which is formally guided by the engineer's knowledge of the domain and the envisioned system.",
        "keywords": [
            "Wheels",
            "Computational modeling",
            "Software",
            "Adaptation models",
            "Calculus",
            "Switches",
            "Semantics"
        ]
    },
    {
        "title": "Resource Management for Complex, Dynamic Environments.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.31",
        "volume": "39",
        "abstract": "This paper describes an approach to the specification and management of the agents and resources that are required to support the execution of complex systems and processes. The paper suggests that a resource should be viewed as a provider of a set of capabilities that are needed by a system or process, where that set may vary dynamically over time and with circumstances. This view of resources is defined and then made the basis for the framework of an approach to specifying, managing, and allocating resources in the presence of real-world complexity and dynamism. The ROMEO prototype resource management system is presented as an example of how this framework can be instantiated. Some case studies of the use of ROMEO to support system execution are presented and used to evaluate the framework, the ROMEO prototype, and our view of the nature of resources.",
        "keywords": [
            "Resource management",
            "Hospitals",
            "Surgery",
            "Software",
            "Context",
            "Databases"
        ]
    },
    {
        "title": "Self-Management of Adaptable Component-Based Applications.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.29",
        "volume": "39",
        "abstract": "The problem of self-optimization and adaptation in the context of customizable systems is becoming increasingly important with the emergence of complex software systems and unpredictable execution environments. Here, a general framework for automatically deciding on when and how to adapt a system whenever it deviates from the desired behavior is presented. In this framework, the system's target behavior is described as a high-level policy that establishes goals for a set of performance indicators. The decision process is based on information provided independently for each component that describes the available adaptations, their impact on performance indicators, and any limitations or requirements. The technique consists of both offline and online phases. Offline, rules are generated specifying component adaptations that may help to achieve the established goals when a given change in the execution context occurs. Online, the corresponding rules are evaluated when a change occurs to choose which adaptations to perform. Experimental results using a prototype framework in the context of a web-based application demonstrate the effectiveness of this approach.",
        "keywords": [
            "Runtime",
            "Context",
            "Software systems",
            "Optimization",
            "Catalogs"
        ]
    },
    {
        "title": "Self-Organizing Roles on Agile Software Development Teams.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.30",
        "volume": "39",
        "abstract": "Self-organizing teams have been recognized and studied in various forms-as autonomous groups in socio-technical systems, enablers of organizational theories, agents of knowledge management, and as examples of complex-adaptive systems. Over the last decade, self-organizing teams have taken center stage in software engineering when they were incorporated as a hallmark of Agile methods. Despite the long and rich history of self-organizing teams and their recent popularity with Agile methods, there has been little research on the topic within software wngineering. Particularly, there is a dearth of research on how Agile teams organize themselves in practice. Through a Grounded Theory research involving 58 Agile practitioners from 23 software organizations in New Zealand and India over a period of four years, we identified informal, implicit, transient, and spontaneous roles that make Agile teams self-organizing. These roles-Mentor, Coordinator, Translator, Champion, Promoter, and Terminator-are focused toward providing initial guidance and encouraging continued adherence to Agile methods, effectively managing customer expectations and coordinating customer collaboration, securing and sustaining senior management support, and identifying and removing team members threatening the self-organizing ability of the team. Understanding these roles will help software development teams and their managers better comprehend and execute their roles and responsibilities as a self-organizing team.",
        "keywords": [
            "Programming",
            "Organizations",
            "Collaboration",
            "Software",
            "Organizing",
            "Software engineering"
        ]
    },
    {
        "title": "A Machine Learning Approach to Software Requirements Prioritization.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.52",
        "volume": "39",
        "abstract": "Deciding which, among a set of requirements, are to be considered first and in which order is a strategic process in software development. This task is commonly referred to as requirements prioritization. This paper describes a requirements prioritization method called Case-Based Ranking (CBRank), which combines project's stakeholders preferences with requirements ordering approximations computed through machine learning techniques, bringing promising advantages. First, the human effort to input preference information can be reduced, while preserving the accuracy of the final ranking estimates. Second, domain knowledge encoded as partial order relations defined over the requirement attributes can be exploited, thus supporting an adaptive elicitation process. The techniques CBRank rests on and the associated prioritization process are detailed. Empirical evaluations of properties of CBRank are performed on simulated data and compared with a state-of-the-art prioritization method, providing evidence of the method ability to support the management of the tradeoff between elicitation effort and ranking accuracy and to exploit domain knowledge. A case study on a real software project complements these experimental measurements. Finally, a positioning of CBRank with respect to state-of-the-art requirements prioritization methods is proposed, together with a discussion of benefits and limits of the method.",
        "keywords": [
            "Approximation methods",
            "Accuracy",
            "Software",
            "Humans",
            "Data models",
            "Boosting"
        ]
    },
    {
        "title": "A Second Replicated Quantitative Analysis of Fault Distributions in Complex Software Systems.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.46",
        "volume": "39",
        "abstract": "Background: Software engineering is searching for general principles that apply across contexts, for example, to help guide software quality assurance. Fenton and Ohlsson presented such observations on fault distributions, which have been replicated once. Objectives: We aimed to replicate their study again to assess the robustness of the findings in a new environment, five years later. Method: We conducted a literal replication, collecting defect data from five consecutive releases of a large software system in the telecommunications domain, and conducted the same analysis as in the original study. Results: The replication confirms results on unevenly distributed faults over modules, and that fault proneness distributions persist over test phases. Size measures are not useful as predictors of fault proneness, while fault densities are of the same order of magnitude across releases and contexts. Conclusions: This replication confirms that the uneven distribution of defects motivates uneven distribution of quality assurance efforts, although predictors for such distribution of efforts are not sufficiently precise.",
        "keywords": [
            "Testing",
            "Measurement",
            "Context",
            "Software",
            "Software engineering",
            "Complexity theory",
            "Telecommunications"
        ]
    },
    {
        "title": "Coverage Estimation in Model Checking with Bitstate Hashing.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.44",
        "volume": "39",
        "abstract": "Explicit-state model checking which is conducted by state space search has difficulty in exploring satisfactory state space because of its memory requirements. Though bitstate hashing achieves memory efficiency, it cannot guarantee complete verification. Thus, it is desirable to provide a reliability indicator such as a coverage estimate. However, the existing approaches for coverage estimation are not very accurate when a verification run covers a small portion of state space. This mainly stems from the lack of information that reflects characteristics of models. Therefore, we propose coverage estimation methods using a growth curve that approximates an increase in reached states by enlarging a bloom filter. Our approaches improve estimation accuracy by leveraging the statistics from multiple verification runs. Coverage is estimated by fitting the growth curve to these statistics. Experimental results confirm the validity of the proposed growth curve and the applicability of our approaches to practical models. In fact, for practical models, our approaches outperformed the conventional ones when the actual coverage is relatively low.",
        "keywords": [
            "Estimation",
            "Reliability",
            "Probabilistic logic",
            "Accuracy",
            "Mathematical model",
            "Space exploration",
            "Equations"
        ]
    },
    {
        "title": "Generating Domain-Specific Visual Language Tools from Abstract Visual Specifications.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.33",
        "volume": "39",
        "abstract": "Domain-specific visual languages support high-level modeling for a wide range of application domains. However, building tools to support such languages is very challenging. We describe a set of key conceptual requirements for such tools and our approach to addressing these requirements, a set of visual language-based metatools. These support definition of metamodels, visual notations, views, modeling behaviors, design critics, and model transformations and provide a platform to realize target visual modeling tools. Extensions support collaborative work, human-centric tool interaction, and multiplatform deployment. We illustrate application of the metatoolset on tools developed with our approach. We describe tool developer and cognitive evaluations of our platform and our exemplar tools, and summarize key future research directions.",
        "keywords": [
            "Visualization",
            "Unified modeling language",
            "Software",
            "Computational modeling",
            "Business",
            "Abstracts",
            "Electronic mail"
        ]
    },
    {
        "title": "Locating Need-to-Externalize Constant Strings for Software Internationalization with Generalized String-Taint Analysis.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.40",
        "volume": "39",
        "abstract": "Nowadays, a software product usually faces a global market. To meet the requirements of different local users, the software product must be internationalized. In an internationalized software product, user-visible hard-coded constant strings are externalized to resource files so that local versions can be generated by translating the resource files. In many cases, a software product is not internationalized at the beginning of the software development process. To internationalize an existing product, the developers must locate the user-visible constant strings that should be externalized. This locating process is tedious and error-prone due to 1) the large number of both user-visible and non-user-visible constant strings and 2) the complex data flows from constant strings to the Graphical User Interface (GUI). In this paper, we propose an automatic approach to locating need-to-externalize constant strings in the source code of a software product. Given a list of precollected API methods that output values of their string argument variables to the GUI and the source code of the software product under analysis, our approach traces from the invocation sites (within the source code) of these methods back to the need-to-externalize constant strings using generalized string-taint analysis. In our empirical evaluation, we used our approach to locate need-to-externalize constant strings in the uninternationalized versions of seven real-world open source software products. The results of our evaluation demonstrate that our approach is able to effectively locate need-to-externalize constant strings in uninternationalized software products. Furthermore, to help developers understand why a constant string requires translation and properly translate the need-to-externalize strings, we provide visual representation of the string dependencies related to the need-to-externalize strings.",
        "keywords": [
            "Software",
            "Graphical user interfaces",
            "Prototypes",
            "Java",
            "Libraries",
            "Production",
            "Globalization"
        ]
    },
    {
        "title": "Ranking and Clustering Software Cost Estimation Models through a Multiple Comparisons Algorithm.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.45",
        "volume": "39",
        "abstract": "Software Cost Estimation can be described as the process of predicting the most realistic effort required to complete a software project. Due to the strong relationship of accurate effort estimations with many crucial project management activities, the research community has been focused on the development and application of a vast variety of methods and models trying to improve the estimation procedure. From the diversity of methods emerged the need for comparisons to determine the best model. However, the inconsistent results brought to light significant doubts and uncertainty about the appropriateness of the comparison process in experimental studies. Overall, there exist several potential sources of bias that have to be considered in order to reinforce the confidence of experiments. In this paper, we propose a statistical framework based on a multiple comparisons algorithm in order to rank several cost estimation models, identifying those which have significant differences in accuracy, and clustering them in nonoverlapping groups. The proposed framework is applied in a large-scale setup of comparing 11 prediction models over six datasets. The results illustrate the benefits and the significant information obtained through the systematic comparison of alternative methods.",
        "keywords": [
            "Predictive models",
            "Estimation",
            "Accuracy",
            "Measurement uncertainty",
            "Prediction algorithms",
            "Clustering algorithms",
            "Systematics"
        ]
    },
    {
        "title": "Reducing Features to Improve Code Change-Based Bug Prediction.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.43",
        "volume": "39",
        "abstract": "Machine learning classifiers have recently emerged as a way to predict the introduction of bugs in changes made to source code files. The classifier is first trained on software history, and then used to predict if an impending change causes a bug. Drawbacks of existing classifier-based bug prediction techniques are insufficient performance for practical use and slow prediction times due to a large number of machine learned features. This paper investigates multiple feature selection techniques that are generally applicable to classification-based bug prediction methods. The techniques discard less important features until optimal classification performance is reached. The total number of features used for training is substantially reduced, often to less than 10 percent of the original. The performance of Naive Bayes and Support Vector Machine (SVM) classifiers when using this technique is characterized on 11 software projects. Naive Bayes using feature selection provides significant improvement in buggy F-measure (21 percent improvement) over prior change classification bug prediction results (by the second and fourth authors [28]). The SVM's improvement in buggy F-measure is 9 percent. Interestingly, an analysis of performance for varying numbers of features shows that strong performance is achieved at even 1 percent of the original number of features.",
        "keywords": [
            "Software",
            "Support vector machines",
            "History",
            "Machine learning",
            "Feature extraction",
            "Measurement",
            "Computer bugs"
        ]
    },
    {
        "title": "Validating Second-Order Mutation at System Level.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.39",
        "volume": "39",
        "abstract": "Mutation has been recognized to be an effective software testing technique. It is based on the insertion of artificial faults in the system under test (SUT) by means of a set of mutation operators. Different operators can mutate each program statement in several ways, which may produce a huge number of mutants. This leads to very high costs for test case execution and result analysis. Several works have approached techniques for cost reduction in mutation testing, like n-order mutation where each mutant contains n artificial faults instead of one. There are two approaches to n-order mutation: increasing the effectiveness of mutation by searching for good n-order mutants, and decreasing the costs of mutation testing by reducing the mutants set through the combination of the first-order mutants into n-order mutants. This paper is focused on the second approach. However, this second use entails a risk: the possibility of leaving undiscovered faults in the SUT, which may distort the perception of the test suite quality. This paper describes an empirical study of different combination strategies to compose second-order mutants at system level as well as a cost-risk analysis of n-order mutation at system level.",
        "keywords": [
            "Algorithm design and analysis",
            "Concrete",
            "Educational institutions",
            "Benchmark testing",
            "Optimization",
            "Software testing"
        ]
    },
    {
        "title": "Editorial [new associate editors].",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.22",
        "volume": "39",
        "abstract": "It is the Editor-in-Chief's (EiC's) pleasure to welcome a number of new associate editors to the editorial board of the IEEE Transactions on Software Engineering. They are: Luciano Baresi, Daniela Damian, Robert DeLine, Audris Mockus, Gail Murphy, Mauro Pezze, Gian Pietro Pico, Helen Sharp, and Paolo Tonella. They bring a wealth of expertise in a broad range of research areas within software engineering, consolidating traditional strengths in areas such as software testing, and strengthening areas such as empirical studies of software development, mobile computing, and adaptive systems. Short professional biographies are included. At the same time, the EiC would like to bid farewell to those associate editors whose terms of service have ended: Martin Robillard, Peggy Storey, and Tetsuo Tamai. He thanks them for their distinguished contributions over a number of years, and for continuing to handle submitted manuscripts already on their editorial stack.",
        "keywords": null
    },
    {
        "title": "A Decentralized Self-Adaptation Mechanism for Service-Based Applications in the Cloud.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.53",
        "volume": "39",
        "abstract": "Cloud computing, with its promise of (almost) unlimited computation, storage, and bandwidth, is increasingly becoming the infrastructure of choice for many organizations. As cloud offerings mature, service-based applications need to dynamically recompose themselves to self-adapt to changing QoS requirements. In this paper, we present a decentralized mechanism for such self-adaptation, using market-based heuristics. We use a continuous double-auction to allow applications to decide which services to choose, among the many on offer. We view an application as a multi-agent system and the cloud as a marketplace where many such applications self-adapt. We show through a simulation study that our mechanism is effective for the individual application as well as from the collective perspective of all applications adapting at the same time.",
        "keywords": [
            "Quality of service",
            "Pricing",
            "Reliability",
            "Resource management",
            "Measurement",
            "Adaptation models",
            "Cloud computing"
        ]
    },
    {
        "title": "Automated API Property Inference Techniques.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.63",
        "volume": "39",
        "abstract": "Frameworks and libraries offer reusable and customizable functionality through Application Programming Interfaces (APIs). Correctly using large and sophisticated APIs can represent a challenge due to hidden assumptions and requirements. Numerous approaches have been developed to infer properties of APIs, intended to guide their use by developers. With each approach come new definitions of API properties, new techniques for inferring these properties, and new ways to assess their correctness and usefulness. This paper provides a comprehensive survey of over a decade of research on automated property inference for APIs. Our survey provides a synthesis of this complex technical field along different dimensions of analysis: properties inferred, mining techniques, and empirical results. In particular, we derive a classification and organization of over 60 techniques into five different categories based on the type of API property inferred: unordered usage patterns, sequential usage patterns, behavioral specifications, migration mappings, and general information.",
        "keywords": [
            "Itemsets",
            "Context",
            "Association rules",
            "Protocols",
            "Programming",
            "Software engineering"
        ]
    },
    {
        "title": "Compositional Verification for Hierarchical Scheduling of Real-Time Systems.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.54",
        "volume": "39",
        "abstract": "Hierarchical Scheduling (HS) techniques achieve resource partitioning among a set of real-time applications, providing reduction of complexity, confinement of failure modes, and temporal isolation among system applications. This facilitates compositional analysis for architectural verification and plays a crucial role in all industrial areas where high-performance microprocessors allow growing integration of multiple applications on a single platform. We propose a compositional approach to formal specification and schedulability analysis of real-time applications running under a Time Division Multiplexing (TDM) global scheduler and preemptive Fixed Priority (FP) local schedulers, according to the ARINC-653 standard. As a characterizing trait, each application is made of periodic, sporadic, and jittering tasks with offsets, jitters, and nondeterministic execution times, encompassing intra-application synchronizations through semaphores and mailboxes and interapplication communications among periodic tasks through message passing. The approach leverages the assumption of a TDM partitioning to enable compositional design and analysis based on the model of preemptive Time Petri Nets (pTPNs), which is expressly extended with a concept of Required Interface (RI) that specifies the embedding environment of an application through sequencing and timing constraints. This enables exact verification of intra-application constraints and approximate but safe verification of interapplication constraints. Experimentation illustrates results and validates their applicability on two challenging workloads in the field of safety-critical avionic systems.",
        "keywords": [
            "Real time systems",
            "Complexity theory",
            "Time division multiplexing",
            "Job shop scheduling",
            "Timing",
            "Resource management",
            "Petri nets"
        ]
    },
    {
        "title": "Software Architecture Optimization Methods: A Systematic Literature Review.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.64",
        "volume": "39",
        "abstract": "Due to significant industrial demands toward software systems with increasing complexity and challenging quality requirements, software architecture design has become an important development activity and the research domain is rapidly evolving. In the last decades, software architecture optimization methods, which aim to automate the search for an optimal architecture design with respect to a (set of) quality attribute(s), have proliferated. However, the reported results are fragmented over different research communities, multiple system domains, and multiple quality attributes. To integrate the existing research results, we have performed a systematic literature review and analyzed the results of 188 research papers from the different research communities. Based on this survey, a taxonomy has been created which is used to classify the existing research. Furthermore, the systematic analysis of the research literature provided in this review aims to help the research community in consolidating the existing research efforts and deriving a research agenda for future developments.",
        "keywords": [
            "Taxonomy",
            "Computer architecture",
            "Software",
            "Software architecture",
            "Systematics",
            "Optimization methods"
        ]
    },
    {
        "title": "Test Case-Aware Combinatorial Interaction Testing.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.65",
        "volume": "39",
        "abstract": "The configuration spaces of modern software systems are too large to test exhaustively. Combinatorial interaction testing (CIT) approaches, such as covering arrays, systematically sample the configuration space and test only the selected configurations by using a battery of test cases. Traditional covering arrays, while taking system-wide interoption constraints into account, do not provide a systematic way of handling test case-specific interoption constraints. The basic justification for t-way covering arrays is that they can cost effectively exercise all system behaviors caused by the settings of t or fewer options. In this paper, we hypothesize, however, that in the presence of test case-specific interoption constraints, many such behaviors may not be tested due to masking effects caused by the overlooked test case-specific constraints. For example, if a test case refuses to run in a configuration due to an unsatisfied test case-specific constraint, none of the valid option setting combinations appearing in the configuration will be tested by that test case. To account for test case-specific constraints, we introduce a new combinatorial object, called a test case-aware covering array. A t-way test case-aware covering array is not just a set of configurations, as is the case in traditional covering arrays, but a set of configurations, each of which is associated with a set of test cases such that all test case-specific constraints are satisfied and that, for each test case, each valid combination of option settings for every combination of t options appears at least once in the set of configurations that the test case is associated with. We furthermore present three algorithms to compute test case-aware covering arrays. Two of the algorithms aim to minimize the number of configurations required (one is fast, but produces larger arrays, the other is slower, but produces smaller arrays), whereas the remaining algorithm aims to minimize the number of test runs required. The results of our empirical studies conducted on two widely used highly configurable software systems suggest that test case-specific constraints do exist in practice, that traditional covering arrays suffer from masking effects caused by ignorance of such constraints, and that test case-aware covering arrays are better than other approaches in handling test case-specific constraints, thus avoiding masking effects.",
        "keywords": [
            "Testing",
            "Servers",
            "Software algorithms",
            "Software systems",
            "Systematics",
            "Computational modeling"
        ]
    },
    {
        "title": "The Role of the Tester's Knowledge in Exploratory Software Testing.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.55",
        "volume": "39",
        "abstract": "We present a field study on how testers use knowledge while performing exploratory software testing (ET) in industrial settings. We video recorded 12 testing sessions in four industrial organizations, having our subjects think aloud while performing their usual functional testing work. Using applied grounded theory, we analyzed how the subjects performed tests and what type of knowledge they utilized. We discuss how testers recognize failures based on their personal knowledge without detailed test case descriptions. The knowledge is classified under the categories of domain knowledge, system knowledge, and general software engineering knowledge. We found that testers applied their knowledge either as a test oracle to determine whether a result was correct or not, or for test design, to guide them in selecting objects for test and designing tests. Interestingly, a large number of failures, windfall failures, were found outside the actual focus areas of testing as a result of exploratory investigation. We conclude that the way exploratory testers apply their knowledge for test design and failure recognition differs clearly from the test-case-based paradigm and is one of the explanatory factors of the effectiveness of the exploratory testing approach.",
        "keywords": [
            "Software testing",
            "Context",
            "Software",
            "Knowledge engineering",
            "Observers",
            "Organizations"
        ]
    },
    {
        "title": "Trustrace: Mining Software Repositories to Improve the Accuracy of Requirement Traceability Links.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.71",
        "volume": "39",
        "abstract": "Traceability is the only means to ensure that the source code of a system is consistent with its requirements and that all and only the specified requirements have been implemented by developers. During software maintenance and evolution, requirement traceability links become obsolete because developers do not/cannot devote effort to updating them. Yet, recovering these traceability links later is a daunting and costly task for developers. Consequently, the literature has proposed methods, techniques, and tools to recover these traceability links semi-automatically or automatically. Among the proposed techniques, the literature showed that information retrieval (IR) techniques can automatically recover traceability links between free-text requirements and source code. However, IR techniques lack accuracy (precision and recall). In this paper, we show that mining software repositories and combining mined results with IR techniques can improve the accuracy (precision and recall) of IR techniques and we propose Trustrace, a trust--based traceability recovery approach. We apply Trustrace on four medium-size open-source systems to compare the accuracy of its traceability links with those recovered using state-of-the-art IR techniques from the literature, based on the Vector Space Model and Jensen-Shannon model. The results of Trustrace are up to 22.7 percent more precise and have 7.66 percent better recall values than those of the other techniques, on average. We thus show that mining software repositories and combining the mined data with existing results from IR techniques improves the precision and recall of requirement traceability links.",
        "keywords": [
            "Accuracy",
            "Data mining",
            "Software maintenance",
            "Information retrieval",
            "Open source software",
            "Principal component analysis"
        ]
    },
    {
        "title": "A Fluid Model for Layered Queueing Networks.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.66",
        "volume": "39",
        "abstract": "Layered queueing networks are a useful tool for the performance modeling and prediction of software systems that exhibit complex characteristics such as multiple tiers of service, fork/join interactions, and asynchronous communication. These features generally result in nonproduct form behavior for which particularly efficient approximations based on mean value analysis (MVA) have been devised. This paper reconsiders the accuracy of such techniques by providing an interpretation of layered queueing networks as fluid models. Mediated by an automatic translation into a stochastic process algebra, PEPA, a network is associated with a set of ordinary differential equations (ODEs) whose size is insensitive to the population levels in the system under consideration. A substantial numerical assessment demonstrates that this approach significantly improves the quality of the approximation for typical performance indices such as utilization, throughput, and response time. Furthermore, backed by established theoretical results of asymptotic convergence, the error trend shows monotonic decrease with larger population sizes-a behavior which is found to be in sharp contrast with that of approximate mean value analysis, which instead tends to increase.",
        "keywords": [
            "Approximation methods",
            "Unified modeling language",
            "Stochastic processes",
            "Sociology",
            "Statistics",
            "Servers",
            "Accuracy"
        ]
    },
    {
        "title": "A Large-Scale Empirical Study of Just-in-Time Quality Assurance.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.70",
        "volume": "39",
        "abstract": "Defect prediction models are a well-known technique for identifying defect-prone files or packages such that practitioners can allocate their quality assurance efforts (e.g., testing and code reviews). However, once the critical files or packages have been identified, developers still need to spend considerable time drilling down to the functions or even code snippets that should be reviewed or tested. This makes the approach too time consuming and impractical for large software systems. Instead, we consider defect prediction models that focus on identifying defect-prone (“risky”) software changes instead of files or packages. We refer to this type of quality assurance activity as “Just-In-Time Quality Assurance,” because developers can review and test these risky changes while they are still fresh in their minds (i.e., at check-in time). To build a change risk model, we use a wide range of factors based on the characteristics of a software change, such as the number of added lines, and developer experience. A large-scale study of six open source and five commercial projects from multiple domains shows that our models can predict whether or not a change will lead to a defect with an average accuracy of 68 percent and an average recall of 64 percent. Furthermore, when considering the effort needed to review changes, we find that using only 20 percent of the effort it would take to inspect all changes, we can identify 35 percent of all defect-inducing changes. Our findings indicate that “Just-In-Time Quality Assurance” may provide an effort-reducing way to focus on the most risky changes and thus reduce the costs of developing high-quality software.",
        "keywords": [
            "Measurement",
            "Quality assurance",
            "Predictive models",
            "Software",
            "Entropy",
            "Object oriented modeling",
            "Accuracy"
        ]
    },
    {
        "title": "Abstracting Runtime Heaps for Program Understanding.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.69",
        "volume": "39",
        "abstract": "Modern programming environments provide extensive support for inspecting, analyzing, and testing programs based on the algorithmic structure of a program. Unfortunately, support for inspecting and understanding runtime data structures during execution is typically much more limited. This paper provides a general purpose technique for abstracting and summarizing entire runtime heaps. We describe the abstract heap model and the associated algorithms for transforming a concrete heap dump into the corresponding abstract model as well as algorithms for merging, comparing, and computing changes between abstract models. The abstract model is designed to emphasize high-level concepts about heap-based data structures, such as shape and size, as well as relationships between heap structures, such as sharing and connectivity. We demonstrate the utility and computational tractability of the abstract heap model by building a memory profiler. We use this tool to identify, pinpoint, and correct sources of memory bloat for programs from DaCapo.",
        "keywords": [
            "Abstracts",
            "Concrete",
            "Shape",
            "Runtime",
            "Arrays",
            "Computational modeling"
        ]
    },
    {
        "title": "An Empirical Evaluation of Mutation Testing for Improving the Test Quality of Safety-Critical Software.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.56",
        "volume": "39",
        "abstract": "Testing provides a primary means for assuring software in safety-critical systems. To demonstrate, particularly to a certification authority, that sufficient testing has been performed, it is necessary to achieve the test coverage levels recommended or mandated by safety standards and industry guidelines. Mutation testing provides an alternative or complementary method of measuring test sufficiency, but has not been widely adopted in the safety-critical industry. In this study, we provide an empirical evaluation of the application of mutation testing to airborne software systems which have already satisfied the coverage requirements for certification. Specifically, we apply mutation testing to safety-critical software developed using high-integrity subsets of C and Ada, identify the most effective mutant types, and analyze the root causes of failures in test cases. Our findings show how mutation testing could be effective where traditional structural coverage analysis and manual peer review have failed. They also show that several testing issues have origins beyond the test activity, and this suggests improvements to the requirements definition and coding process. Our study also examines the relationship between program characteristics and mutation survival and considers how program size can provide a means for targeting test areas most likely to have dormant faults. Industry feedback is also provided, particularly on how mutation testing can be integrated into a typical verification life cycle of airborne software.",
        "keywords": [
            "Testing",
            "Certification",
            "Software systems",
            "Safety",
            "Industries",
            "Guidelines"
        ]
    },
    {
        "title": "Event Logs for the Analysis of Software Failures: A Rule-Based Approach.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.67",
        "volume": "39",
        "abstract": "Event logs have been widely used over the last three decades to analyze the failure behavior of a variety of systems. Nevertheless, the implementation of the logging mechanism lacks a systematic approach and collected logs are often inaccurate at reporting software failures: This is a threat to the validity of log-based failure analysis. This paper analyzes the limitations of current logging mechanisms and proposes a rule-based approach to make logs effective to analyze software failures. The approach leverages artifacts produced at system design time and puts forth a set of rules to formalize the placement of the logging instructions within the source code. The validity of the approach, with respect to traditional logging mechanisms, is shown by means of around 12,500 software fault injection experiments into real-world systems.",
        "keywords": [
            "Unified modeling language",
            "Failure analysis",
            "Analytical models",
            "Systematics",
            "Proposals",
            "Software systems"
        ]
    },
    {
        "title": "Local versus Global Lessons for Defect Prediction and Effort Estimation.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.83",
        "volume": "39",
        "abstract": "Existing research is unclear on how to generate lessons learned for defect prediction and effort estimation. Should we seek lessons that are global to multiple projects or just local to particular projects? This paper aims to comparatively evaluate local versus global lessons learned for effort estimation and defect prediction. We applied automated clustering tools to effort and defect datasets from the PROMISE repository. Rule learners generated lessons learned from all the data, from local projects, or just from each cluster. The results indicate that the lessons learned after combining small parts of different data sources (i.e., the clusters) were superior to either generalizations formed over all the data or local lessons formed from particular projects. We conclude that when researchers attempt to draw lessons from some historical data source, they should 1) ignore any existing local divisions into multiple sources, 2) cluster across all available data, then 3) restrict the learning of lessons to the clusters from other sources that are nearest to the test data.",
        "keywords": [
            "Estimation",
            "Data models",
            "Context",
            "Java",
            "Telecommunications",
            "Measurement",
            "Software"
        ]
    },
    {
        "title": "The Effects of Test-Driven Development on External Quality and Productivity: A Meta-Analysis.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.28",
        "volume": "39",
        "abstract": "This paper provides a systematic meta-analysis of 27 studies that investigate the impact of Test-Driven Development (TDD) on external code quality and productivity. The results indicate that, in general, TDD has a small positive effect on quality but little to no discernible effect on productivity. However, subgroup analysis has found both the quality improvement and the productivity drop to be much larger in industrial studies in comparison with academic studies. A larger drop of productivity was found in studies where the difference in test effort between the TDD and the control group's process was significant. A larger improvement in quality was also found in the academic studies when the difference in test effort is substantial; however, no conclusion could be derived regarding the industrial studies due to the lack of data. Finally, the influence of developer experience and task size as moderator variables was investigated, and a statistically significant positive correlation was found between task size and the magnitude of the improvement in quality.",
        "keywords": [
            "Productivity",
            "Computational modeling",
            "Testing",
            "Process control",
            "Programming",
            "Size measurement"
        ]
    },
    {
        "title": "Using Timed Automata for Modeling Distributed Systems with Clocks: Challenges and Solutions.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.73",
        "volume": "39",
        "abstract": "The application of model checking for the formal verification of distributed embedded systems requires the adoption of techniques for realistically modeling the temporal behavior of such systems. This paper discusses how to model with timed automata the different types of relationships that may be found among the computer clocks of a distributed system, namely, ideal clocks, drifting clocks, and synchronized clocks. For each kind of relationship, a suitable modeling pattern is thoroughly described and formally verified.",
        "keywords": [
            "Real-time systems",
            "Automata",
            "Formal verification",
            "Distributed processing",
            "Embedded systems"
        ]
    },
    {
        "title": "What Industry Needs from Architectural Languages: A Survey.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.74",
        "volume": "39",
        "abstract": "Many times we are faced with the proliferation of definitions, concepts, languages, and tools in certain (research) topics. But often there is a gap between what is provided by existing technologies and what is needed by their users. The strengths, limitations, and needs of the available technologies can be dubious. The same applies to software architectures, and specifically to languages designed to represent architectural models. Tens of different architectural languages have been introduced by the research and industrial communities in the last two decades. However, it is unclear if they fulfill the user's perceived needs in architectural description. As a way to plan for next generation languages for architectural description, this study analyzes practitioners' perceived strengths, limitations, and needs associated with existing languages for software architecture modeling in industry. We run a survey by interviewing 48 practitioners from 40 different IT companies in 15 countries. Each participant is asked to fill in a questionnaire of 51 questions. By analyzing the data collected through this study, we have concluded that 1) while practitioners are generally satisfied with the design capabilities provided by the languages they use, they are dissatisfied with the architectural language analysis features and their abilities to define extra-functional properties; 2) architectural languages used in practice mostly originate from industrial development instead of from academic research; 3) more formality and better usability are required of an architectural language.",
        "keywords": [
            "Unified modeling language",
            "Software architecture",
            "Computer architecture",
            "Industries",
            "Communities",
            "Software",
            "Google"
        ]
    },
    {
        "title": "Amorphous Slicing of Extended Finite State Machines.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.72",
        "volume": "39",
        "abstract": "Slicing is useful for many software engineering applications and has been widely studied for three decades, but there has been comparatively little work on slicing extended finite state machines (EFSMs). This paper introduces a set of dependence-based EFSM slicing algorithms and an accompanying tool. We demonstrate that our algorithms are suitable for dependence-based slicing. We use our tool to conduct experiments on 10 EFSMs, including benchmarks and industrial EFSMs. Ours is the first empirical study of dependence-based program slicing for EFSMs. Compared to the only previously published dependence-based algorithm, our average slice is smaller 40 percent of the time and larger only 10 percent of the time, with an average slice size of 35 percent for termination insensitive slicing.",
        "keywords": [
            "Automata",
            "Algorithm design and analysis",
            "Approximation algorithms",
            "Software algorithms",
            "Unified modeling language",
            "Educational institutions",
            "Electronic mail"
        ]
    },
    {
        "title": "EDZL Schedulability Analysis in Real-Time Multicore Scheduling.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.75",
        "volume": "39",
        "abstract": "In real-time systems, correctness depends not only on functionality but also on timeliness. A great number of scheduling theories have been developed for verification of the temporal correctness of jobs (software) in such systems. Among them, the Earliest Deadline first until Zero-Laxity (EDZL) scheduling algorithm has received growing attention thanks to its effectiveness in multicore real-time scheduling. However, the true potential of EDZL has not yet been fully exploited in its schedulability analysis as the state-of-the-art EDZL analysis techniques involve considerable pessimism. In this paper, we propose a new EDZL multicore schedulability test. We first introduce an interesting observation that suggests an insight toward pessimism reduction in the schedulability analysis of EDZL. We then incorporate it into a well-known existing Earliest Deadline First (EDF) schedulability test, resulting in a new EDZL schedulability test. We demonstrate that the proposed EDZL test not only has lower time complexity than existing EDZL schedulability tests, but also significantly improves the schedulability of EDZL by up to 36.6 percent compared to the best existing EDZL schedulability tests.",
        "keywords": [
            "Real-time systems",
            "Silicon",
            "Scheduling algorithms",
            "Scheduling",
            "Exponential distribution",
            "Time factors",
            "Aerospace electronics"
        ]
    },
    {
        "title": "Embedding Polychrony into Synchrony.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.85",
        "volume": "39",
        "abstract": "This paper presents an embedding of polychronous programs into synchronous ones. Due to this embedding, it is not only possible to deepen the understanding of these different models of computation, but, more importantly, it is possible to transfer compilation techniques that were developed for synchronous programs to polychronous programs. This transfer is nontrivial because the underlying paradigms differ more than their names suggest: Since synchronous systems react deterministically to given inputs in discrete steps, they are typically used to describe reactive systems with a totally ordered notion of time. In contrast, polychronous system models entail a partially ordered notion of time, and are most suited to interface a system with an asynchronous environment by specifying input/output constraints from which a deterministic controller may eventually be refined and synthesized. As particular examples for the mentioned cross fertilization, we show how a simulator and a verification backend for synchronous programs can be made available to polychronous specifications, which is a first step toward integrating heterogeneous models of computation.",
        "keywords": [
            "Clocks",
            "Computational modeling",
            "Synchronization",
            "Embedded systems",
            "Hardware",
            "Unified modeling language"
        ]
    },
    {
        "title": "Pair Programming and Software Defects-A Large, Industrial Case Study.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.68",
        "volume": "39",
        "abstract": "In the last decade, there has been increasing interest in pair programming (PP). However, despite the existing work, there is still a lack of substantial evidence of the effects of PP in industrial environments. To address this issue, we have analyzed the work of a team of 17 industrial developers for 14 months. The team is part of the IT department of a large Italian manufacturing company; it adopts a customized version of extreme programming (XP). We have investigated the effects of PP on software quality in five different scenarios. The results show that PP appears to provide a perceivable but small effect on the reduction of defects in these settings.",
        "keywords": [
            "Programming",
            "Software"
        ]
    },
    {
        "title": "Proactive and Reactive Runtime Service Discovery: A Framework and Its Evaluation.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.84",
        "volume": "39",
        "abstract": "The identification of services during the execution of service-based applications to replace services in them that are no longer available and/or fail to satisfy certain requirements is an important issue. In this paper, we present a framework to support runtime service discovery. This framework can execute service discovery queries in pull and push mode. In pull mode, it executes queries when a need for finding a replacement service arises. In push mode, queries are subscribed to the framework to be executed proactively and, in parallel with the operation of the application, to identify adequate services that could be used if the need for replacing a service arises. Hence, the proactive (push) mode of query execution makes it more likely to avoid interruptions in the operation of service-based applications when a service in them needs to be replaced at runtime. In both modes of query execution, the identification of services relies on distance-based matching of structural, behavioral, quality, and contextual characteristics of services and applications. A prototype implementation of the framework has been developed and an evaluation was carried out to assess the performance of the framework. This evaluation has shown positive results, which are discussed in the paper.",
        "keywords": [
            "Runtime",
            "Context",
            "Servers",
            "Educational institutions",
            "Database languages",
            "Unified modeling language",
            "Informatics"
        ]
    },
    {
        "title": "Synthesizing Modal Transition Systems from Triggered Scenarios.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.62",
        "volume": "39",
        "abstract": "Synthesis of operational behavior models from scenario-based specifications has been extensively studied. The focus has been mainly on either existential or universal interpretations. One noteworthy exception is Live Sequence Charts (LSCs), which provides expressive constructs for conditional universal scenarios and some limited support for nonconditional existential scenarios. In this paper, we propose a scenario-based language that supports both existential and universal interpretations for conditional scenarios. Existing model synthesis techniques use traditional two-valued behavior models, such as Labeled Transition Systems. These are not sufficiently expressive to accommodate specification languages with both existential and universal scenarios. We therefore shift the target of synthesis to Modal Transition Systems (MTS), an extension of labeled Transition Systems that can distinguish between required, unknown, and proscribed behavior to capture the semantics of existential and universal scenarios. Modal Transition Systems support elaboration of behavior models through refinement, which complements an incremental elicitation process suitable for specifying behavior with scenario-based notations. The synthesis algorithm that we define constructs a Modal Transition System that uses refinement to characterize all the Labeled Transition Systems models that satisfy a mixed, conditional existential and universal scenario-based specification. We show how this combination of scenario language, synthesis, and Modal Transition Systems supports behavior model elaboration.",
        "keywords": [
            "Semantics",
            "Analytical models",
            "Online banking",
            "Merging",
            "Unified modeling language",
            "Indexes",
            "Cognition"
        ]
    },
    {
        "title": "Trends in the Quality of Human-Centric Software Engineering Experiments-A Quasi-Experiment.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.76",
        "volume": "39",
        "abstract": "Context: Several text books and papers published between 2000 and 2002 have attempted to introduce experimental design and statistical methods to software engineers undertaking empirical studies. Objective: This paper investigates whether there has been an increase in the quality of human-centric experimental and quasi-experimental journal papers over the time period 1993 to 2010. Method: Seventy experimental and quasi-experimental papers published in four general software engineering journals in the years 1992-2002 and 2006-2010 were each assessed for quality by three empirical software engineering researchers using two quality assessment methods (a questionnaire-based method and a subjective overall assessment). Regression analysis was used to assess the relationship between paper quality and the year of publication, publication date group (before 2003 and after 2005), source journal, average coauthor experience, citation of statistical text books and papers, and paper length. The results were validated both by removing papers for which the quality score appeared unreliable and using an alternative quality measure. Results: Paper quality was significantly associated with year, citing general statistical texts, and paper length (p <; 0.05). Paper length did not reach significance when quality was measured using an overall subjective assessment. Conclusions: The quality of experimental and quasi-experimental software engineering papers appears to have improved gradually since 1993.",
        "keywords": [
            "Software engineering",
            "Guidelines",
            "Correlation",
            "Manuals",
            "Educational institutions",
            "Humans",
            "Materials"
        ]
    },
    {
        "title": "Verifying Linearizability via Optimized Refinement Checking.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.82",
        "volume": "39",
        "abstract": "Linearizability is an important correctness criterion for implementations of concurrent objects. Automatic checking of linearizability is challenging because it requires checking that: (1) All executions of concurrent operations are serializable, and (2) the serialized executions are correct with respect to the sequential semantics. In this work, we describe a method to automatically check linearizability based on refinement relations from abstract specifications to concrete implementations. The method does not require that linearization points in the implementations be given, which is often difficult or impossible. However, the method takes advantage of linearization points if they are given. The method is based on refinement checking of finite-state systems specified as concurrent processes with shared variables. To tackle state space explosion, we develop and apply symmetry reduction, dynamic partial order reduction, and a combination of both for refinement checking. We have built the method into the PAT model checker, and used PAT to automatically check a variety of implementations of concurrent objects, including the first algorithm for scalable nonzero indicators. Our system is able to find all known and injected bugs in these implementations.",
        "keywords": [
            "History",
            "Sun",
            "Educational institutions",
            "Optimization",
            "Electronic mail",
            "Semantics"
        ]
    },
    {
        "title": "Active Learning and Effort Estimation: Finding the Essential Content of Software Effort Estimation Data.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.88",
        "volume": "39",
        "abstract": "Background: Do we always need complex methods for software effort estimation (SEE)? Aim: To characterize the essential content of SEE data, i.e., the least number of features and instances required to capture the information within SEE data. If the essential content is very small, then 1) the contained information must be very brief and 2) the value added of complex learning schemes must be minimal. Method: Our QUICK method computes the euclidean distance between rows (instances) and columns (features) of SEE data, then prunes synonyms (similar features) and outliers (distant instances), then assesses the reduced data by comparing predictions from 1) a simple learner using the reduced data and 2) a state-of-the-art learner (CART) using all data. Performance is measured using hold-out experiments and expressed in terms of mean and median MRE, MAR, PRED(25), MBRE, MIBRE, or MMER. Results: For 18 datasets, QUICK pruned 69 to 96 percent of the training data (median = 89 percent). K = 1 nearest neighbor predictions (in the reduced data) performed as well as CART's predictions (using all data). Conclusion: The essential content of some SEE datasets is very small. Complex estimation methods may be overelaborate for such datasets and can be simplified. We offer QUICK as an example of such a simpler SEE method.",
        "keywords": [
            "Estimation",
            "Indexes",
            "Labeling",
            "Frequency selective surfaces",
            "Euclidean distance",
            "Complexity theory",
            "Principal component analysis"
        ]
    },
    {
        "title": "Balancing Privacy and Utility in Cross-Company Defect Prediction.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.6",
        "volume": "39",
        "abstract": "Background: Cross-company defect prediction (CCDP) is a field of study where an organization lacking enough local data can use data from other organizations for building defect predictors. To support CCDP, data must be shared. Such shared data must be privatized, but that privatization could severely damage the utility of the data. Aim: To enable effective defect prediction from shared data while preserving privacy. Method: We explore privatization algorithms that maintain class boundaries in a dataset. CLIFF is an instance pruner that deletes irrelevant examples. MORPH is a data mutator that moves the data a random distance, taking care not to cross class boundaries. CLIFF+MORPH are tested in a CCDP study among 10 defect datasets from the PROMISE data repository. Results: We find: 1) The CLIFFed+MORPHed algorithms provide more privacy than the state-of-the-art privacy algorithms; 2) in terms of utility measured by defect prediction, we find that CLIFF+MORPH performs significantly better. Conclusions: For the OO defect data studied here, data can be privatized and shared without a significant degradation in utility. To the best of our knowledge, this is the first published result where privatization does not compromise defect prediction.",
        "keywords": [
            "Testing",
            "Software",
            "Genetic algorithms",
            "Sociology",
            "Statistics",
            "Search problems",
            "Arrays"
        ]
    },
    {
        "title": "Featured Transition Systems: Foundations for Verifying Variability-Intensive Systems and Their Application to LTL Model Checking.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.86",
        "volume": "39",
        "abstract": "The premise of variability-intensive systems, specifically in software product line engineering, is the ability to produce a large family of different systems efficiently. Many such systems are critical. Thorough quality assurance techniques are thus required. Unfortunately, most quality assurance techniques were not designed with variability in mind. They work for single systems, and are too costly to apply to the whole system family. In this paper, we propose an efficient automata-based approach to linear time logic (LTL) model checking of variability-intensive systems. We build on earlier work in which we proposed featured transitions systems (FTSs), a compact mathematical model for representing the behaviors of a variability-intensive system. The FTS model checking algorithms verify all products of a family at once and pinpoint those that are faulty. This paper complements our earlier work, covering important theoretical aspects such as expressiveness and parallel composition as well as more practical things like vacuity detection and our logic feature LTL. Furthermore, we provide an in-depth treatment of the FTS model checking algorithm. Finally, we present SNIP, a new model checker for variability-intensive systems. The benchmarks conducted with SNIP confirm the speedups reported previously.",
        "keywords": [
            "Unified modeling language",
            "Semantics",
            "Software",
            "Labeling",
            "Automata",
            "Quality assurance"
        ]
    },
    {
        "title": "MADMatch: Many-to-Many Approximate Diagram Matching for Design Comparison.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.9",
        "volume": "39",
        "abstract": "Matching algorithms play a fundamental role in many important but difficult software engineering activities, especially design evolution analysis and model comparison. We present MADMatch, a fast and scalable many-to-many approximate diagram matching approach based on an error-tolerant graph matching (ETGM) formulation. Diagrams are represented as graphs, costs are assigned to possible differences between two given graphs, and the goal is to retrieve the cheapest matching. We address the resulting optimization problem with a tabu search enhanced by the novel use of lexical and structural information. Through several case studies with different types of diagrams and tasks, we show that our generic approach obtains better results than dedicated state-of-the-art algorithms, such as AURA, PLTSDiff, or UMLDiff, on the exact same datasets used to introduce (and evaluate) these algorithms.",
        "keywords": [
            "Unified modeling language",
            "Algorithm design and analysis",
            "Software",
            "Scalability",
            "Software algorithms",
            "Software engineering",
            "Optimization"
        ]
    },
    {
        "title": "Monitor-Based Instant Software Refactoring.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.4",
        "volume": "39",
        "abstract": "Software refactoring is an effective method for improvement of software quality while software external behavior remains unchanged. To facilitate software refactoring, a number of tools have been proposed for code smell detection and/or for automatic or semi-automatic refactoring. However, these tools are passive and human driven, thus making software refactoring dependent on developers' spontaneity. As a result, software engineers with little experience in software refactoring might miss a number of potential refactorings or may conduct refactorings later than expected. Few refactorings might result in poor software quality, and delayed refactorings may incur higher refactoring cost. To this end, we propose a monitor-based instant refactoring framework to drive inexperienced software engineers to conduct more refactorings promptly. Changes in the source code are instantly analyzed by a monitor running in the background. If these changes have the potential to introduce code smells, i.e., signs of potential problems in the code that might require refactorings, the monitor invokes corresponding smell detection tools and warns developers to resolve detected smells promptly. Feedback from developers, i.e., whether detected smells have been acknowledged and resolved, is consequently used to optimize smell detection algorithms. The proposed framework has been implemented, evaluated, and compared with the traditional human-driven refactoring tools. Evaluation results suggest that the proposed framework could drive inexperienced engineers to resolve more code smells (by an increase of 140 percent) promptly. The average lifespan of resolved smells was reduced by 92 percent. Results also suggest that the proposed framework could help developers to avoid similar code smells through timely warnings at the early stages of software development, thus reducing the total number of code smells by 51 percent.",
        "keywords": [
            "Software",
            "Monitoring",
            "Detection algorithms",
            "Cloning",
            "Detectors",
            "Algorithm design and analysis",
            "Inspection"
        ]
    },
    {
        "title": "Name-Based Analysis of Equally Typed Method Arguments.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.7",
        "volume": "39",
        "abstract": "When calling a method that requires multiple arguments, programmers must pass the arguments in the expected order. For statically typed languages, the compiler helps programmers by checking that the type of each argument matches the type of the formal parameter. Unfortunately, types are futile for methods with multiple parameters of the same type. How can a programmer check that equally typed arguments are passed in the correct order? This paper presents two simple, yet effective, static program analyses that detect problems related to the order of equally typed arguments. The key idea is to leverage identifier names to infer the semantics of arguments and their intended positions. The analyses reveal problems that affect the correctness, understandability, and maintainability of a program, such as accidentally reversed arguments and misleading parameter names. Most parts of the analyses are language-agnostic. We evaluate the approach with 24 real-world programs written in Java and C. Our results show the analyses to be effective and efficient. One analysis reveals anomalies in the order of equally typed arguments; it finds 54 relevant problems with a precision of 82 percent. The other analysis warns about misleading parameter names and finds 31 naming bugs with a precision of 39 percent.",
        "keywords": [
            "Java",
            "Access control",
            "Engines",
            "Feature extraction",
            "Context",
            "Robustness",
            "Program processors"
        ]
    },
    {
        "title": "Quantifying the Effect of Code Smells on Maintenance Effort.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.89",
        "volume": "39",
        "abstract": "Context: Code smells are assumed to indicate bad design that leads to less maintainable code. However, this assumption has not been investigated in controlled studies with professional software developers. Aim: This paper investigates the relationship between code smells and maintenance effort. Method: Six developers were hired to perform three maintenance tasks each on four functionally equivalent Java systems originally implemented by different companies. Each developer spent three to four weeks. In total, they modified 298 Java files in the four systems. An Eclipse IDE plug-in measured the exact amount of time a developer spent maintaining each file. Regression analysis was used to explain the effort using file properties, including the number of smells. Result: None of the 12 investigated smells was significantly associated with increased effort after we adjusted for file size and the number of changes; Refused Bequest was significantly associated with decreased effort. File size and the number of changes explained almost all of the modeled variation in effort. Conclusion: The effects of the 12 smells on maintenance effort were limited. To reduce maintenance effort, a focus on reducing code size and the work practices that limit the number of changes may be more beneficial than refactoring code smells.",
        "keywords": [
            "Maintenance engineering",
            "Java",
            "Software",
            "Surgery",
            "Time measurement",
            "Context",
            "Electronic mail"
        ]
    },
    {
        "title": "Session Reliability of Web Systems under Heavy-Tailed Workloads: An Approach Based on Design and Analysis of Experiments.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.3",
        "volume": "39",
        "abstract": "While workload characterization and performance of web systems have been studied extensively, reliability has received much less attention. In this paper, we propose a framework for session reliability modeling which integrates the user view represented by the session layer and the system view represented by the service layer. A unique characteristic of the session layer is that, in addition to the user navigation patterns, it incorporates the session length in number of requests and allows us to account for heavy-tailed workloads shown to exist in real web systems. The service layer is focused on the request reliability as it is observed at the service provider side. It considers the multifier web server architecture and the way components interact in serving each request. Within this framework, we develop a session reliability model and solve it using simulation. Instead of the traditional one-factor-at-a-time sensitivity analysis, we use statistical design and analysis of experiments, which allow us to identify the factors and interactions that have statistically significant effect on session reliability. Our findings indicate that session reliability, which accounts for the distribution of failed requests within sessions, provides better representation of the user perceived quality than the request-based reliability.",
        "keywords": [
            "Software reliability",
            "Availability",
            "Navigation",
            "Web servers",
            "Reliability engineering",
            "Analytical models"
        ]
    },
    {
        "title": "Software Reliability Modeling with Software Metrics Data via Gaussian Processes.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2012.87",
        "volume": "39",
        "abstract": "In this paper, we describe statistical inference and prediction for software reliability models in the presence of covariate information. Specifically, we develop a semiparametric, Bayesian model using Gaussian processes to estimate the numbers of software failures over various time periods when it is assumed that the software is changed after each time period and that software metrics information is available after each update. Model comparison is also carried out using the deviance information criterion, and predictive inferences on future failures are shown. Real-life examples are presented to illustrate the approach.",
        "keywords": [
            "Software",
            "Gaussian processes",
            "Software reliability",
            "Software metrics",
            "Predictive models",
            "Bayesian methods"
        ]
    },
    {
        "title": "Editorial.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.41",
        "volume": "39",
        "abstract": "IT is my pleasure to introduce a number of distinguished researchers to the Editorial Board of IEEE Transactions on Software Engineering (TSE) this month. Their expertise covers a range of of areas that have seen consistently large numbers of submissions in recent times, and each new associate editor (AE) brings a track record of significant contributions to their field. Their short biographies are provided. Additionally, I am happy to report in the meantime that the latest journal Impact Factors have recently been published, and TSE's has risen to 2.6. It continues to be the highest of all software engineering and related journals.",
        "keywords": null
    },
    {
        "title": "Capsule-Based User Interface Modeling for Large-Scale Applications.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.8",
        "volume": "39",
        "abstract": "We present a novel approach to modeling and implementing user interfaces (UI) of large business applications. The approach is based on the concept of capsule, a profiled structured class from UML which models a simple UI component or a coherent UI fragment of logically and functionally coupled components or other fragments with a clear interface. Consequently, the same modeling concept of capsule with internal structure can be reapplied recursively at successively lower levels of detail within a model, starting from high architectural modeling levels down to the lowest levels of modeling simple UI components. The interface of capsules is defined in terms of pins, while the functional coupling of capsules is specified declaratively by simply wiring their pins. Pins and wires transport messages between capsules, ensuring strict encapsulation. The approach includes a method for formal coupling of capsules' behavior with the underlying object space that provides proper impedance matching between the UI and the business logic while preserving clear separation of concerns between them. We also briefly describe an implementation of a framework that supports the proposed method, including a rich library of ready-to-use capsules, and report on our experience in applying the approach in large-scale industrial systems.",
        "keywords": [
            "Unified modeling language",
            "Business",
            "Couplings",
            "Complexity theory",
            "Object oriented modeling",
            "Buildings",
            "User interfaces"
        ]
    },
    {
        "title": "Data Quality: Some Comments on the NASA Software Defect Datasets.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.11",
        "volume": "39",
        "abstract": "Background--Self-evidently empirical analyses rely upon the quality of their data. Likewise, replications rely upon accurate reporting and using the same rather than similar versions of datasets. In recent years, there has been much interest in using machine learners to classify software modules into defect-prone and not defect-prone categories. The publicly available NASA datasets have been extensively used as part of this research. Objective--This short note investigates the extent to which published analyses based on the NASA defect datasets are meaningful and comparable. Method--We analyze the five studies published in the IEEE Transactions on Software Engineering since 2007 that have utilized these datasets and compare the two versions of the datasets currently in use. Results--We find important differences between the two versions of the datasets, implausible values in one dataset and generally insufficient detail documented on dataset preprocessing. Conclusions--It is recommended that researchers 1) indicate the provenance of the datasets they use, 2) report any preprocessing in sufficient detail to enable meaningful replication, and 3) invest effort in understanding the data prior to applying machine learners.",
        "keywords": [
            "NASA",
            "Software",
            "PROM",
            "Educational institutions",
            "Sun",
            "Communities",
            "Abstracts"
        ]
    },
    {
        "title": "Generating Test Cases for Real-Time Systems Based on Symbolic Models.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.13",
        "volume": "39",
        "abstract": "The state space explosion problem is one of the challenges to be faced by test case generation techniques, particularly when data values need to be enumerated. This problem gets even worse for real-time systems (RTS) that also have time constraints. The usual solution in this context, based on finite state machines or time automata, consists of enumerating data values (restricted to finite domains) while treating time symbolically. In this paper, a symbolic model for conformance testing of real-time systems software named TIOSTS that addresses both data and time symbolically is presented. Moreover, a test case generation process is defined to select more general test cases with variables and parameters that can be instantiated at testing execution time. Generation is based on a combination of symbolic execution and constraint solving for the data part and symbolic analysis for timed aspects. Furthermore, the practical application of the process is investigated through a case study.",
        "keywords": [
            "Testing",
            "Clocks",
            "Cost accounting",
            "Real-time systems",
            "Data models",
            "Automata",
            "Semantics"
        ]
    },
    {
        "title": "Model-Based Test Oracle Generation for Automated Unit Testing of Agent Systems.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.10",
        "volume": "39",
        "abstract": "Software testing remains the most widely used approach to verification in industry today, consuming between 30-50 percent of the entire development cost. Test input selection for intelligent agents presents a problem due to the very fact that the agents are intended to operate robustly under conditions which developers did not consider and would therefore be unlikely to test. Using methods to automatically generate and execute tests is one way to provide coverage of many conditions without significantly increasing cost. However, one problem using automatic generation and execution of tests is the oracle problem: How can we automatically decide if observed program behavior is correct with respect to its specification? In this paper, we present a model-based oracle generation method for unit testing belief-desire-intention agents. We develop a fault model based on the features of the core units to capture the types of faults that may be encountered and define how to automatically generate a partial, passive oracle from the agent design models. We evaluate both the fault model and the oracle generation by testing 14 agent systems. Over 400 issues were raised, and these were analyzed to ascertain whether they represented genuine faults or were false positives. We found that over 70 percent of issues raised were indicative of problems in either the design or the code. Of the 19 checks performed by our oracle, faults were found by all but 5 of these checks. We also found that 8 out the 11 fault types identified in our fault model exhibited at least one fault. The evaluation indicates that the fault model is a productive conceptualization of the problems to be expected in agent unit testing and that the oracle is able to find a substantial number of such faults with relatively small overhead in terms of false positives.",
        "keywords": [
            "Testing",
            "Context",
            "Object oriented modeling",
            "Computational modeling",
            "Fault diagnosis",
            "Arrays",
            "Robustness"
        ]
    },
    {
        "title": "OBEY: Optimal Batched Refactoring Plan Execution for Class Responsibility Redistribution.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.19",
        "volume": "39",
        "abstract": "The redistribution of class responsibilities is a common reengineering practice in object-oriented (OO) software evolution. During the redistribution, developers frequently construct batched refactoring plans for moving multiple methods and fields among various classes. With an objective of carefully maintaining the cohesion and coupling degree of the class design, executing a batched refactoring plan without introducing any objective-violating side effect into the refactored code is essential. However, using most refactoring engines for batched refactoring plan execution introduces coupling-increasing Middle Man bad smell in the final refactored code and therefore makes the refactoring execution suboptimal in achieving the redistribution objective. This work proposes Obey, a methodology for optimal batched refactoring plan execution. Obey analyzes a batched refactoring plan, identifies Middle Man symptoms that cause suboptimal execution, and renovates the plan for optimal execution. We have conducted an empirical study on three open-source software projects to confirm the effectiveness of Obey in a practical context.",
        "keywords": [
            "Couplings",
            "Engines",
            "Software systems",
            "Measurement",
            "Optimization",
            "Context"
        ]
    },
    {
        "title": "Patterns of Knowledge in API Reference Documentation.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.12",
        "volume": "39",
        "abstract": "Reading reference documentation is an important part of programming with application programming interfaces (APIs). Reference documentation complements the API by providing information not obvious from the API syntax. To improve the quality of reference documentation and the efficiency with which the relevant information it contains can be accessed, we must first understand its content. We report on a study of the nature and organization of knowledge contained in the reference documentation of the hundreds of APIs provided as a part of two major technology platforms: Java SDK 6 and .NET 4.0. Our study involved the development of a taxonomy of knowledge types based on grounded methods and independent empirical validation. Seventeen trained coders used the taxonomy to rate a total of 5,574 randomly sampled documentation units to assess the knowledge they contain. Our results provide a comprehensive perspective on the patterns of knowledge in API documentation: observations about the types of knowledge it contains and how this knowledge is distributed throughout the documentation. The taxonomy and patterns of knowledge we present in this paper can be used to help practitioners evaluate the content of their API documentation, better organize their documentation, and limit the amount of low-value content. They also provide a vocabulary that can help structure and facilitate discussions about the content of APIs.",
        "keywords": [
            "Documentation",
            "Taxonomy",
            "Encoding",
            "Reliability",
            "Java",
            "Software",
            "Sociology"
        ]
    },
    {
        "title": "TACO: Efficient SAT-Based Bounded Verification Using Symmetry Breaking and Tight Bounds.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.15",
        "volume": "39",
        "abstract": "SAT-based bounded verification of annotated code consists of translating the code together with the annotations to a propositional formula, and analyzing the formula for specification violations using a SAT-solver. If a violation is found, an execution trace exposing the failure is exhibited. Code involving linked data structures with intricate invariants is particularly hard to analyze using these techniques. In this paper, we present Translation of Annotated COde (TACO), a prototype tool which implements a novel, general, and fully automated technique for the SAT-based analysis of JML-annotated Java sequential programs dealing with complex linked data structures. We instrument code analysis with a symmetry-breaking predicate which, on one hand, reduces the size of the search space by ignoring certain classes of isomorphic models and, on the other hand, allows for the parallel, automated computation of tight bounds for Java fields. Experiments show that the translations to propositional formulas require significantly less propositional variables, leading to an improvement of the efficiency of the analysis of orders of magnitude, compared to the noninstrumented SAT--based analysis. We show that in some cases our tool can uncover bugs that cannot be detected by state-of-the-art tools based on SAT-solving, model checking, or SMT-solving.",
        "keywords": [
            "Metals",
            "Java",
            "Cost accounting",
            "Instruments",
            "Analytical models",
            "Contracts",
            "Context"
        ]
    },
    {
        "title": "Verifying Protocol Conformance Using Software Model Checking for the Model-Driven Development of Embedded Systems.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.14",
        "volume": "39",
        "abstract": "To facilitate modular development, the use of state machines has been proposed to specify the protocol (i.e., the sequence of messages) that each port of a component can engage in. The protocol conformance checking problem consists of determining whether the actual behavior of a component conforms to the protocol specifications on its ports. In this paper, we consider this problem in the context of the model-driven development (MDD) of embedded systems based on UML 2, in which UML 2 state machines are used to specify component behavior. We provide a definition of conformance which slightly extends those found in the literature and reduce the conformance check to a state space exploration. We describe a tool implementing the approach using the Java PathFinder software model checker and the MDD tool IBM Rational RoseRT, discuss its application to three case studies, and show how the tool repeatedly allowed us to find unexpected conformance errors with encouraging performance. We conclude that the approach is promising for supporting the modular development of embedded components in the context of industrial applications of MDD.",
        "keywords": [
            "Unified modeling language",
            "Protocols",
            "Ports (Computers)",
            "Software",
            "Safety",
            "Context",
            "Java"
        ]
    },
    {
        "title": "A Uniform Representation of Hybrid Criteria for Regression Testing.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.16",
        "volume": "39",
        "abstract": "Regression testing tasks of test case prioritization, test suite reduction/minimization, and regression test selection are typically centered around criteria that are based on code coverage, test execution costs, and code modifications. Researchers have developed and evaluated new individual criteria; others have combined existing criteria in different ways to form what we--and some others--call hybrid criteria. In this paper, we formalize the notion of combining multiple criteria into a hybrid. Our goal is to create a uniform representation of such combinations so that they can be described unambiguously and shared among researchers. We envision that such sharing will allow researchers to implement, study, extend, and evaluate the hybrids using a common set of techniques and tools. We precisely formulate three hybrid combinations, Rank, Merge, and Choice, and demonstrate their usefulness in two ways. First, we recast, in terms of our formulations, others' previously reported work on hybrid criteria. Second, we use our previous results on test case prioritization to create and evaluate new hybrid criteria. Our findings suggest that hybrid criteria of others can be described using our Merge and Rank formulations, and that the hybrid criteria we developed most often outperformed their constituent individual criteria.",
        "keywords": [
            "Testing",
            "Fault detection",
            "Educational institutions",
            "Genetic algorithms",
            "Vectors",
            "Loss measurement",
            "Minimization"
        ]
    },
    {
        "title": "Assessing the Cost Effectiveness of Fault Prediction in Acceptance Testing.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.21",
        "volume": "39",
        "abstract": "Until now, various techniques for predicting fault-prone modules have been proposed and evaluated in terms of their prediction performance; however, their actual contribution to business objectives such as quality improvement and cost reduction has rarely been assessed. This paper proposes using a simulation model of software testing to assess the cost effectiveness of test effort allocation strategies based on fault prediction results. The simulation model estimates the number of discoverable faults with respect to the given test resources, the resource allocation strategy, a set of modules to be tested, and the fault prediction results. In a case study applying fault prediction of a small system to acceptance testing in the telecommunication industry, results from our simulation model showed that the best strategy was to let the test effort be proportional to \"the number of expected faults in a module × log(module size).\" By using this strategy with our best fault prediction model, the test effort could be reduced by 25 percent while still detecting as many faults as were normally discovered in testing, although the company required about 6 percent of the test effort for metrics collection, data cleansing, and modeling. The simulation results also indicate that the lower bound of acceptable prediction accuracy is around 0.78 in terms of an effort-aware measure, Norm(P\n<sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">opt</sub>\n). The results indicate that reduction of the test effort can be achieved by fault prediction only if the appropriate test strategy is employed with high enough fault prediction accuracy. Based on these preliminary results, we expect further research to assess their general validity with larger systems.",
        "keywords": [
            "Testing",
            "Predictive models",
            "Measurement",
            "Software",
            "Resource management",
            "Companies",
            "Accuracy"
        ]
    },
    {
        "title": "Early Detection of Collaboration Conflicts and Risks.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.28",
        "volume": "39",
        "abstract": "Conflicts among developers' inconsistent copies of a shared project arise in collaborative development and can slow progress and decrease quality. Identifying and resolving such conflicts early can help. Identifying situations which may lead to conflicts can prevent some conflicts altogether. By studying nine open-source systems totaling 3.4 million lines of code, we establish that conflicts are frequent, persistent, and appear not only as overlapping textual edits but also as subsequent build and test failures. Motivated by this finding, we develop a speculative analysis technique that uses previously unexploited information from version control operations to precisely diagnose important classes of conflicts. Then, we design and implement Crystal, a publicly available tool that helps developers identify, manage, and prevent conflicts. Crystal uses speculative analysis to make concrete advice unobtrusively available to developers.",
        "keywords": [
            "Crystals",
            "Collaboration",
            "History",
            "Open source software",
            "Control systems",
            "Terminology",
            "Computer science"
        ]
    },
    {
        "title": "Generating Test Data from OCL Constraints with Search Techniques.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.17",
        "volume": "39",
        "abstract": "Model-based testing (MBT) aims at automated, scalable, and systematic testing solutions for complex industrial software systems. To increase chances of adoption in industrial contexts, software systems can be modeled using well-established standards such as the Unified Modeling Language (UML) and the Object Constraint Language (OCL). Given that test data generation is one of the major challenges to automate MBT, we focus on test data generation from OCL constraints in this paper. This endeavor is all the more challenging given the numerous OCL constructs and operations that are designed to facilitate the definition of constraints. Though search-based software testing has been applied to test data generation for white-box testing (e.g., branch coverage), its application to the MBT of industrial software systems has been limited. In this paper, we propose a set of search heuristics targeted to OCL constraints to guide test data generation and automate MBT in industrial applications. We evaluate these heuristics for three search algorithms: Genetic Algorithm, (1+1) Evolutionary Algorithm, and Alternating Variable Method. We empirically evaluate our heuristics using complex artificial problems, followed by empirical analyses of the feasibility of our approach on one industrial system in the context of robustness testing. Our approach is also compared with the most widely referenced OCL solver (UMLtoCSP) in the literature and shows to be significantly more efficient.",
        "keywords": [
            "Unified modeling language",
            "Search problems",
            "Software algorithms",
            "Genetic algorithms",
            "Standards",
            "Software testing"
        ]
    },
    {
        "title": "Monitoring Data Usage in Distributed Systems.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.18",
        "volume": "39",
        "abstract": "IT systems manage increasing amounts of sensitive data and there is a growing concern that they comply with policies that regulate data usage. In this paper, we use temporal logic to express policies and runtime monitoring to check system compliance. While well-established methods for monitoring linearly ordered system behavior exist, a major challenge is monitoring distributed and concurrent systems where actions are locally observed in the different system parts. These observations can only be partially ordered, while policy compliance may depend on the actions' actual order of appearance. Technically speaking, it is in general intractable to check compliance of partially ordered traces. We identify fragments of our policy specification language for which compliance can be checked efficiently, namely, by monitoring a single representative trace in which the observed actions are totally ordered. Through a case study we show that the fragments are capable of expressing nontrivial policies and that monitoring representative traces is feasible on real-world data.",
        "keywords": [
            "Monitoring",
            "Cost accounting",
            "Periodic structures",
            "Semantics",
            "Distributed databases",
            "Standards",
            "Finite element analysis"
        ]
    },
    {
        "title": "The Impact of Classifier Configuration and Classifier Combination on Bug Localization.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.27",
        "volume": "39",
        "abstract": "Bug localization is the task of determining which source code entities are relevant to a bug report. Manual bug localization is labor intensive since developers must consider thousands of source code entities. Current research builds bug localization classifiers, based on information retrieval models, to locate entities that are textually similar to the bug report. Current research, however, does not consider the effect of classifier configuration, i.e., all the parameter values that specify the behavior of a classifier. As such, the effect of each parameter or which parameter values lead to the best performance is unknown. In this paper, we empirically investigate the effectiveness of a large space of classifier configurations, 3,172 in total. Further, we introduce a framework for combining the results of multiple classifier configurations since classifier combination has shown promise in other domains. Through a detailed case study on over 8,000 bug reports from three large-scale projects, we make two main contributions. First, we show that the parameters of a classifier have a significant impact on its performance. Second, we show that combining multiple classifiers--whether those classifiers are hand-picked or randomly chosen relative to intelligently defined subspaces of classifiers--improves the performance of even the best individual classifiers.",
        "keywords": [
            "Large scale integration",
            "Measurement",
            "Vectors",
            "Information retrieval",
            "Matrix decomposition",
            "Indexes",
            "Resource management"
        ]
    },
    {
        "title": "Whitening SOA Testing via Event Exposure.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.20",
        "volume": "39",
        "abstract": "Whitening the testing of service-oriented applications can provide service consumers confidence on how well an application has been tested. However, to protect business interests of service providers and to prevent information leakage, the implementation details of services are usually invisible to service consumers. This makes it challenging to determine the test coverage of a service composition as a whole and design test cases effectively. To address this problem, we propose an approach to whiten the testing of service compositions based on events exposed by services. By deriving event interfaces to explore only necessary test coverage information from service implementations, our approach allows service consumers to determine test coverage based on selected events exposed by services at runtime without releasing the service implementation details. We also develop an approach to design test cases effectively based on event interfaces concerning both effectiveness and information leakage. The experimental results show that our approach outperforms existing testing approaches for service compositions with up to 49 percent more test coverage and an up to 24 percent higher fault-detection rate. Moreover, our solution can trade off effectiveness, efficiency, and information leakage for test case generation.",
        "keywords": [
            "Testing",
            "Service-oriented architecture",
            "Books",
            "Runtime",
            "Catalogs",
            "Jacobian matrices"
        ]
    },
    {
        "title": "A Learning-Based Framework for Engineering Feature-Oriented Self-Adaptive Software Systems.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.37",
        "volume": "39",
        "abstract": "Self-adaptive software systems are capable of adjusting their behavior at runtime to achieve certain functional or quality-of-service goals. Often a representation that reflects the internal structure of the managed system is used to reason about its characteristics and make the appropriate adaptation decisions. However, runtime conditions can radically change the internal structure in ways that were not accounted for during their design. As a result, unanticipated changes at runtime that violate the assumptions made about the internal structure of the system could degrade the accuracy of the adaptation decisions. We present an approach for engineering self-adaptive software systems that brings about two innovations: 1) a feature-oriented approach for representing engineers' knowledge of adaptation choices that are deemed practical, and 2) an online learning-based approach for assessing and reasoning about adaptation decisions that does not require an explicit representation of the internal structure of the managed software system. Engineers' knowledge, represented in feature-models, adds structure to learning, which in turn makes online learning feasible. We present an empirical evaluation of the framework using a real-world self-adaptive software system. Results demonstrate the framework's ability to accurately learn the changing dynamics of the system while achieving efficient analysis and adaptation.",
        "keywords": [
            "Software systems",
            "Runtime",
            "Adaptation models",
            "Quality of service",
            "Authentication",
            "Measurement"
        ]
    },
    {
        "title": "A Taxonomy and Mapping of Computer-Based Critiquing Tools.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.32",
        "volume": "39",
        "abstract": "Critics have emerged in recent times as a specific tool feature to support users in computer-mediated tasks. These computer-supported critics provide proactive guidelines or suggestions for improvement to designs, code, and other digital artifacts. The concept of a critic has been adopted in various domains, including medical, programming, software engineering, design sketching, and others. Critics have been shown to be an effective mechanism for providing feedback to users. We propose a new critic taxonomy based on extensive review of the critic literature. The groups and elements of our critic taxonomy are presented and explained collectively with examples, including the mapping of 13 existing critic tools, predominantly for software engineering and programming education tasks to the taxonomy. We believe this critic taxonomy will assist others in identifying, categorizing, developing, and deploying computer-supported critics in a range of domains.",
        "keywords": [
            "Taxonomy",
            "Software",
            "Recommender systems",
            "Programming",
            "Software engineering",
            "Unified modeling language",
            "Java"
        ]
    },
    {
        "title": "Conservative Reasoning about the Probability of Failure on Demand of a 1-out-of-2 Software-Based System in Which One Channel Is \"Possibly Perfect\".",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.35",
        "volume": "39",
        "abstract": "In earlier work, [11] (henceforth LR), an analysis was presented of a 1-out-of-2 software-based system in which one channel was “possibly perfect”. It was shown that, at the aleatory level, the system pfd (probability of failure on demand) could be bounded above by the product of the pfd of channel A and the pnp (probability of nonperfection) of channel B. This result was presented as a way of avoiding the well-known difficulty that for two certainly-fallible channels, failures of the two will be dependent, i.e., the system pfd cannot be expressed simply as a product of the channel pfds. A price paid in this new approach for avoiding the issue of failure dependence is that the result is conservative. Furthermore, a complete analysis requires that account be taken of epistemic uncertainty-here concerning the numeric values of the two parameters pfd\n<sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">A</sub>\n and pnp\n<sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">B</sub>\n. Unfortunately this introduces a different difficult problem of dependence: estimating the dependence between an assessor's beliefs about the parameters. The work reported here avoids this problem by obtaining results that require only an assessor's marginal beliefs about the individual channels, i.e., they do not require knowledge of the dependence between these beliefs. The price paid is further conservatism in the results.",
        "keywords": [
            "Phase frequency detector",
            "Uncertainty",
            "Cognition",
            "Software reliability",
            "Software",
            "Safety"
        ]
    },
    {
        "title": "Determining the Cause of a Design Model Inconsistency.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.30",
        "volume": "39",
        "abstract": "When a software engineer finds an inconsistency in a model, then the first question is why? What caused it? Obviously, there must be an error. But where could it be? Or is the design rule erroneous and if yes then which part? The cause of an inconsistency identifies the part of the model or design rule where the error must be. We believe that the visualization of an inconsistency ought to visualize the cause. Understanding the cause is of vital importance before a repair can even be formulated. Indeed, any automation (e.g., code generation, refactoring) has to be considered with caution if it involves model elements that cause inconsistencies. This paper analyzes the basic structure of inconsistent design rules as well as their behavior during validation and presents an algorithm for computing its cause. The approach is fully automated, tool supported, and was evaluated on 14,111 inconsistencies across 29 design models. We found that our approach computes correct causes for inconsistencies, these causes are nearly always a subset of the model elements investigated by the design rules' validation (a naive cause computation approximation), and the computation is very fast (99.8 percent of the causes are computable in < 100 ms).",
        "keywords": [
            "Unified modeling language",
            "Computational modeling",
            "Context",
            "Maintenance engineering",
            "Visualization",
            "Context modeling",
            "Light emitting diodes"
        ]
    },
    {
        "title": "Equality to Equals and Unequals: A Revisit of the Equivalence and Nonequivalence Criteria in Class-Level Testing of Object-Oriented Software.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.33",
        "volume": "39",
        "abstract": "Algebraic specifications have been used in the testing of object-oriented programs and received much attention since the 1990s. It is generally believed that class-level testing based on algebraic specifications involves two independent aspects: the testing of equivalent and nonequivalent ground terms. Researchers have cited intuitive examples to illustrate the philosophy that even if an implementation satisfies all the requirements specified by the equivalence of ground terms, it may still fail to satisfy some of the requirements specified by the nonequivalence of ground terms. Thus, both the testing of equivalent ground terms and the testing of nonequivalent ground terms have been considered as significant and cannot replace each other. In this paper, we present an innovative finding that, given any canonical specification of a class with proper imports, a complete implementation satisfies all the observationally equivalent ground terms if and only if it satisfies all the observationally nonequivalent ground terms. As a result, these two aspects of software testing cover each other and can therefore replace each other. These findings provide a deeper understanding of software testing based on algebraic specifications, rendering the theory more elegant and complete. We also highlight a couple of important practical implications of our theoretical results.",
        "keywords": [
            "Software",
            "Software testing",
            "Observers",
            "Context",
            "Semantics",
            "Computer science"
        ]
    },
    {
        "title": "Optimizing Ordered Throughput Using Autonomic Cloud Bursting Schedulers.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.26",
        "volume": "39",
        "abstract": "Optimizing ordered throughput not only improves the system efficiency but also makes cloud bursting transparent to the user. This is critical from the perspective of user fairness in customer-facing systems, correctness in stream processing systems, and so on. In this paper, we consider optimizing ordered throughput for near real-time, data-intensive, independent computations using cloud bursting. Intercloud computation of data-intensive applications is a challenge due to large data transfer requirements, low intercloud bandwidth, and best-effort traffic on the Internet. The system model we consider is comprised of two processing stages. The first stage uses cloud bursting opportunistically for parallel processing, while the second stage (sequential) expects the output of the first stage to be in the same order as the arrival sequence. We propose three scheduling heuristics as part of an autonomic cloud bursting approach that adapt to changing workload characteristics, variation in bandwidth, and available resources to optimize ordered throughput. We also characterize the operational regimes for cloud bursting as stabilization mode versus acceleration mode, depending on the workload characteristics like the size of data to be transferred for a given compute load. The operational regime characterization helps in deciding how many instances can be optimally utilized in the external cloud.",
        "keywords": [
            "Cloud computing",
            "Optimization",
            "Scheduling"
        ]
    },
    {
        "title": "Usability through Software Design.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.29",
        "volume": "39",
        "abstract": "Over the past two decades, the HCI community has proposed specific features that software applications should include to overcome some of the most common usability problems. However, incorporating such usability features into software applications may not be a straightforward process for software developers who have not been trained in usability (i.e., determining when, how, and why usability features should been considered). We have defined a set of usability guidelines for software development to help software engineers incorporate particular usability features into their applications. In this paper, we focus on the software design artifacts provided by the guidelines. We detail the structure of the proposed design artifacts and how they should be used according to the software development process and software architecture used in each application. We have tested our guidelines in an academic setting. Preliminary validation shows that the use of the guidelines reduces development time, improves the quality of the resulting designs, and significantly decreases the perceived complexity of the usability features from the developers' perspective.",
        "keywords": [
            "Usability",
            "Guidelines",
            "Human computer interaction",
            "Unified modeling language",
            "Communities"
        ]
    },
    {
        "title": "Where Should We Fix This Bug? A Two-Phase Recommendation Model.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.24",
        "volume": "39",
        "abstract": "To support developers in debugging and locating bugs, we propose a two-phase prediction model that uses bug reports' contents to suggest the files likely to be fixed. In the first phase, our model checks whether the given bug report contains sufficient information for prediction. If so, the model proceeds to predict files to be fixed, based on the content of the bug report. In other words, our two-phase model \"speaks up\" only if it is confident of making a suggestion for the given bug report; otherwise, it remains silent. In the evaluation on the Mozilla \"Firefox\" and \"Core\" packages, the two-phase model was able to make predictions for almost half of all bug reports; on average, 70 percent of these predictions pointed to the correct files. In addition, we compared the two-phase model with three other prediction models: the Usual Suspects, the one-phase model, and BugScout. The two-phase model manifests the best prediction performance.",
        "keywords": [
            "Predictive models",
            "Feature extraction",
            "Computer bugs",
            "Software",
            "Computational modeling",
            "Data mining",
            "Noise"
        ]
    },
    {
        "title": "A Study of Variability Models and Languages in the Systems Software Domain.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.34",
        "volume": "39",
        "abstract": "Variability models represent the common and variable features of products in a product line. Since the introduction of FODA in 1990, several variability modeling languages have been proposed in academia and industry, followed by hundreds of research papers on variability models and modeling. However, little is known about the practical use of such languages. We study the constructs, semantics, usage, and associated tools of two variability modeling languages, Kconfig and CDL, which are independently developed outside academia and used in large and significant software projects. We analyze 128 variability models found in 12 open--source projects using these languages. Our study 1) supports variability modeling research with empirical data on the real-world use of its flagship concepts. However, we 2) also provide requirements for concepts and mechanisms that are not commonly considered in academic techniques, and 3) challenge assumptions about size and complexity of variability models made in academic papers. These results are of interest to researchers working on variability modeling and analysis techniques and to designers of tools, such as feature dependency checkers and interactive product configurators.",
        "keywords": [
            "Biological system modeling",
            "Software products",
            "Product line",
            "Analytical models",
            "Computational modeling",
            "Semantics",
            "Computer architecture"
        ]
    },
    {
        "title": "Conservative Bounds for the pfd of a 1-out-of-2 Software-Based System Based on an Assessor's Subjective Probability of \"Not Worse Than Independence\".",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.31",
        "volume": "39",
        "abstract": "We consider the problem of assessing the reliability of a 1-out-of-2 software-based system, in which failures of the two channels cannot be assumed to be independent with certainty. An informal approach to this problem assesses the channel probabilities of failure on demand (pfds) conservatively, and then multiplies these together in the hope that the conservatism will be sufficient to overcome any possible dependence between the channel failures. Our intention here is to place this kind of reasoning on a formal footing. We introduce a notion of \"not worse than independence\"' and assume that an assessor has a prior belief about this, expressed as a probability. We obtain a conservative prior system pfd, and show how a conservative posterior system pfd can be obtained following the observation of a number of demands without system failure. We present some illustrative numerical examples, discuss some of the difficulties involved in this way of reasoning, and suggest some avenues of future research.",
        "keywords": [
            "Phase frequency detector",
            "Cognition",
            "Software reliability",
            "Fault tolerant systems",
            "Software reliability",
            "Reliability engineering",
            "Failure analysis"
        ]
    },
    {
        "title": "Identification, Impact, and Refactoring of Smells in Pipe-Like Web Mashups.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.42",
        "volume": "39",
        "abstract": "With the emergence of tools to support visual mashup creation, tens of thousands of users have started to access, manipulate, and compose data from web sources. We have observed, however, that mashups created by these users tend to suffer from deficiencies that propagate as mashups are reused, which happens frequently. To address these deficiencies, we would like to bring some of the benefits of software engineering techniques to the end users creating these programs. In this work, we focus on identifying code smells indicative of the deficiencies we observed in web mashups programmed in the popular Yahoo! Pipes environment. Through an empirical study, we explore the impact of those smells on the preferences of 61 users, and observe that a significant majority of users prefer mashups without smells. We then introduce refactorings targeting those smells. These refactorings reduce the complexity of the mashup programs, increase their abstraction, update broken data sources and dated components, and standardize their structures to fit the community development patterns. Our assessment of a sample of over 8,000 mashups shows that smells are present in 81 percent of them and that the proposed refactorings can reduce the number of smelly mashups to 16 percent, illustrating the potential of refactoring to support the thousands of end-users programming mashups. Further, we explore how the smells and refactorings can apply to other end-user programming domains to show the generalizability of our approach.",
        "keywords": [
            "Mashups",
            "Visualization",
            "Factoring",
            "Generators",
            "Programming"
        ]
    },
    {
        "title": "Identifying Code of Individual Features in Client-Side Web Applications.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.38",
        "volume": "39",
        "abstract": "Web applications are one of today's fastest growing software systems. Structurally, they are composed of two parts: the server side, used for data access and business logic, and the client side, used as a user interface. In recent years, with developers building complex interfaces, the client side is playing an increasingly important role. Unfortunately, the techniques and tools used to support development are not as advanced as in other disciplines. From the user's perspective, the client side offers a number of features that are relatively easy to distinguish. However, the same cannot be said for their implementation details. This makes the understanding, maintenance, and reuse of code difficult. The goal of the work presented in this paper is to improve reusability, maintainability, and performance of client-side web applications by identifying the code that implements a particular feature. We have evaluated the approach based on three different experiments: extracting features, extracting library functionalities, and page optimization. The evaluation shows that the method is able to identify the implementation details of individual features, and that by extracting the identified code, a considerable reduction in code size and increase in performance can be achieved.",
        "keywords": [
            "Feature extraction",
            "HTML",
            "Cascading style sheets",
            "Browsers",
            "Web and internet services",
            "Optimization",
            "Codes"
        ]
    },
    {
        "title": "Learning Project Management Decisions: A Case Study with Case-Based Reasoning versus Data Farming.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.43",
        "volume": "39",
        "abstract": "Background: Given information on just a few prior projects, how do we learn the best and fewest changes for current projects? Aim: To conduct a case study comparing two ways to recommend project changes. 1) Data farmers use Monte Carlo sampling to survey and summarize the space of possible outcomes. 2) Case-based reasoners (CBR) explore the neighborhood around test instances. Method: We applied a state-of-the data farmer (SEESAW) and a CBR tool ()'V2) to software project data. Results: CBR with )'V2 was more effective than SEESAW's data farming for learning best and recommended project changes, effectively reducing runtime, effort, and defects. Further, CBR with )'V2 was comparably easier to build, maintain, and apply in novel domains, especially on noisy data sets. Conclusion: Use CBR tools like )'V2 when data are scarce or noisy or when project data cannot be expressed in the required form of a data farmer. Future Work: This study applied our own CBR tool to several small data sets. Future work could apply other CBR tools and data farmers to other data (perhaps to explore other goals such as, say, minimizing maintenance effort).",
        "keywords": [
            "Data models",
            "Project management",
            "Search methods",
            "Monte Carlo methods",
            "Mathematical model",
            "Software engineering"
        ]
    },
    {
        "title": "Proactive Self-Adaptation for Improving the Reliability of Mission-Critical, Embedded, and Mobile Software.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.36",
        "volume": "39",
        "abstract": "Embedded and mobile software systems are marked with a high degree of unpredictability and dynamism in the execution context. At the same time, such systems are often mission-critical, meaning that they need to satisfy strict reliability requirements. Most current software reliability analysis approaches are not suitable for these types of software systems, as they do not take the changes in the execution context of the system into account. We propose an approach geared to such systems which continuously furnishes refined reliability predictions at runtime by incorporating various sources of information, including the execution context of the system. The reliability predictions are leveraged to proactively place the software in the (near-)optimal configuration with respect to changing conditions. Our approach considers two representative architectural reconfiguration decisions that impact the system's reliability: reallocation of components to processes and changing the number of component replicas. We have realized the approach as part of a framework intended for mission-critical settings, called REsilient SItuated SofTware system (RESIST), and evaluated it using a mobile emergency response system.",
        "keywords": [
            "Mobile communication",
            "Software reliability",
            "Context awareness",
            "Reliability engineering",
            "Software  architecture",
            "Computer architecture"
        ]
    },
    {
        "title": "Supporting Domain Analysis through Mining and Recommending Features from Online Product Listings.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.39",
        "volume": "39",
        "abstract": "Domain analysis is a labor-intensive task in which related software systems are analyzed to discover their common and variable parts. Many software projects include extensive domain analysis activities, intended to jumpstart the requirements process through identifying potential features. In this paper, we present a recommender system that is designed to reduce the human effort of performing domain analysis. Our approach relies on data mining techniques to discover common features across products as well as relationships among those features. We use a novel incremental diffusive algorithm to extract features from online product descriptions, and then employ association rule mining and the (k)-nearest neighbor machine learning method to make feature recommendations during the domain analysis process. Our feature mining and feature recommendation algorithms are quantitatively evaluated and the results are presented. Also, the performance of the recommender system is illustrated and evaluated within the context of a case study for an enterprise-level collaborative software suite. The results clearly highlight the benefits and limitations of our approach, as well as the necessary preconditions for its success.",
        "keywords": [
            "Feature extraction",
            "Recommender systems",
            "Clustering algorithms",
            "Domain analysis",
            "Data mining",
            "Algorithm design and analysis",
            "Electronic mail",
            "Nearest neighbor search",
            "Clustering"
        ]
    },
    {
        "title": "Task Environment Complexity, Global Team Dispersion, Process Capabilities, and Coordination in Software Development.",
        "venue_name": "tse",
        "year": 2013,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2013.40",
        "volume": "39",
        "abstract": "Software development teams are increasingly global. Team members are separated by multiple boundaries such as geographic location, time zone, culture, and organization, presenting substantial coordination challenges. Global software development becomes even more challenging when user requirements change dynamically. However, little empirical research has investigated how team dispersion across multiple boundaries and user requirements dynamism, which collectively increase task environment complexity, influence team coordination and software development success in the global context. Further, we have a limited understanding of how software process capabilities such as rigor, standardization, agility, and customizability mitigate the negative effects of global team dispersion and user requirements dynamism. To address these important issues, we test a set of relevant hypotheses using field survey data obtained from both project managers and stakeholders. Our results show that global team dispersion and user requirements dynamism have a negative effect on coordination effectiveness. We find that the negative effect of global team dispersion on coordination effectiveness decreases as process standardization increases and that the negative effect of user requirements dynamism on coordination effectiveness decreases as process agility increases. We find that coordination effectiveness has a positive effect on global software development success in terms of both process and product aspects.",
        "keywords": [
            "User centered design",
            "Complexity theory",
            "Dispersion",
            "Global communication",
            "Software development",
            "Process capability",
            "Globalization"
        ]
    }
]