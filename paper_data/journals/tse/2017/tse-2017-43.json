[
    {
        "title": "An Empirical Comparison of Model Validation Techniques for Defect Prediction Models.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2584050",
        "volume": "43",
        "abstract": "Defect prediction models help software quality assurance teams to allocate their limited resources to the most defect-prone modules. Model validation techniques, such as \n<inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math> </inline-formula>\n-fold cross-validation, use historical data to estimate how well a model will perform in the future. However, little is known about how accurate the estimates of model validation techniques tend to be. In this paper, we investigate the bias and variance of model validation techniques in the domain of defect prediction. Analysis of 101 public defect datasets suggests that 77 percent of them are highly susceptible to producing unstable results– - selecting an appropriate model validation technique is a critical experimental design choice. Based on an analysis of 256 studies in the defect prediction literature, we select the 12 most commonly adopted model validation techniques for evaluation. Through a case study of 18 systems, we find that single-repetition holdout validation tends to produce estimates with 46-229 percent more bias and 53-863 percent more variance than the top-ranked model validation techniques. On the other hand, out-of-sample bootstrap validation yields the best balance between the bias and variance of estimates in the context of our study. Therefore, we recommend that future defect prediction studies avoid single-repetition holdout validation, and instead, use out-of-sample bootstrap validation.",
        "keywords": [
            "Predictive models",
            "Data models",
            "Analytical models",
            "Context",
            "Context modeling",
            "Software",
            "Logistics"
        ]
    },
    {
        "title": "Interaction Models and Automated Control under Partial Observable Environments.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2564959",
        "volume": "43",
        "abstract": "The problem of automatically constructing a software component such that when executed in a given environment satisfies a goal, is recurrent in software engineering. Controller synthesis is a field which fits into this vision. In this paper we study controller synthesis for partially observable LTS models. We exploit the link between partially observable control and non-determinism and show that, unlike fully observable LTS or Kripke structure control problems, in this setting the existence of a solution depends on the interaction model between the controller-to-be and its environment. We identify two interaction models, namely Interface Automata and Weak Interface Automata, define appropriate control problems and describe synthesis algorithms for each of them.",
        "keywords": [
            "Servers",
            "Maintenance engineering",
            "Automata",
            "Context",
            "Observability",
            "Uncertainty"
        ]
    },
    {
        "title": "Nopol: Automatic Repair of Conditional Statement Bugs in Java Programs.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2560811",
        "volume": "43",
        "abstract": "We propose Nopol, an approach to automatic repair of buggy conditional statements (i.e., if-then-else statements). This approach takes a buggy program as well as a test suite as input and generates a patch with a conditional expression as output. The test suite is required to contain passing test cases to model the expected behavior of the program and at least one failing test case that reveals the bug to be repaired. The process of Nopol consists of three major phases. First, Nopol employs angelic fix localization to identify expected values of a condition during the test execution. Second, runtime trace collection is used to collect variables and their actual values, including primitive data types and objected-oriented features (e.g., nullness checks), to serve as building blocks for patch generation. Third, Nopol encodes these collected data into an instance of a Satisfiability Modulo Theory (SMT) problem; then a feasible solution to the SMT instance is translated back into a code patch. We evaluate Nopol on 22 real-world bugs (16 bugs with buggy if conditions and six bugs with missing preconditions) on two large open-source projects, namely Apache Commons Math and Apache Commons Lang. Empirical analysis on these bugs shows that our approach can effectively fix bugs with buggy if conditions and missing preconditions. We illustrate the capabilities and limitations of Nopol using case studies of real bug fixes.",
        "keywords": [
            "Maintenance engineering",
            "Computer bugs",
            "Runtime",
            "Java",
            "Encoding",
            "Open source software",
            "Indexes"
        ]
    },
    {
        "title": "Process Aspects and Social Dynamics of Contemporary Code Review: Insights from Open Source Development and Industrial Practice at Microsoft.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2576451",
        "volume": "43",
        "abstract": "Many open source and commercial developers practice contemporary code review, a lightweight, informal, tool-based code review process. To better understand this process and its benefits, we gathered information about code review practices via surveys of open source software developers and developers from Microsoft. The results of our analysis suggest that developers spend approximately 10-15 percent of their time in code reviews, with the amount of effort increasing with experience. Developers consider code review important, stating that in addition to finding defects, code reviews offer other benefits, including knowledge sharing, community building, and maintaining code quality. The quality of the code submitted for review helps reviewers form impressions about their teammates, which can influence future collaborations. We found a large amount of similarity between the Microsoft and OSS respondents. One interesting difference is that while OSS respondents view code review as an important method of impression formation, Microsoft respondents found knowledge dissemination to be more important. Finally, we found little difference between distributed and co-located Microsoft teams. Our findings identify the following key areas that warrant focused research: 1) exploring the non-technical benefits of code reviews, 2) helping developers in articulating review comments, and 3) assisting reviewers' program comprehension during code reviews.",
        "keywords": [
            "Inspection",
            "Organizations",
            "Collaboration",
            "Context",
            "Instruments",
            "Measurement",
            "Human factors"
        ]
    },
    {
        "title": "Timed Automata Modeling and Verification for Publish-Subscribe Structures Using Distributed Resources.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2560842",
        "volume": "43",
        "abstract": "In this paper we present a Timed Automata model for the Publish/Subscribe paradigm in the context of Web Service Compositions with distributed resources, on the basis of an algebraic language inspired by the WSRF standard constructions. This framework allows a set of participants in a Web Service composition to interact with one another and also to manage a collection of distributed resources. The model includes operations for clients to publish, discover and subscribe to resources, so as to be notified when the resource property values fulfill certain conditions (topic-based subscription). Simulation and model-checking techniques can therefore be applied to the obtained network of timed automata, in order to check whether certain properties of interest are satisfied. A specific case study is finally presented to illustrate the model and the verification of the relevant properties on the obtained timed automata model.",
        "keywords": [
            "Web services",
            "Automata",
            "Unified modeling language",
            "Semantics",
            "Clocks",
            "Probabilistic logic",
            "Context modeling"
        ]
    },
    {
        "title": "ARENA: An Approach for the Automated Generation of Release Notes.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2591536",
        "volume": "43",
        "abstract": "Release notes document corrections, enhancements, and, in general, changes that were implemented in a new release of a software project. They are usually created manually and may include hundreds of different items, such as descriptions of new features, bug fixes, structural changes, new or deprecated APIs, and changes to software licenses. Thus, producing them can be a time-consuming and daunting task. This paper describes ARENA (Automatic RElease Notes generAtor), an approach for the automatic generation of release notes. ARENA extracts changes from the source code, summarizes them, and integrates them with information from versioning systems and issue trackers. ARENA was designed based on the manual analysis of 990 existing release notes. In order to evaluate the quality of the release notes automatically generated by ARENA, we performed four empirical studies involving a total of 56 participants (48 professional developers and eight students). The obtained results indicate that the generated release notes are very good approximations of the ones manually produced by developers and often include important information that is missing in the manually created release notes.",
        "keywords": [
            "Libraries",
            "Licenses",
            "Feature extraction",
            "Documentation",
            "Computer bugs",
            "Open source software"
        ]
    },
    {
        "title": "A Study of Causes and Consequences of Client-Side JavaScript Bugs.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2586066",
        "volume": "43",
        "abstract": "Client-side JavaScript is widely used in web applications to improve user-interactivity and minimize client-server communications. Unfortunately, JavaScript is known to be error-prone. While prior studies have demonstrated the prevalence of JavaScript faults, no attempts have been made to determine their causes and consequences. The goal of our study is to understand the root causes and impact of JavaScript faults and how the results can impact JavaScript programmers, testers and tool developers. We perform an empirical study of 502 bug reports from 19 bug repositories. The bug reports are thoroughly examined to classify and extract information about each bug' cause (the error) and consequence (the failure and impact). Our results show that the majority (68 percent) of JavaScript faults are DOM-related, meaning they are caused by faulty interactions of the JavaScript code with the Document Object Model (DOM). Further, 80 percent of the highest impact JavaScript faults are DOM-related. Finally, most JavaScript faults originate from programmer mistakes committed in the JavaScript code itself, as opposed to other web application components. These results indicate that JavaScript programmers and testers need tools that can help them reason about the DOM. Additionally, developers can use the error patterns we found to design more powerful static analysis tools for JavaScript.",
        "keywords": [
            "Computer bugs",
            "Servers",
            "Market research",
            "HTML",
            "Data mining",
            "Reliability",
            "Cascading style sheets"
        ]
    },
    {
        "title": "Automated Synthesis and Dynamic Analysis of Tradeoff Spaces for Object-Relational Mapping.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2587646",
        "volume": "43",
        "abstract": "Producing software systems that achieve acceptable tradeoffs among multiple non-functional properties remains a significant engineering problem. We propose an approach to solving this problem that combines synthesis of spaces of design alternatives from logical specifications and dynamic analysis of each point in the resulting spaces. We hypothesize that this approach has potential to help engineers understand important tradeoffs among dynamically measurable properties of system components at meaningful scales within reach of existing synthesis tools. To test this hypothesis, we developed tools to enable, and we conducted, a set of experiments in the domain of relational databases for object-oriented data models. For each of several data models, we used our approach to empirically test the accuracy of a published suite of metrics to predict tradeoffs based on the static schema structure alone. The results show that exhaustive synthesis and analysis provides a superior view of the tradeoff spaces for such designs. This work creates a path forward toward systems that achieve significantly better tradeoffs among important system properties.",
        "keywords": [
            "Data models",
            "Load modeling",
            "Object oriented modeling",
            "Semantics",
            "Relational databases",
            "Measurement"
        ]
    },
    {
        "title": "DECAF: A Platform-Neutral Whole-System Dynamic Binary Analysis Platform.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2589242",
        "volume": "43",
        "abstract": "Dynamic binary analysis is a prevalent and indispensable technique in program analysis. While several dynamic binary analysis tools and frameworks have been proposed, all suffer from one or more of: prohibitive performance degradation, a semantic gap between the analysis code and the program being analyzed, architecture/OS specificity, being user-mode only, and lacking APIs. We present DECAF, a virtual machine based, multi-target, whole-system dynamic binary analysis framework built on top of QEMU. DECAF provides Just-In-Time Virtual Machine Introspection and a plugin architecture with a simple-to-use event-driven programming interface. DECAF implements a new instruction-level taint tracking engine at bit granularity, which exercises fine control over the QEMU Tiny Code Generator (TCG) intermediate representation to accomplish on-the-fly optimizations while ensuring that the taint propagation is sound and highly precise. We perform a formal analysis of DECAF's taint propagation rules to verify that most instructions introduce neither false positives nor false negatives. We also present three platform-neutral plugins-Instruction Tracer, Keylogger Detector, and API Tracer, to demonstrate the ease of use and effectiveness of DECAF in writing cross-platform and system-wide analysis tools. Implementation of DECAF consists of 9,550 lines of C++ code and 10,270 lines of C code and we evaluate DECAF using CPU2006 SPEC benchmarks and show average overhead of 605 percent for system wide tainting and 12 percent for VMI.",
        "keywords": [
            "Instruments",
            "Virtual machining",
            "Kernel",
            "Semantics",
            "Computer architecture",
            "Registers",
            "Context"
        ]
    },
    {
        "title": "How Social and Communication Channels Shape and Challenge a Participatory Culture in Software Development.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2584053",
        "volume": "43",
        "abstract": "Software developers use many different communication tools and channels in their work. The diversity of these tools has dramatically increased over the past decade and developers now have access to a wide range of socially enabled communication channels and social media to support their activities. The availability of such social tools is leading to a participatory culture of software development, where developers want to engage with, learn from, and co-create software with other developers. However, the interplay of these social channels, as well as the opportunities and challenges they may create when used together within this participatory development culture are not yet well understood. In this paper, we report on a large-scale survey conducted with 1,449 GitHub users. We discuss the channels these developers find essential to their work and gain an understanding of the challenges they face using them. Our findings lay the empirical foundation for providing recommendations to developers and tool designers on how to use and improve tools for software developers.",
        "keywords": [
            "Software",
            "Communication channels",
            "Media",
            "Collaboration",
            "Electronic mail",
            "Face",
            "Knowledge engineering"
        ]
    },
    {
        "title": "Improving Timeliness and Visibility in Publishing Software Engineering Research.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2663918",
        "volume": "43",
        "abstract": "Reports on initiatives to improve and enhance the IEEE Transactions on Software Engineering. ",
        "keywords": null
    },
    {
        "title": "Automating Live Update for Generic Server Programs.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2584066",
        "volume": "43",
        "abstract": "The pressing demand to deploy software updates without stopping running programs has fostered much research on live update systems in the past decades. Prior solutions, however, either make strong assumptions on the nature of the update or require extensive and error-prone manual effort, factors which discourage the adoption of live update. This paper presents \n<i>Mutable Checkpoint-Restart</i>\n (\n<i>MCR</i>\n), a new live update solution for generic (multiprocess and multithreaded) server programs written in C. Compared to prior solutions, MCR can support arbitrary software updates and automate most of the common live update operations. The key idea is to allow the running version to safely reach a quiescent state and then allow the new version to restart as similarly to a fresh program initialization as possible, relying on existing code paths to automatically restore the old program threads and reinitialize a relevant portion of the program data structures. To transfer the remaining data structures, MCR relies on a combination of precise and conservative garbage collection techniques to trace all the global pointers and apply the required state transformations on the fly. Experimental results on popular server programs (\n<i>Apache httpd</i>\n, \n<i>nginx</i>\n, \n<i>OpenSSH</i>\n and \n<i>vsftpd</i>\n) confirm that our techniques can effectively automate problems previously deemed difficult at the cost of negligible performance overhead (2 percent on average) and moderate memory overhead (3.9\n<inline-formula><tex-math notation=\"LaTeX\">$\\times$ </tex-math></inline-formula>\n on average, without optimizations).",
        "keywords": [
            "Servers",
            "Data structures",
            "Convergence",
            "Software",
            "Manuals",
            "System recovery",
            "Buildings"
        ]
    },
    {
        "title": "CACheck: Detecting and Repairing Cell Arrays in Spreadsheets.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2584059",
        "volume": "43",
        "abstract": "Spreadsheets are widely used by end users for numerical computation in their business. Spreadsheet cells whose computation is subject to the same semantics are often clustered in a row or column as a cell array. When a spreadsheet evolves, the cells in a cell array can degenerate due to ad hoc modifications. Such degenerated cell arrays no longer keep cells prescribing the same computational semantics, and are said to exhibit ambiguous computation smells. We propose CACheck, a novel technique that automatically detects and repairs smelly cell arrays by recovering their intended computational semantics. Our empirical study on the EUSES and Enron corpora finds that such smelly cell arrays are common. Our study also suggests that CACheck is useful for detecting and repairing real spreadsheet problems caused by smelly cell arrays. Compared with our previous work AmCheck, CACheck detects smelly cell arrays with higher precision and recall rate.",
        "keywords": [
            "Semantics",
            "Maintenance engineering",
            "Software",
            "Computer science",
            "Nonhomogeneous media",
            "Electronic mail",
            "Business"
        ]
    },
    {
        "title": "Dependence Guided Symbolic Execution.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2584063",
        "volume": "43",
        "abstract": "Symbolic execution is a powerful technique for systematically exploring the paths of a program and generating the corresponding test inputs. However, its practical usage is often limited by the \n<i>path explosion</i>\n problem, that is, the number of explored paths usually grows exponentially with the increase of program size. In this paper, we argue that for the purpose of fault detection it is not necessary to systematically explore the paths, and propose a new symbolic execution approach to mitigate the path explosion problem by predicting and eliminating the redundant paths based on symbolic value. Our approach can achieve the equivalent fault detection capability as traditional symbolic execution without exhaustive path exploration. In addition, we develop a practical implementation called Dependence Guided Symbolic Execution (DGSE) to soundly approximate our approach. Through exploiting program dependence, DGSE can predict and eliminate the redundant paths at a reasonable computational cost. Our empirical study shows that the redundant paths are abundant and widespread in a program. Compared with traditional symbolic execution, DGSE only explores 6.96 to 96.57 percent of the paths and achieves a speedup of 1.02\n<inline-formula> <tex-math notation=\"LaTeX\">$\\times$</tex-math></inline-formula>\n to 49.56\n<inline-formula><tex-math notation=\"LaTeX\">$\\times$</tex-math></inline-formula>\n. We have released our tool and the benchmarks used to evaluate DGSE\n<inline-formula><tex-math notation=\"LaTeX\">$^\\ast$</tex-math></inline-formula>\n.",
        "keywords": [
            "Fault detection",
            "Explosions",
            "Benchmark testing",
            "Electronic mail",
            "Computational efficiency",
            "Input variables"
        ]
    },
    {
        "title": "Improving Automated Bug Triaging with Specialized Topic Model.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2576454",
        "volume": "43",
        "abstract": "Bug triaging refers to the process of assigning a bug to the most appropriate developer to fix. It becomes more and more difficult and complicated as the size of software and the number of developers increase. In this paper, we propose a new framework for bug triaging, which maps the words in the bug reports (i.e., the term space) to their corresponding topics (i.e., the topic space). We propose a specialized topic modeling algorithm named \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> multi-feature topic model (MTM)</i>\n which extends Latent Dirichlet Allocation (LDA) for bug triaging. \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">MTM </i>\n considers product and component information of bug reports to map the term space to the topic space. Finally, we propose an incremental learning method named \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">TopicMiner</i>\n which considers the topic distribution of a new bug report to assign an appropriate fixer based on the affinity of the fixer to the topics. We pair \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"> TopicMiner</i>\n with MTM (\n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">TopicMiner<inline-formula><tex-math notation=\"LaTeX\">$^{MTM}$</tex-math> <alternatives><inline-graphic xlink:href=\"xia-ieq1-2576454.gif\"/></alternatives></inline-formula></i>\n). We have evaluated our solution on 5 large bug report datasets including GCC, OpenOffice, Mozilla, Netbeans, and Eclipse containing a total of 227,278 bug reports. We show that \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">TopicMiner<inline-formula><tex-math notation=\"LaTeX\"> $^{MTM}$</tex-math><alternatives><inline-graphic xlink:href=\"xia-ieq2-2576454.gif\"/></alternatives></inline-formula> </i>\n can achieve top-1 and top-5 prediction accuracies of 0.4831-0.6868, and 0.7686-0.9084, respectively. We also compare \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">TopicMiner<inline-formula><tex-math notation=\"LaTeX\">$^{MTM}$</tex-math><alternatives> <inline-graphic xlink:href=\"xia-ieq3-2576454.gif\"/></alternatives></inline-formula></i>\n with Bugzie, LDA-KL, SVM-LDA, LDA-Activity, and Yang et al.'s approach. The results show that \n<italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">TopicMiner<inline-formula> <tex-math notation=\"LaTeX\">$^{MTM}$</tex-math><alternatives><inline-graphic xlink:href=\"xia-ieq4-2576454.gif\"/> </alternatives></inline-formula></i>\n on average improves top-1 and top-5 prediction accuracies of Bugzie by 128.48 and 53.22 percent, LDA-KL by 262.91 and 105.97 percent, SVM-LDA by 205.89 and 110.48 percent, LDA-Activity by 377.60 and 176.32 percent, and Yang et al.'s approach by 59.88 and 13.70 percent, respectively.",
        "keywords": [
            "Software",
            "Resource management",
            "Software algorithms",
            "Support vector machines",
            "Learning systems",
            "Indexes",
            "Computer bugs"
        ]
    },
    {
        "title": "An Enhanced Bailout Protocol for Mixed Criticality Embedded Software.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2592907",
        "volume": "43",
        "abstract": "To move mixed criticality research into industrial practice requires models whose run-time behaviour is acceptable to systems engineers. Certain aspects of current models, such as abandoning lower criticality tasks when certain situations arise, do not give the robustness required in application domains such as the automotive and aerospace industries. In this paper a new bailout protocol is developed that still guarantees high criticality software but minimises the negative impact on lower criticality software via a timely return to normal operation. We show how the bailout protocol can be integrated with existing techniques, utilising both offline slack and online gain-time to further improve performance. Static analysis is provided for schedulability guarantees, while scenario-based evaluation via simulation is used to explore the effectiveness of the protocol.",
        "keywords": [
            "Protocols",
            "Standards",
            "Software",
            "Analytical models",
            "Job shop scheduling",
            "Software engineering",
            "Safety"
        ]
    },
    {
        "title": "An Improved SDA Based Defect Prediction Framework for Both Within-Project and Cross-Project Class-Imbalance Problems.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2597849",
        "volume": "43",
        "abstract": "<i>Background.</i> Solving the class-imbalance problem of within-project software defect prediction (SDP) is an important research topic. Although some class-imbalance learning methods have been presented, there exists room for improvement. For cross-project SDP, we found that the class-imbalanced source usually leads to misclassification of defective instances. However, only one work has paid attention to this cross-project class-imbalance problem. <i>Objective.</i> We aim to provide effective solutions for both within-project and cross-project class-imbalance problems. <i>Method.</i> Subclass discriminant analysis (SDA), an effective feature learning method, is introduced to solve the problems. It can learn features with more powerful classification ability from original metrics. For within-project prediction, we improve SDA for achieving balanced subclasses and propose the improved SDA (ISDA) approach. For cross-project prediction, we employ the semi-supervised transfer component analysis (SSTCA) method to make the distributions of source and target data consistent, and propose the SSTCA&#x002B;ISDA prediction approach. <i>Results</i>. Extensive experiments on four widely used datasets indicate that: 1) ISDA-based solution performs better than other state-of-the-art methods for within-project class-imbalance problem; 2) SSTCA&#x002B;ISDA proposed for cross-project class-imbalance problem significantly outperforms related methods. <i> Conclusion</i>. Within-project and cross-project class-imbalance problems greatly affect prediction performance, and we provide a unified and effective prediction framework for both problems.",
        "keywords": [
            "Support vector machines",
            "Learning systems",
            "Predictive models",
            "Software",
            "Software engineering",
            "Measurement"
        ]
    },
    {
        "title": "Efficient Dynamic Updates of Distributed Components Through Version Consistency.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2592913",
        "volume": "43",
        "abstract": "Modern component-based distributed software systems are increasingly required to offer non-stop service and thus their updates must be carried out at runtime. Different authors have already proposed solutions for the safe management of dynamic updates. Our contribution aims at improving their efficiency without compromising safety. We propose a new criterion, called version consistency, which defines when a dynamic update can be safely and efficiently applied to the components that execute distributed transactions. Version consistency ensures that distributed transactions be served as if they were operated on a single coherent version of the system despite possible concurrent updates. The paper presents a distributed algorithm for checking version consistency efficiently, formalizes the proposed approach by means of a graph transformation system, and verifies its correctness through model checking. The paper also presents ConUp, a novel prototype framework that supports the approach and offers a viable, concrete solution for the use of version consistency. Both the approach and ConUp are evaluated on a significant third-party application. Obtained results witness the benefits of the proposed solution with respect to both timeliness and disruption.",
        "keywords": [
            "Portals",
            "Runtime",
            "Software systems",
            "Safety",
            "Model checking",
            "Concrete"
        ]
    },
    {
        "title": "Mining Sequences of Developer Interactions in Visual Studio for Usage Smells.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2592905",
        "volume": "43",
        "abstract": "In this paper, we present a semi-automatic approach for mining a large-scale dataset of IDE interactions to extract usage smells, i.e., inefficient IDE usage patterns exhibited by developers in the field. The approach outlined in this paper first mines frequent IDE usage patterns, filtered via a set of thresholds and by the authors, that are subsequently supported (or disputed) using a developer survey, in order to form usage smells. In contrast with conventional mining of IDE usage data, our approach identifies time-ordered sequences of developer actions that are exhibited by many developers in the field. This pattern mining workflow is resilient to the ample noise present in IDE datasets due to the mix of actions and events that these datasets typically contain. We identify usage patterns and smells that contribute to the understanding of the usability of Visual Studio for debugging, code search, and active file navigation, and, more broadly, to the understanding of developer behavior during these software development activities. Among our findings is the discovery that developers are reluctant to use conditional breakpoints when debugging, due to perceived IDE performance problems as well as due to the lack of error checking in specifying the conditional.",
        "keywords": [
            "Data mining",
            "Visualization",
            "Usability",
            "Data analysis",
            "Debugging",
            "Software engineering",
            "Navigation"
        ]
    },
    {
        "title": "Test Oracle Strategies for Model-Based Testing.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2597136",
        "volume": "43",
        "abstract": "Testers use model-based testing to design abstract tests from models of the system's behavior. Testers instantiate the abstract tests into concrete tests with test input values and test oracles that check the results. Given the same test inputs, more elaborate test oracles have the potential to reveal more failures, but may also be more costly. This research investigates the ability for test oracles to reveal failures. We define ten new test oracle strategies that vary in amount and frequency of program state checked. We empirically compared them with two baseline test oracle strategies. The paper presents several main findings. (1) Test oracles must check more than runtime exceptions because checking exceptions alone is not effective at revealing failures. (2) Test oracles do not need to check the entire output state because checking partial states reveals nearly as many failures as checking entire states. (3) Test oracles do not need to check program states multiple times because checking states less frequently is as effective as checking states more frequently. In general, when state machine diagrams are used to generate tests, checking state invariants is a reasonably effective low cost approach to creating test oracles.",
        "keywords": [
            "Unified modeling language",
            "Software",
            "Context",
            "Concrete",
            "System testing",
            "Observability"
        ]
    },
    {
        "title": "Approaches to Co-Evolution of Metamodels and Models: A Survey.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2610424",
        "volume": "43",
        "abstract": "Modeling languages, just as all software artifacts, evolve. This poses the risk that legacy models of a company get lost, when they become incompatible with the new language version. To address this risk, a multitude of approaches for metamodel-model co-evolution were proposed in the last 10 years. However, the high number of solutions makes it difficult for practitioners to choose an appropriate approach. In this paper, we present a survey on 31 approaches to support metamodel-model co-evolution. We introduce a taxonomy of solution techniques and classify the existing approaches. To support researchers, we discuss the state of the art, in order to better identify open issues. Furthermore, we use the results to provide a decision support for practitioners, who aim to adopt solutions from research.",
        "keywords": [
            "Unified modeling language",
            "Companies",
            "Taxonomy",
            "Biological system modeling",
            "Atmospheric modeling",
            "Libraries",
            "Productivity"
        ]
    },
    {
        "title": "A System for Profiling and Monitoring Database Access Patterns by Application Programs for Anomaly Detection.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2598336",
        "volume": "43",
        "abstract": "Database Management Systems (DBMSs) provide access control mechanisms that allow database administrators (DBAs) to grant application programs access privileges to databases. Though such mechanisms are powerful, in practice finer-grained access control mechanism tailored to the semantics of the data stored in the DMBS is required as a first class defense mechanism against smart attackers. Hence, custom written applications which access databases implement an additional layer of access control. Therefore, securing a database alone is not enough for such applications, as attackers aiming at stealing data can take advantage of vulnerabilities in the privileged applications and make these applications to issue malicious database queries. An access control mechanism can only prevent application programs from accessing the data to which the programs are not authorized, but it is unable to prevent misuse of the data to which application programs are authorized for access. Hence, we need a mechanism able to detect malicious behavior resulting from previously authorized applications. In this paper, we present the architecture of an anomaly detection mechanism, DetAnom, that aims to solve such problem. Our approach is based the analysis and profiling of the application in order to create a succinct representation of its interaction with the database. Such a profile keeps a signature for every submitted query and also the corresponding constraints that the application program must satisfy to submit the query. Later, in the detection phase, whenever the application issues a query, a module captures the query before it reaches the database and verifies the corresponding signature and constraints against the current context of the application. If there is a mismatch, the query is marked as anomalous. The main advantage of our anomaly detection mechanism is that, in order to build the application profiles, we need neither any previous knowledge of application vulnerabilities nor any example of possible attacks. As a result, our mechanism is able to protect the data from attacks tailored to database applications such as code modification attacks, SQL injections, and also from other data-centric attacks as well. We have implemented our mechanism with a software testing technique called concolic testing and the PostgreSQL DBMS. Experimental results show that our profiling technique is close to accurate, requires acceptable amount of time, and the detection mechanism incurs low runtime overhead.",
        "keywords": [
            "Databases",
            "Access control",
            "Software testing",
            "Software",
            "Engines"
        ]
    },
    {
        "title": "Model-Based Self-Aware Performance and Resource Management Using the Descartes Modeling Language.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2613863",
        "volume": "43",
        "abstract": "Modern IT systems have increasingly distributed and dynamic architectures providing flexibility to adapt to changes in the environment and thus enabling higher resource efficiency. However, these benefits come at the cost of higher system complexity and dynamics. Thus, engineering systems that manage their end-to-end application performance and resource efficiency in an autonomic manner is a challenge. In this article, we present a holistic model-based approach for self-aware performance and resource management leveraging the Descartes Modeling Language (DML), an architecture-level modeling language for online performance and resource management. We propose a novel online performance prediction process that dynamically tailors the model solving depending on the requirements regarding accuracy and overhead. Using these prediction capabilities, we implement a generic model-based control loop for proactive system adaptation. We evaluate our model-based approach in the context of two representative case studies showing that with the proposed methods, significant resource efficiency gains can be achieved while maintaining performance requirements. These results represent the first end-to-end validation of our approach, demonstrating its potential for self-aware performance and resource management in the context of modern IT systems and infrastructures.",
        "keywords": [
            "Adaptation models",
            "Resource management",
            "Computer architecture",
            "Predictive models",
            "Unified modeling language",
            "Software",
            "Dynamic scheduling"
        ]
    },
    {
        "title": "Self-Adaptive and Online QoS Modeling for Cloud-Based Software Services.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2608826",
        "volume": "43",
        "abstract": "In the presence of scale, dynamism, uncertainty and elasticity, cloud software engineers faces several challenges when modeling Quality of Service (QoS) for cloud-based software services. These challenges can be best managed through self-adaptivity because engineers' intervention is difficult, if not impossible, given the dynamic and uncertain QoS sensitivity to the environment and control knobs in the cloud. This is especially true for the shared infrastructure of cloud, where unexpected interference can be caused by co-located software services running on the same virtual machine; and co-hosted virtual machines within the same physical machine. In this paper, we describe the related challenges and present a fully dynamic, self-adaptive and online QoS modeling approach, which grounds on sound information theory and machine learning algorithms, to create QoS model that is capable to predict the QoS value as output over time by using the information on environmental conditions, control knobs and interference as inputs. In particular, we report on in-depth analysis on the correlations of selected inputs to the accuracy of QoS model in cloud. To dynamically selects inputs to the models at runtime and tune accuracy, we design self-adaptive hybrid dual-learners that partition the possible inputs space into two sub-spaces, each of which applies different symmetric uncertainty based selection techniques; the results of sub-spaces are then combined. Subsequently, we propose the use of adaptive multi-learners for building the model. These learners simultaneously allow several learning algorithms to model the QoS function, permitting the capability for dynamically selecting the best model for prediction on the fly. We experimentally evaluate our models in the cloud environment using RUBiS benchmark and realistic FIFA 98 workload. The results show that our approach is more accurate and effective than state-of-the-art modelings.",
        "keywords": [
            "Quality of service",
            "Cloud computing",
            "Interference",
            "Adaptation models",
            "Sensitivity",
            "Uncertainty"
        ]
    },
    {
        "title": "The Use of Summation to Aggregate Software Metrics Hinders the Performance of Defect Prediction Models.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2599161",
        "volume": "43",
        "abstract": "Defect prediction models help software organizations to anticipate where defects will appear in the future. When training a defect prediction model, historical defect data is often mined from a Version Control System (VCS, e.g., Subversion), which records software changes at the file-level. Software metrics, on the other hand, are often calculated at the class- or method-level (e.g., McCabe's Cyclomatic Complexity). To address the disagreement in granularity, the class- and method-level software metrics are aggregated to file-level, often using summation (i.e., McCabe of a file is the sum of the McCabe of all methods within the file). A recent study shows that summation significantly inflates the correlation between lines of code (Sloc) and cyclomatic complexity (Cc) in Java projects. While there are many other aggregation schemes (e.g., central tendency, dispersion), they have remained unexplored in the scope of defect prediction. In this study, we set out to investigate how different aggregation schemes impact defect prediction models. Through an analysis of 11 aggregation schemes using data collected from 255 open source projects, we find that: (1) aggregation schemes can significantly alter correlations among metrics, as well as the correlations between metrics and the defect count; (2) when constructing models to predict defect proneness, applying only the summation scheme (i.e., the most commonly used aggregation scheme in the literature) only achieves the best performance (the best among the 12 studied configurations) in 11 percent of the studied projects, while applying all of the studied aggregation schemes achieves the best performance in 40 percent of the studied projects; (3) when constructing models to predict defect rank or count, either applying only the summation or applying all of the studied aggregation schemes achieves similar performance, with both achieving the closest to the best performance more often than the other studied aggregation schemes; and (4) when constructing models for effort-aware defect prediction, the mean or median aggregation schemes yield performance values that are significantly closer to the best performance than any of the other studied aggregation schemes. Broadly speaking, the performance of defect prediction models are often underestimated due to our community's tendency to only use the summation aggregation scheme. Given the potential benefit of applying additional aggregation schemes, we advise that future defect prediction models should explore a variety of aggregation schemes.",
        "keywords": [
            "Predictive models",
            "Correlation",
            "Software metrics",
            "Indexes",
            "Software",
            "Data models"
        ]
    },
    {
        "title": "A Taxonomy and Qualitative Comparison of Program Analysis Techniques for Security Assessment of Android Software.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2615307",
        "volume": "43",
        "abstract": "In parallel with the meteoric rise of mobile software, we are witnessing an alarming escalation in the number and sophistication of the security threats targeted at mobile platforms, particularly Android, as the dominant platform. While existing research has made significant progress towards detection and mitigation of Android security, gaps and challenges remain. This paper contributes a comprehensive taxonomy to classify and characterize the state-of-the-art research in this area. We have carefully followed the systematic literature review process, and analyzed the results of more than 300 research papers, resulting in the most comprehensive and elaborate investigation of the literature in this area of research. The systematic analysis of the research literature has revealed patterns, trends, and gaps in the existing literature, and underlined key challenges and opportunities that will shape the focus of future research efforts.",
        "keywords": [
            "Androids",
            "Humanoid robots",
            "Security",
            "Taxonomy",
            "Mobile communication",
            "Malware",
            "Systematics"
        ]
    },
    {
        "title": "Automated Steering of Model-Based Test Oracles to Admit Real Program Behaviors.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2615311",
        "volume": "43",
        "abstract": "The test oracle-a judge of the correctness of the system under test (SUT)-is a major component of the testing process. Specifying test oracles is challenging for some domains, such as real-time embedded systems, where small changes in timing or sensory input may cause large behavioral differences. Models of such systems, often built for analysis and simulation, are appealing for reuse as test oracles. These models, however, typically represent an idealized system, abstracting away certain issues such as non-deterministic timing behavior and sensor noise. Thus, even with the same inputs, the model's behavior may fail to match an acceptable behavior of the SUT, leading to many false positives reported by the test oracle. We propose an automated steering framework that can adjust the behavior of the model to better match the behavior of the SUT to reduce the rate of false positives. This model steering is limited by a set of constraints (defining the differences in behavior that are acceptable) and is based on a search process attempting to minimize a dissimilarity metric. This framework allows non-deterministic, but bounded, behavioral differences, while preventing future mismatches by guiding the oracle-within limits-to match the execution of the SUT. Results show that steering significantly increases SUT-oracle conformance with minimal masking of real faults and, thus, has significant potential for reducing false positives and, consequently, testing and debugging costs while improving the quality of the testing process.",
        "keywords": [
            "Testing",
            "Analytical models",
            "Computational modeling",
            "Software",
            "Delays",
            "Hardware",
            "Pacemakers"
        ]
    },
    {
        "title": "Online Reliability Prediction via Motifs-Based Dynamic Bayesian Networks for Service-Oriented Systems.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2615615",
        "volume": "43",
        "abstract": "A service-oriented System of Systems (SoS) considers a system as a service and constructs a robust and value-added SoS by outsourcing external component systems through service composition techniques. Online reliability prediction for the component systems for the purpose of assuring the overall Quality of Service (QoS) is often a major challenge in coping with a loosely coupled SoS operating under dynamic and uncertain running environments. It is also a prerequisite for guaranteeing runtime QoS of a SoS through optimal service selection for reliable system construction. We propose a novel online reliability time series prediction approach for the component systems in a service-oriented SoS. We utilize Probabilistic Graphical Models (PGMs) to yield near-future, time series predictions. We assess the approach via invocation records collected from widely used real Web services. Experimental results have confirmed the effectiveness of the approach.",
        "keywords": [
            "Time series analysis",
            "Quality of service",
            "Web services",
            "Throughput",
            "Software reliability",
            "Time factors"
        ]
    },
    {
        "title": "The Value of Exact Analysis in Requirements Selection.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2615100",
        "volume": "43",
        "abstract": "Uncertainty is characterised by incomplete understanding. It is inevitable in the early phase of requirements engineering, and can lead to unsound requirement decisions. Inappropriate requirement choices may result in products that fail to satisfy stakeholders' needs, and might cause loss of revenue. To overcome uncertainty, requirements engineering decision support needs uncertainty management. In this research, we develop a decision support framework METRO for the Next Release Problem (NRP) to manage algorithmic uncertainty and requirements uncertainty. An exact NRP solver (NSGDP) lies at the heart of METRO. NSGDP's exactness eliminates interference caused by approximate existing NRP solvers. We apply NSGDP to three NRP instances, derived from a real world NRP instance, RALIC, and compare with NSGA-II, a widely-used approximate (inexact) technique. We find the randomness of NSGA-II results in decision makers missing up to 99.95 percent of the optimal solutions and obtaining up to 36.48 percent inexact requirement selection decisions. The chance of getting an inexact decision using existing approximate approaches is negatively correlated with the implementation cost of a requirement (Spearman r up to -0.72). Compared to the inexact existing approach, NSGDP saves 15.21 percent lost revenue, on average, for the RALIC dataset.",
        "keywords": [
            "Uncertainty",
            "Stakeholders",
            "Robustness",
            "Optimization",
            "Software",
            "Software engineering",
            "Software algorithms"
        ]
    },
    {
        "title": "A Dissection of the Test-Driven Development Process: Does It Really Matter to Test-First or to Test-Last?",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2616877",
        "volume": "43",
        "abstract": "Background: Test-driven development (TDD) is a technique that repeats short coding cycles interleaved with testing. The developer first writes a unit test for the desired functionality, followed by the necessary production code, and refactors the code. Many empirical studies neglect unique process characteristics related to TDD iterative nature. Aim: We formulate four process characteristic: sequencing, granularity, uniformity, and refactoring effort. We investigate how these characteristics impact quality and productivity in TDD and related variations. Method: We analyzed 82 data points collected from 39 professionals, each capturing the process used while performing a specific development task. We built regression models to assess the impact of process characteristics on quality and productivity. Quality was measured by functional correctness. Result: Quality and productivity improvements were primarily positively associated with the granularity and uniformity. Sequencing, the order in which test and production code are written, had no important influence. Refactoring effort was negatively associated with both outcomes. We explain the unexpected negative correlation with quality by possible prevalence of mixed refactoring. Conclusion: The claimed benefits of TDD may not be due to its distinctive test-first dynamic, but rather due to the fact that TDD-like processes encourage fine-grained, steady steps that improve focus and flow.",
        "keywords": [
            "Testing",
            "Productivity",
            "Context",
            "Companies",
            "Sequential analysis",
            "Conferences"
        ]
    },
    {
        "title": "A Feature-Based Classification of Model Repair Approaches.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2620145",
        "volume": "43",
        "abstract": "Consistency management, the ability to detect, diagnose and handle inconsistencies, is crucial during the development process in Model-driven Engineering (MDE). As the popularity and application scenarios of MDE expanded, a variety of different techniques were proposed to address these tasks in specific contexts. Of the various stages of consistency management, this work focuses on inconsistency handling in MDE, particularly in model repair techniques. This paper proposes a feature-based classification system for model repair techniques, based on an systematic literature review of the area. We expect this work to assist developers and researchers from different disciplines in comparing their work under a unifying framework, and aid MDE practitioners in selecting suitable model repair approaches.",
        "keywords": [
            "Maintenance engineering",
            "Unified modeling language",
            "Taxonomy",
            "Context",
            "Feature extraction",
            "Software engineering",
            "Systematics"
        ]
    },
    {
        "title": "A Framework for Evaluating the Results of the SZZ Approach for Identifying Bug-Introducing Changes.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2616306",
        "volume": "43",
        "abstract": "The approach proposed by Silwerski, Zimmermann, and Zeller (SZZ) for identifying bug-introducing changes is at the foundation of several research areas within the software engineering discipline. Despite the foundational role of SZZ, little effort has been made to evaluate its results. Such an evaluation is a challenging task because the ground truth is not readily available. By acknowledging such challenges, we propose a framework to evaluate the results of alternative SZZ implementations. The framework evaluates the following criteria: (1) the earliest bug appearance, (2) the future impact of changes, and (3) the realism of bug introduction. We use the proposed framework to evaluate five SZZ implementations using data from ten open source projects. We find that previously proposed improvements to SZZ tend to inflate the number of incorrectly identified bug-introducing changes. We also find that a single bug-introducing change may be blamed for introducing hundreds of future bugs. Furthermore, we find that SZZ implementations report that at least 46 percent of the bugs are caused by bug-introducing changes that are years apart from one another. Such results suggest that current SZZ implementations still lack mechanisms to accurately identify bug-introducing changes. Our proposed framework provides a systematic mean for evaluating the data that is generated by a given SZZ implementation.",
        "keywords": [
            "Computer bugs",
            "Software engineering",
            "Electronic mail",
            "Software",
            "Manuals",
            "History",
            "Systematics"
        ]
    },
    {
        "title": "Keyword Search for Building Service-Based Systems.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2624293",
        "volume": "43",
        "abstract": "With the fast growth of applications of service-oriented architecture (SOA) in software engineering, there has been a rapid increase in demand for building service-based systems (SBSs) by composing existing Web services. Finding appropriate component services to compose is a key step in the SBS engineering process. Existing approaches require that system engineers have detailed knowledge of SOA techniques which is often too demanding. To address this issue, we propose Keyword Search for Service-based Systems (KS3), a novel approach that integrates and automates the system planning, service discovery and service selection operations for building SBSs based on keyword search. KS3 assists system engineers without detailed knowledge of SOA techniques in searching for component services to build SBSs by typing a few keywords that represent the tasks of the SBSs with quality constraints and optimisation goals for system quality, e.g., reliability, throughput and cost. KS3 offers a new paradigm for SBS engineering that can significantly save the time and effort during the system engineering process. We conducted large-scale experiments using two real-world Web service datasets to demonstrate the practicality, effectiveness and efficiency of KS3.",
        "keywords": [
            "Service-oriented architecture",
            "Data models",
            "Keyword search",
            "Buildings",
            "Planning",
            "Libraries"
        ]
    },
    {
        "title": "Supporting Change Impact Analysis Using a Recommendation System: An Industrial Case Study in a Safety-Critical Context.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2620458",
        "volume": "43",
        "abstract": "Change Impact Analysis (CIA) during software evolution of safety-critical systems is a labor-intensive task. Several authors have proposed tool support for CIA, but very few tools were evaluated in industry. We present a case study on ImpRec, a recommendation System for Software Engineering (RSSE), tailored for CIA at a process automation company. ImpRec builds on assisted tracing, using information retrieval solutions and mining software repositories to recommend development artifacts, potentially impacted when resolving incoming issue reports. In contrast to the majority of tools for automated CIA, ImpRec explicitly targets development artifacts that are not source code. We evaluate ImpRec in a two-phase study. First, we measure the correctness of ImpRec's recommendations by a simulation based on 12 years' worth of issue reports in the company. Second, we assess the utility of working with ImpRec by deploying the RSSE in two development teams on different continents. The results suggest that ImpRec presents about 40 percent of the true impact among the top-10 recommendations. Furthermore, user log analysis indicates that ImpRec can support CIA in industry, and developers acknowledge the value of ImpRec in interviews. In conclusion, our findings show the potential of reusing traceability associated with developers' past activities in an RSSE.",
        "keywords": [
            "Context",
            "Industries",
            "Software engineering",
            "Unified modeling language",
            "Automation",
            "Software systems"
        ]
    },
    {
        "title": "Automatic Contract Insertion with CCBot.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2625248",
        "volume": "43",
        "abstract": "Existing static analysis tools require significant programmer effort. On large code bases, static analysis tools produce thousands of warnings. It is unrealistic to expect users to review such a massive list and to manually make changes for each warning. To address this issue we propose CCBot (short for CodeContracts Bot), a new tool that applies the results of static analysis to existing code through automatic code transformation. Specifically, CCBot instruments the code with method preconditions, postconditions, and object invariants which detect faults at runtime or statically using a static contract checker. The only configuration the programmer needs to perform is to give CCBot the file paths to code she wants instrumented. This allows the programmer to adopt contract-based static analysis with little effort. CCBot's instrumented version of the code is guaranteed to compile if the original code did. This guarantee means the programmer can deploy or test the instrumented code immediately without additional manual effort. The inserted contracts can detect common errors such as null pointer dereferences and out-of-bounds array accesses. CCBot is a robust large-scale tool with an open-source C# implementation. We have tested it on real world projects with tens of thousands of lines of code. We discuss several projects as case studies, highlighting undiscovered bugs found by CCBot, including 22 new contracts that were accepted by the project authors.",
        "keywords": [
            "Contracts",
            "C# languages",
            "Instruments",
            "Computer bugs",
            "Reactive power",
            "Semantics",
            "Runtime"
        ]
    },
    {
        "title": "GK-Tail+ An Efficient Approach to Learn Software Models.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2623623",
        "volume": "43",
        "abstract": "Inferring models of program behavior from execution samples can provide useful information about a system, also in the increasingly common case of systems that evolve and adapt in their lifetime, and without requiring large developers' effort. Techniques for learning models of program behavior from execution traces shall address conflicting challenges of recall, specificity and performance: They shall generate models that comprehensively represent the system behavior (recall) while limiting the amount of illegal behaviors that may be erroneously accepted by the model (specificity), and should infer the models within a reasonable time budget to process industrial scale systems (performance). In our early work, we designed GK-tail, an approach that can infer guarded finite state machines that model the behavior of object-oriented programs in terms of sequences of method calls and constraints on the parameter values. GK-tail addresses well two of the three main challenges, since it infers guarded finite state machines with a high level of recall and specificity, but presents severe limitations in terms of performance that reduce its scalability. In this paper, we present GK-tail+, a new approach to infer guarded finite state machines from execution traces of object-oriented programs. GK-tail+ proposes a new set of inference criteria that represent the core element of the inference process: It largely reduces the inference time of GK-tail while producing guarded finite state machines with a comparable level of recall and specificity. Thus, GK-tail+ advances the preliminary results of GK-tail by addressing all the three main challenges of learning models of program behavior from execution traces.",
        "keywords": [
            "Object oriented modeling",
            "Merging",
            "Analytical models",
            "Adaptation models",
            "Software systems",
            "Limiting"
        ]
    },
    {
        "title": "Imprecise Matching of Requirements Specifications for Software Services Using Fuzzy Logic.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2632115",
        "volume": "43",
        "abstract": "Today, software components are provided by global markets in the form of services. In order to optimally satisfy service requesters and service providers, adequate techniques for automatic service matching are needed. However, a requester's requirements may be vague and the information available about a provided service may be incomplete. As a consequence, fuzziness is induced into the matching procedure. The contribution of this paper is the development of a systematic matching procedure that leverages concepts and techniques from fuzzy logic and possibility theory based on our formal distinction between different sources and types of fuzziness in the context of service matching. In contrast to existing methods, our approach is able to deal with imprecision and incompleteness in service specifications and to inform users about the extent of induced fuzziness in order to improve the user's decision-making. We demonstrate our approach on the example of specifications for service reputation based on ratings given by previous users. Our evaluation based on real service ratings shows the utility and applicability of our approach.",
        "keywords": [
            "Uncertainty",
            "Fuzzy logic",
            "Context",
            "Security",
            "Software",
            "Software engineering",
            "Decision making"
        ]
    },
    {
        "title": "Locating Software Faults Based on Minimum Debugging Frontier Set.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2632122",
        "volume": "43",
        "abstract": "In this article, we propose a novel state-based fault-localization approach. Given an observed failure that is reproducible under the same program input, this new approach uses two main techniques to reduce the state exploration cost. Firstly, the execution trace to be analyzed for the observed failure is successively narrowed by making the set of trace points in each step a cut of the dynamic dependence graph. Such a cut divides the remaining trace into two parts and, based on the sparse symbolic exploration outcome, one part is removed from further exploration. This process continues until reaching where the fault is determined to be. Second, the cut in each step is chosen such that the union of the program states from the members of the cut is of the minimum size among all candidate cuts. The set of statement instances in the chosen cut is called a minimum debugging frontier set (MDFS). To evaluate our approach, we apply it to 16 real bugs from real world programs and compare our fault reports with those generated by state-of-the-art approaches. Results show that the MDFS approach obtains high quality fault reports for these test cases with considerably higher efficiency than previous approaches.",
        "keywords": [
            "Debugging",
            "Computer aided software engineering",
            "Computer bugs",
            "Software",
            "Computer architecture",
            "Computers",
            "Indexes"
        ]
    },
    {
        "title": "Preventing Defects: The Impact of Requirements Traceability Completeness on Software Quality.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2622264",
        "volume": "43",
        "abstract": "Requirements traceability has long been recognized as an important quality of a well-engineered system. Among stakeholders, traceability is often unpopular due to the unclear benefits. In fact, little evidence exists regarding the expected traceability benefits. There is a need for empirical work that studies the effect of traceability. In this paper, we focus on the four main requirements implementation supporting activities that utilize traceability. For each activity, we propose generalized traceability completeness measures. In a defined process, we selected 24 medium to large-scale open-source projects. For each software project, we quantified the degree to which a studied development activity was enabled by existing traceability with the proposed measures. We analyzed that data in a multi-level Poisson regression analysis. We found that the degree of traceability completeness for three of the studied activities significantly affects software quality, which we quantified as defect rate. Our results provide for the first time empirical evidence that more complete traceability decreases the expected defect rate in the developed software. The strong impact of traceability completeness on the defect rate suggests that traceability is of great practical value for any kind of software development project, even if traceability is not mandated by a standard or regulation.",
        "keywords": [
            "Software quality",
            "Software systems",
            "Context",
            "Software engineering",
            "Stakeholders",
            "Standards"
        ]
    },
    {
        "title": "A Qualitative Study of Application-Level Caching.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2633992",
        "volume": "43",
        "abstract": "Latency and cost of Internet-based services are encouraging the use of application-level caching to continue satisfying users' demands, and improve the scalability and availability of origin servers. Despite its popularity, this level of caching involves the manual implementation by developers and is typically addressed in an ad-hoc way, given that it depends on specific details of the application. As a result, application-level caching is a time-consuming and error-prone task, becoming a common source of bugs. Furthermore, it forces application developers to reason about a crosscutting concern, which is unrelated to the application business logic. In this paper, we present the results of a qualitative study of how developers handle caching logic in their web applications, which involved the investigation of ten software projects with different characteristics. The study we designed is based on comparative and interactive principles of grounded theory, and the analysis of our data allowed us to extract and understand how developers address cache-related concerns to improve performance and scalability of their web applications. Based on our analysis, we derived guidelines and patterns, which guide developers while designing, implementing and maintaining application-level caching, thus supporting developers in this challenging task that is crucial for enterprise web applications.",
        "keywords": [
            "Databases",
            "Guidelines",
            "Maintenance engineering",
            "Servers",
            "Software",
            "Scalability",
            "HTML"
        ]
    },
    {
        "title": "A Survey of App Store Analysis for Software Engineering.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2630689",
        "volume": "43",
        "abstract": "App Store Analysis studies information about applications obtained from app stores. App stores provide a wealth of information derived from users that would not exist had the applications been distributed via previous software deployment methods. App Store Analysis combines this non-technical information with technical information to learn trends and behaviours within these forms of software repositories. Findings from App Store Analysis have a direct and actionable impact on the software teams that develop software for app stores, and have led to techniques for requirements engineering, release planning, software design, security and testing. This survey describes and compares the areas of research that have been explored thus far, drawing out common aspects, trends and directions future research should take to address open problems and challenges.",
        "keywords": [
            "Software",
            "Security",
            "Software engineering",
            "Market research",
            "Ecosystems",
            "Mobile communication",
            "Google"
        ]
    },
    {
        "title": "Reporting Usability Defects: A Systematic Literature Review.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2638427",
        "volume": "43",
        "abstract": "Usability defects can be found either by formal usability evaluation methods or indirectly during system testing or usage. No matter how they are discovered, these defects must be tracked and reported. However, empirical studies indicate that usability defects are often not clearly and fully described. This study aims to identify the state of the art in reporting of usability defects in the software engineering and usability engineering literature. We conducted a systematic literature review of usability defect reporting drawing from both the usability and software engineering literature from January 2000 until March 2016. As a result, a total of 57 studies were identified, in which we classified the studies into three categories: reporting usability defect information, analysing usability defect data and key challenges. Out of these, 20 were software engineering studies and 37 were usability studies. The results of this systematic literature review show that usability defect reporting processes suffer from a number of limitations, including: mixed data, inconsistency of terms and values of usability defect data, and insufficient attributes to classify usability defects. We make a number of recommendations to improve usability defect reporting and management in software engineering.",
        "keywords": [
            "Usability",
            "Systematics",
            "Software engineering",
            "Testing",
            "Human computer interaction",
            "Bibliographies"
        ]
    },
    {
        "title": "Static Analysis of Model Transformations.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2635137",
        "volume": "43",
        "abstract": "Model transformations are central to Model-Driven Engineering (MDE), where they are used to transform models between different languages; to refactor and simulate models; or to generate code from models. Thus, given their prominent role in MDE, practical methods helping in detecting errors in transformations and automate their verification are needed. In this paper, we present a method for the static analysis of ATL model transformations. The method aims at discovering typing and rule errors, like unresolved bindings, uninitialized features or rule conflicts. It relies on static analysis and type inference, and uses constraint solving to assert whether a source model triggering the execution of a given problematic statement can possibly exist. Our method is supported by a tool that integrates seamlessly with the ATL development environment. To evaluate the usefulness of our method, we have used it to analyse a public repository of ATL transformations. The high number of errors discovered shows that static analysis of ATL transformations is needed in practice. Moreover, we have measured the precision and recall of the method by considering a synthetic set of transformations obtained by mutation techniques, and comparing with random testing. The experiment shows good overall results in terms of false positives and negatives.",
        "keywords": [
            "Unified modeling language",
            "Analytical models",
            "Testing",
            "Model driven engineering",
            "Transforms",
            "Manuals",
            "Computational modeling"
        ]
    },
    {
        "title": "Adaptive Multi-Objective Evolutionary Algorithms for Overtime Planning in Software Projects.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2650914",
        "volume": "43",
        "abstract": "Software engineering and development is well-known to suffer from unplanned overtime, which causes stress and illness in engineers and can lead to poor quality software with higher defects. Recently, we introduced a multi-objective decision support approach to help balance project risks and duration against overtime, so that software engineers can better plan overtime. This approach was empirically evaluated on six real world software projects and compared against state-of-the-art evolutionary approaches and currently used overtime strategies. The results showed that our proposal comfortably outperformed all the benchmarks considered. This paper extends our previous work by investigating adaptive multi-objective approaches to meta-heuristic operator selection, thereby extending and (as the results show) improving algorithmic performance. We also extended our empirical study to include two new real world software projects, thereby enhancing the scientific evidence for the technical performance claims made in the paper. Our new results, over all eight projects studied, showed that our adaptive algorithm outperforms the considered state of the art multi-objective approaches in 93 percent of the experiments (with large effect size). The results also confirm that our approach significantly outperforms current overtime planning practices in 100 percent of the experiments (with large effect size).",
        "keywords": [
            "Software",
            "Planning",
            "Software engineering",
            "Search problems",
            "Adaptive algorithms",
            "Project management",
            "Standards"
        ]
    },
    {
        "title": "Automated Extraction and Clustering of Requirements Glossary Terms.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2635134",
        "volume": "43",
        "abstract": "A glossary is an important part of any software requirements document. By making explicit the technical terms in a domain and providing definitions for them, a glossary helps mitigate imprecision and ambiguity. A key step in building a glossary is to decide upon the terms to include in the glossary and to find any related terms. Doing so manually is laborious, particularly for large requirements documents. In this article, we develop an automated approach for extracting candidate glossary terms and their related terms from natural language requirements documents. Our approach differs from existing work on term extraction mainly in that it <i>clusters</i> the extracted terms by relevance, instead of providing a flat list of terms. We provide an automated, mathematically-based procedure for selecting the number of clusters. This procedure makes the underlying clustering algorithm transparent to users, thus alleviating the need for any user-specified parameters. To evaluate our approach, we report on three industrial case studies, as part of which we also examine the perceptions of the involved subject matter experts about the usefulness of our approach. Our evaluation notably suggests that: (1) Over requirements documents, our approach is more accurate than major generic term extraction tools. Specifically, in our case studies, our approach leads to gains of 20 percent or more in terms of recall when compared to existing tools, while at the same time either improving precision or leaving it virtually unchanged. And, (2) the experts involved in our case studies find the clusters generated by our approach useful as an aid for glossary construction.",
        "keywords": [
            "Terminology",
            "Servers",
            "Pipelines",
            "Natural languages",
            "Monitoring",
            "Software",
            "Clustering algorithms"
        ]
    },
    {
        "title": "Deriving Bisimulation Relations from Path Extension Based Equivalence Checkers.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2645687",
        "volume": "43",
        "abstract": "Constructing bisimulation relations between programs as a means of translation validation has been an active field of study. The problem is in general undecidable. Currently available mechanisms suffer from drawbacks such as non-termination and significant restrictions on the structures of programs to be checked. We have developed a path extension based equivalence checking method as an alternative translation validation technique to alleviate these drawbacks. In this work, path extension based equivalence checking of programs (flowcharts) is leveraged to establish a bisimulation relation between a program and its translated version by constructing the relation from the outputs of the equivalence checker.",
        "keywords": [
            "Computational modeling",
            "Inference algorithms",
            "Processor scheduling",
            "Computer science",
            "Electronic mail",
            "Optimization",
            "Integrated circuit modeling"
        ]
    },
    {
        "title": "Identifying Extract Method Refactoring Opportunities Based on Functional Relevance.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2645572",
        "volume": "43",
        "abstract": "`Extract Method' is considered one of the most frequently applied and beneficial refactorings, since the corresponding Long Method smell is among the most common and persistent ones. Although Long Method is conceptually related to the implementation of diverse functionalities within a method, until now, this relationship has not been utilized while identifying refactoring opportunities. In this paper we introduce an approach (accompanied by a tool) that aims at identifying source code chunks that collaborate to provide a specific functionality, and propose their extraction as separate methods. The accuracy of the proposed approach has been empirically validated both in an industrial and an open-source setting. In the former case, the approach was capable of identifying functionally related statements within two industrial long methods (approx. 500 LoC each), with a recall rate of 93 percent. In the latter case, based on a comparative study on open-source data, our approach ranks better compared to two well-known techniques of the literature. To assist software engineers in the prioritization of the suggested refactoring opportunities the approach ranks them based on an estimate of their fitness for extraction. The provided ranking has been validated in both settings and proved to be strongly correlated with experts' opinion.",
        "keywords": [
            "Measurement",
            "Open source software",
            "Mathematics",
            "Data mining",
            "Computer science",
            "Syntactics"
        ]
    },
    {
        "title": "Software Numerical Instability Detection and Diagnosis by Combining Stochastic and Infinite-Precision Testing.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2016.2642956",
        "volume": "43",
        "abstract": "Numerical instability is a well-known problem that may cause serious runtime failures. This paper discusses the reason of instability in software development process, and presents a toolchain that not only detects the potential instability in software, but also diagnoses the reason for such instability. We classify the reason of instability into two categories. When it is introduced by software requirements, we call the instability caused by problem . In this case, it cannot be avoided by improving software development, but requires inspecting the requirements, especially the underlying mathematical properties. Otherwise, we call the instability caused by practice. We design our toolchain as four loosely-coupled tools, which combine stochastic arithmetic with infinite-precision testing. Each tool in our toolchain can be configured with different strategies according to the properties of the analyzed software. We evaluate our toolchain on subjects from literature. The results show that it effectively detects and separates the instabilities caused by problems from others. We also conduct an evaluation on the latest version of GNU Scientific Library, and the toolchain finds a few real bugs in the well-maintained and widely deployed numerical library. With the help of our toolchain, we report the details and fixing advices to the GSL buglist.",
        "keywords": [
            "Software",
            "Software algorithms",
            "Algorithm design and analysis",
            "Libraries",
            "Computer bugs",
            "Software testing"
        ]
    },
    {
        "title": "Language Inclusion Checking of Timed Automata with Non-Zenoness.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2653778",
        "volume": "43",
        "abstract": "Given a timed automaton P modeling an implementation and a timed automaton S as a specification, the problem of language inclusion checking is to decide whether the language of P is a subset of that of S. It is known to be undecidable. The problem gets more complicated if non-Zenoness is taken into consideration. A run is Zeno if it permits infinitely many actions within finite time. Otherwise it is non-Zeno. Zeno runs might present in both P and S. It is necessary to check whether a run is Zeno or not so as to avoid presenting Zeno runs as counterexamples of language inclusion checking. In this work, we propose a zone-based semi-algorithm for language inclusion checking with non-Zenoness. It is further improved with simulation reduction based on LU-simulation. Though our approach is not guaranteed to terminate, we show that it does in many cases through empirical study. Our approach has been incorporated into the PAT model checker, and applied to multiple systems to show its usefulness.",
        "keywords": [
            "Automata",
            "Clocks",
            "Safety",
            "Analytical models",
            "Sun",
            "Real-time systems",
            "Semantics"
        ]
    },
    {
        "title": "Model Transformation Modularization as a Many-Objective Optimization Problem.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2654255",
        "volume": "43",
        "abstract": "Model transformation programs are iteratively refined, restructured, and evolved due to many reasons such as fixing bugs and adapting existing transformation rules to new metamodels version. Thus, modular design is a desirable property for model transformations as it can significantly improve their evolution, comprehensibility, maintainability, reusability, and thus, their overall quality. Although language support for modularization of model transformations is emerging, model transformations are created as monolithic artifacts containing a huge number of rules. To the best of our knowledge, the problem of automatically modularizing model transformation programs was not addressed before in the current literature. These programs written in transformation languages, such as ATL, are implemented as one main module including a huge number of rules. To tackle this problem and improve the quality and maintainability of model transformation programs, we propose an automated search-based approach to modularize model transformations based on higher-order transformations. Their application and execution is guided by our search framework which combines an in-place transformation engine and a search-based algorithm framework. We demonstrate the feasibility of our approach by using ATL as concrete transformation language and NSGA-III as search algorithm to find a trade-off between different well-known conflicting design metrics for the fitness functions to evaluate the generated modularized solutions. To validate our approach, we apply it to a comprehensive dataset of model transformations. As the study shows, ATL transformations can be modularized automatically, efficiently, and effectively by our approach. We found that, on average, the majority of recommended modules, for all the ATL programs, by NSGA-III are considered correct with more than 84 percent of precision and 86 percent of recall when compared to manual solutions provided by active developers. The statistical analysis of our experiments over several runs shows that NSGA-III performed significantly better than multi-objective algorithms and random search. We were not able to compare with existing model transformations modularization approaches since our study is the first to address this problem. The software developers considered in our experiments confirm the relevance of the recommended modularization solutions for several maintenance activities based on different scenarios and interviews.",
        "keywords": [
            "Unified modeling language",
            "Object oriented modeling",
            "Adaptation models",
            "Measurement",
            "Algorithm design and analysis",
            "Software engineering",
            "Computer bugs"
        ]
    },
    {
        "title": "Testing from Partial Finite State Machines without Harmonised Traces.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2652457",
        "volume": "43",
        "abstract": "This paper concerns the problem of testing from a partial, possibly non-deterministic, finite state machine (FSM) S. Two notions of correctness (quasi-reduction and quasi-equivalence) have previously been defined for partial FSMs but these, and the corresponding test generation techniques, only apply to FSMs that have harmonised traces. We show how quasi-reduction and quasi-equivalence can be generalised to all partial FSMs. We also consider the problem of generating an m-complete test suite from a partial FSM S: a test suite that is guaranteed to determine correctness as long as the system under test has no more than m states. We prove that we can complete S to form a completely-specified non-deterministic FSM S' such that any m-complete test suite generated from S' can be converted into an m-complete test suite for S. We also show that there is a correspondence between test suites that are reduced for S and S' and also that are minimal for S and S'.",
        "keywords": [
            "Testing",
            "Fault detection",
            "Redundancy",
            "Automata",
            "Indexes",
            "Software",
            "Debugging"
        ]
    },
    {
        "title": "Using Natural Language Processing to Automatically Detect Self-Admitted Technical Debt.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2654244",
        "volume": "43",
        "abstract": "The metaphor of technical debt was introduced to express the trade off between productivity and quality, i.e., when developers take shortcuts or perform quick hacks. More recently, our work has shown that it is possible to detect technical debt using source code comments (i.e., self-admitted technical debt), and that the most common types of self-admitted technical debt are design and requirement debt. However, all approaches thus far heavily depend on the manual classification of source code comments. In this paper, we present an approach to automatically identify design and requirement self-admitted technical debt using Natural Language Processing (NLP). We study 10 open source projects: Ant, ArgoUML, Columba, EMF, Hibernate, JEdit, JFreeChart, JMeter, JRuby and SQuirrel SQL and find that 1) we are able to accurately identify self-admitted technical debt, significantly outperforming the current state-of-the-art based on fixed keywords and phrases; 2) words related to sloppy code or mediocre source code quality are the best indicators of design debt, whereas words related to the need to complete a partially implemented requirement in the future are the best indicators of requirement debt; and 3) we can achieve 90 percent of the best classification performance, using as little as 23 percent of the comments for both design and requirement self-admitted technical debt, and 80 percent of the best performance, using as little as 9 and 5 percent of the comments for design and requirement self-admitted technical debt, respectively. The last finding shows that the proposed approach can achieve a good accuracy even with a relatively small training dataset.",
        "keywords": [
            "Software",
            "Natural language processing",
            "Manuals",
            "Entropy",
            "Unified modeling language",
            "Java",
            "Structured Query Language"
        ]
    },
    {
        "title": "When and Why Your Code Starts to Smell Bad (and Whether the Smells Go Away).",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2653105",
        "volume": "43",
        "abstract": "Technical debt is a metaphor introduced by Cunningham to indicate “not quite right code which we postpone making it right”. One noticeable symptom of technical debt is represented by code smells, defined as symptoms of poor design and implementation choices. Previous studies showed the negative impact of code smells on the comprehensibility and maintainability of code. While the repercussions of smells on code quality have been empirically assessed, there is still only anecdotal evidence on when and why bad smells are introduced, what is their survivability, and how they are removed by developers. To empirically corroborate such anecdotal evidence, we conducted a large empirical study over the change history of 200 open source projects. This study required the development of a strategy to identify smell-introducing commits, the mining of over half a million of commits, and the manual analysis and classification of over 10K of them. Our findings mostly contradict common wisdom, showing that most of the smell instances are introduced when an artifact is created and not as a result of its evolution. At the same time, 80 percent of smells survive in the system. Also, among the 20 percent of removed instances, only 9 percent are removed as a direct consequence of refactoring operations.",
        "keywords": [
            "Ecosystems",
            "History",
            "Androids",
            "Humanoid robots",
            "Software systems",
            "Maintenance engineering"
        ]
    },
    {
        "title": "Clarifications on the Construction and Use of the ManyBugs Benchmark.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2755651",
        "volume": "43",
        "abstract": "Automated repair techniques produce variant php interpreters, which should naturally serve as the tested interpreters. However, the answer to the question of what should serve as the testing interpreter is less obvious. php's default test harness configuration uses the same version of the interpreter for both the tested and testing interpreter. However, php may be configured via a command-line argument to use a different interpreter, such as the unmodified defective version, or a separate, manually-repaired version.",
        "keywords": [
            "Maintenance engineering",
            "Benchmark testing",
            "Computer science",
            "Electronic mail",
            "Software engineering",
            "Software"
        ]
    },
    {
        "title": "Comments on ScottKnottESD in Response to \"An Empirical Comparison of Model Validation Techniques for Defect Prediction Models\".",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2748129",
        "volume": "43",
        "abstract": "In this article, we discuss the ScottKnottESD test, which was proposed in a recent paper “An Empirical Comparison of Model Validation Techniques for Defect Prediction Models” that was published in this journal. We discuss the implications and the empirical impact of the proposed normality correction of ScottKnottESD and come to the conclusion that this correction does not necessarily lead to the fulfillment of the assumptions of the original Scott-Knott test and may cause problems with the statistical analysis.",
        "keywords": [
            "Analysis of variance",
            "Measurement",
            "Distributed databases",
            "Predictive models",
            "Sociology"
        ]
    },
    {
        "title": "Autofolding for Source Code Summarization.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2664836",
        "volume": "43",
        "abstract": "Developers spend much of their time reading and browsing source code, raising new opportunities for summarization methods. Indeed, modern code editors provide code folding, which allows one to selectively hide blocks of code. However this is impractical to use as folding decisions must be made manually or based on simple rules. We introduce the autofolding problem, which is to automatically create a code summary by folding less informative code regions. We present a novel solution by formulating the problem as a sequence of AST folding decisions, leveraging a scoped topic model for code tokens. On an annotated set of popular open source projects, we show that our summarizer outperforms simpler baselines, yielding a 28 percent error reduction. Furthermore, we find through a case study that our summarizer is strongly preferred by experienced developers. More broadly, we hope this work will aid program comprehension by turning code folding into a usable and valuable tool.",
        "keywords": [
            "Software development",
            "Natural languages",
            "Source coding",
            "Feature extraction",
            "Complexity theory"
        ]
    },
    {
        "title": "AutoSense: A Framework for Automated Sensitivity Analysis of Program Data.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2654251",
        "volume": "43",
        "abstract": "In recent times, approximate computing is being increasingly adopted across the computing stack, from algorithms to computing hardware, to gain energy and performance efficiency by trading accuracy within acceptable limits. Approximation aware programming languages have been proposed where programmers can annotate data with type qualifiers (e.g., precise and approx) to denote its reliability. However, programmers need to judiciously annotate so that the accuracy loss remains within the desired limits. This can be non-trivial for large applications where error resilient and non-resilient program data may not be easily identifiable. Mis-annotation of even one data as error resilient/insensitive may result in an unacceptable output. In this paper, we present AutoSense, a framework to automatically classify resilient (insensitive) program data versus the sensitive ones with probabilistic reliability guarantee. AutoSense implements a combination of dynamic and static analysis methods for data sensitivity analysis. The dynamic analysis is based on statistical hypothesis testing, while the static analysis is based on classical data flow analysis. Experimental results compare our automated data classification with reported manual annotations on popular benchmarks used in approximate computing literature. AutoSense achieves promising reliability results compared to manual annotations and earlier methods, as evident from the experimental results.",
        "keywords": [
            "Quality of service",
            "Sensitivity analysis",
            "Approximate computing",
            "Probabilistic logic",
            "Sequential analysis"
        ]
    },
    {
        "title": "On the Positive Effect of Reactive Programming on Software Comprehension: An Empirical Study.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2655524",
        "volume": "43",
        "abstract": "Starting from the first investigations with strictly functional languages, reactive programming has been proposed as the programming paradigm for reactive applications. Over the years, researchers have enriched reactive languages with more powerful abstractions, embedded these abstractions into mainstream languages-including object-oriented languages-and applied reactive programming to several domains, such as GUIs, animations, Web applications, robotics, and sensor networks. However, an important assumption behind this line of research is that, beside other claimed advantages, reactive programming makes a wide class of otherwise cumbersome applications more comprehensible. This claim has never been evaluated. In this paper, we present the first empirical study that evaluates the effect of reactive programming on comprehension. The study involves 127 subjects and compares reactive programming to the traditional object-oriented style with the Observer design pattern. Our findings show that program comprehension is significantly enhanced by the reactive-programming paradigm-a result that suggests to further develop research in this field.",
        "keywords": [
            "Programming",
            "Runtime",
            "Software development",
            "Robot sensing systems"
        ]
    },
    {
        "title": "Reasoning About Identifier Spaces: How to Make Chord Correct.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2655056",
        "volume": "43",
        "abstract": "The Chord distributed hash table (DHT) is well-known and often used to implement peer-to-peer systems. Chord peers find other peers, and access their data, through a ring-shaped pointer structure in a large identifier space. Despite claims of proven correctness, i.e., eventual reachability, previous work has shown that the Chord ring-maintenance protocol is not correct under its original operating assumptions. Previous work has not, however, discovered whether Chord could be made correct under the same assumptions. The contribution of this paper is to provide the first specification of correct operations and initialization for Chord, an inductive invariant that is necessary and sufficient to support a proof of correctness, and two independent proofs of correctness. One proof is informal and intuitive, and applies to networks of any size. The other proof is based on a formal model in Alloy, and uses fully automated analysis to prove the assertions for networks of bounded size. The two proofs complement each other in several important ways.",
        "keywords": [
            "Peer-to-peer computing",
            "Formal verification",
            "Information processing",
            "Analytical models",
            "Structural rings",
            "Distributed processing"
        ]
    },
    {
        "title": "Semantics-Based Obfuscation-Resilient Binary Code Similarity Comparison with Applications to Software and Algorithm Plagiarism Detection.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2655046",
        "volume": "43",
        "abstract": "Existing code similarity comparison methods, whether source or binary code based, are mostly not resilient to obfuscations. Identifying similar or identical code fragments among programs is very important in some applications. For example, one application is to detect illegal code reuse. In the code theft cases, emerging obfuscation techniques have made automated detection increasingly difficult. Another application is to identify cryptographic algorithms which are widely employed by modern malware to circumvent detection, hide network communications, and protect payloads among other purposes. Due to diverse coding styles and high programming flexibility, different implementation of the same algorithm may appear very distinct, causing automatic detection to be very hard, let alone code obfuscations are sometimes applied. In this paper, we propose a binary-oriented, obfuscation-resilient binary code similarity comparison method based on a new concept, longest common subsequence of semantically equivalent basic blocks , which combines rigorous program semantics with longest common subsequence based fuzzy matching. We model the semantics of a basic block by a set of symbolic formulas representing the input-output relations of the block. This way, the semantic equivalence (and similarity) of two blocks can be checked by a theorem prover. We then model the semantic similarity of two paths using the longest common subsequence with basic blocks as elements. This novel combination has resulted in strong resiliency to code obfuscation. We have developed a prototype. The experimental results show that our method can be applied to software plagiarism and algorithm detection, and is effective and practical to analyze real-world software.",
        "keywords": [
            "Semantics",
            "Software development",
            "Plagiarism",
            "Binary codes",
            "Software algorithms",
            "Syntactics",
            "Computational modeling"
        ]
    },
    {
        "title": "The Work Life of Developers: Activities, Switches and Perceived Productivity.",
        "venue_name": "IEEE Transactions on Software Engineering",
        "year": 2017,
        "venue_type": "journals",
        "url": "https://doi.org/10.1109/TSE.2017.2656886",
        "volume": "43",
        "abstract": "Many software development organizations strive to enhance the productivity of their developers. All too often, efforts aimed at improving developer productivity are undertaken without knowledge about how developers spend their time at work and how it influences their own perception of productivity. To fill in this gap, we deployed a monitoring application at 20 computers of professional software developers from four companies for an average of 11 full work day in situ. Corroborating earlier findings, we found that developers spend their time on a wide variety of activities and switch regularly between them, resulting in highly fragmented work. Our findings extend beyond existing research in that we correlate developers' work habits with perceived productivity and also show productivity is a personal matter. Although productivity is personal, developers can be roughly grouped into morning, low-at-lunch and afternoon people. A stepwise linear regression per participant revealed that more user input is most often associated with a positive, and emails, planned meetings and work unrelated websites with a negative perception of productivity. We discuss opportunities of our findings, the potential to predict high and low productivity and suggest design approaches to create better tool support for planning developers' work day and improving their personal productivity.",
        "keywords": [
            "Productivity",
            "Software development",
            "Encoding",
            "Human factors",
            "Monitoring"
        ]
    }
]